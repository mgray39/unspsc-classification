{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a77125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.10.0)\n",
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 KB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch) (4.0.0)\n",
      "Collecting gdown>=4.0.0\n",
      "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gdown>=4.0.0->nlpaug) (4.10.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14933 sha256=7dd876cc2f04f275541e82ab3aa24501979510a56053846056672c69e28fb823\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/8d/df/71/846b2aa0fabaac2af23fbc5214eeaa55f0616e9d1a05187d72\n",
      "Successfully built gdown\n",
      "Installing collected packages: tokenizers, xxhash, responses, huggingface-hub, transformers, gdown, nlpaug, datasets\n",
      "Successfully installed datasets-2.4.0 gdown-4.5.1 huggingface-hub-0.9.1 nlpaug-1.1.11 responses-0.18.0 tokenizers-0.12.1 transformers-4.22.1 xxhash-3.0.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch nlpaug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27d081",
   "metadata": {},
   "source": [
    "# Download Data & Prepare\n",
    "\n",
    "The script `wrangling_segment.py` by default will use files downloaded to prepare the datasets for predicting UNSPSC market Segment. If the --download flag is passed, the program first hits the web sources for this data and downloads them prior to creating prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bca60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://data.ok.gov/dataset/18a622a6-32d1-48f6-842a-8232bc4ca06c/resource/b92ad3ac-b0f5-4c62-9bd0-eac023cfd083/download/data-unspsc-codes.csv to ./data/codes/data-unspsc-codes.csv\n",
      "    done\n",
      "downloading https://data.ca.gov/dataset/ae343670-f827-4bc8-9d44-2af937d60190/resource/bb82edc5-9c78-44e2-8947-68ece26197c5/download/purchase-order-data-2012-2015-.csv to ./data/california/purchase-order-data-2012-2015-.csv\n",
      "    done\n",
      "downloading https://data.gov.au/data/dataset/5c7fa69b-b0e9-4553-b8df-2a022dd2e982/resource/561a549b-5a65-450e-86cf-81d392d8fef3/download/20142015fy.csv to ./data/australia/20142015fy.csv\n",
      "    done\n",
      "downloading https://data.gov.au/data/dataset/5c7fa69b-b0e9-4553-b8df-2a022dd2e982/resource/21212500-169f-4745-86b3-6ac1c1174151/download/2016-2017-australian-government-contract-data.csv to ./data/australia/2016-2017-australian-government-contract-data.csv\n",
      "    done\n",
      "downloading https://data.gov.au/data/dataset/5c7fa69b-b0e9-4553-b8df-2a022dd2e982/resource/bc2097b7-8116-4e9d-9953-98813635892a/download/17-18-fy-dataset.csv to ./data/australia/17-18-fy-dataset.csv\n",
      "    done\n",
      "57\n",
      "check if code numbers preserved...\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "!python wrangling_segment.py --download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f588e",
   "metadata": {},
   "source": [
    "# Conduct Data Augmentation\n",
    "\n",
    "Class imbalance is a significant problem in this task. The largest segment has several thousand langauge samples, while the smallest has less than 100. As a result, data augmentation using randomised synonym replacement has been used to try to augment the training set.\n",
    "\n",
    "The script `data_augmentation.py` is able to implement this and includes options for the augmentation routine. In particular, it is possible to perform augmentation to a certain level while undersampling the larger segments to ensure that the classification problem is perfectly balanced. Excess samples are redistributed to the test set for later use in validation. It is also possible to set the number to increase the under-represented classes to. \n",
    "\n",
    "The default behaviour lifts the number of samples in each of the under-represented classes to 1000 records, while leaving the over-represented classes unchanged.\n",
    "\n",
    "Defaults are sufficient for our current purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir prepared_data/rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2944d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "!python data_augmentation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff6f3c",
   "metadata": {},
   "source": [
    "# Upload Prepared Files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73c0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload again.\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'unspsc-data-private'\n",
    "prefix = 'segment_training'\n",
    "\n",
    "data_input = sagemaker_session.upload_data(path = './prepared_data/rebalanced/', bucket= bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5dd7d",
   "metadata": {},
   "source": [
    "# RNN Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a0769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for restart --\n",
    "\n",
    "data_input = 's3://unspsc-data/segment_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18a261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fca44",
   "metadata": {},
   "source": [
    "We need to pass with our entry point script a requirements.txt file for sagemaker to be able to install torchtext.\n",
    "\n",
    "Information on how to do this obtained from:\n",
    "\n",
    "https://github.com/awslabs/sagemaker-privacy-for-nlp/blob/master/source/sagemaker/2.Model_Training.ipynb\n",
    "\n",
    "I install pip-tools and use the pip-compile feature to extract all libraries to support what is in requirements.in which are all of the modules imported by train_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e6d6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pip-tools\n",
      "  Downloading pip_tools-6.8.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (0.37.0)\n",
      "Requirement already satisfied: click>=7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (59.2.0)\n",
      "Requirement already satisfied: pip>=21.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (22.0.4)\n",
      "Collecting build\n",
      "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
      "Collecting pep517>=0.9.1\n",
      "  Downloading pep517-0.13.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from build->pip-tools) (1.2.2)\n",
      "Requirement already satisfied: packaging>=19.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from build->pip-tools) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=19.0->build->pip-tools) (3.0.6)\n",
      "Installing collected packages: pep517, build, pip-tools\n",
      "Successfully installed build-0.8.0 pep517-0.13.0 pip-tools-6.8.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pip-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962fb859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#\u001b[0m\u001b[0m\n",
      "\u001b[32m# This file is autogenerated by pip-compile with python 3.8\u001b[0m\u001b[0m\n",
      "\u001b[32m# To update, run:\u001b[0m\u001b[0m\n",
      "\u001b[32m#\u001b[0m\u001b[0m\n",
      "\u001b[32m#    pip-compile ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "\u001b[32m#\u001b[0m\u001b[0m\n",
      "--extra-index-url https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[0m\n",
      "attrs==21.4.0\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "awscli==1.25.76\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "boto3==1.24.75\n",
      "    \u001b[32m# via\n",
      "    #   -r ./rnn_baseline/requirements.in\n",
      "    #   sagemaker\u001b[0m\u001b[0m\n",
      "botocore==1.27.75\n",
      "    \u001b[32m# via\n",
      "    #   awscli\n",
      "    #   boto3\n",
      "    #   s3transfer\u001b[0m\u001b[0m\n",
      "certifi==2022.9.14\n",
      "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
      "charset-normalizer==2.1.1\n",
      "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
      "colorama==0.4.4\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "dill==0.3.5.1\n",
      "    \u001b[32m# via\n",
      "    #   multiprocess\n",
      "    #   pathos\u001b[0m\u001b[0m\n",
      "docutils==0.16\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "google-pasta==0.2.0\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "idna==3.4\n",
      "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
      "importlib-metadata==4.12.0\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "jmespath==1.0.1\n",
      "    \u001b[32m# via\n",
      "    #   boto3\n",
      "    #   botocore\u001b[0m\u001b[0m\n",
      "joblib==1.2.0\n",
      "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
      "multiprocess==0.70.13\n",
      "    \u001b[32m# via pathos\u001b[0m\u001b[0m\n",
      "numpy==1.23.3\n",
      "    \u001b[32m# via\n",
      "    #   pandas\n",
      "    #   sagemaker\n",
      "    #   scikit-learn\n",
      "    #   scipy\n",
      "    #   torchtext\u001b[0m\u001b[0m\n",
      "packaging==21.3\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "pandas==1.4.4\n",
      "    \u001b[32m# via\n",
      "    #   -r ./rnn_baseline/requirements.in\n",
      "    #   sagemaker\u001b[0m\u001b[0m\n",
      "pathos==0.2.9\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "pox==0.3.1\n",
      "    \u001b[32m# via pathos\u001b[0m\u001b[0m\n",
      "ppft==1.7.6.5\n",
      "    \u001b[32m# via pathos\u001b[0m\u001b[0m\n",
      "protobuf==3.20.2\n",
      "    \u001b[32m# via\n",
      "    #   protobuf3-to-dict\n",
      "    #   sagemaker\u001b[0m\u001b[0m\n",
      "protobuf3-to-dict==0.1.5\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "pyasn1==0.4.8\n",
      "    \u001b[32m# via rsa\u001b[0m\u001b[0m\n",
      "pyparsing==3.0.9\n",
      "    \u001b[32m# via packaging\u001b[0m\u001b[0m\n",
      "python-dateutil==2.8.2\n",
      "    \u001b[32m# via\n",
      "    #   botocore\n",
      "    #   pandas\u001b[0m\u001b[0m\n",
      "pytz==2022.2.1\n",
      "    \u001b[32m# via pandas\u001b[0m\u001b[0m\n",
      "pyyaml==5.4.1\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "requests==2.28.1\n",
      "    \u001b[32m# via torchtext\u001b[0m\u001b[0m\n",
      "rsa==4.7.2\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "s3transfer==0.6.0\n",
      "    \u001b[32m# via\n",
      "    #   awscli\n",
      "    #   boto3\u001b[0m\u001b[0m\n",
      "sagemaker==2.109.0\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "scikit-learn==1.1.2\n",
      "    \u001b[32m# via sklearn\u001b[0m\u001b[0m\n",
      "scipy==1.9.1\n",
      "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
      "six==1.16.0\n",
      "    \u001b[32m# via\n",
      "    #   google-pasta\n",
      "    #   ppft\n",
      "    #   protobuf3-to-dict\n",
      "    #   python-dateutil\u001b[0m\u001b[0m\n",
      "sklearn==0.0\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "smdebug-rulesconfig==1.0.1\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "threadpoolctl==3.1.0\n",
      "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
      "torch==1.10.0\n",
      "    \u001b[32m# via\n",
      "    #   -r ./rnn_baseline/requirements.in\n",
      "    #   torchtext\u001b[0m\u001b[0m\n",
      "torchtext==0.11.0\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "tqdm==4.64.1\n",
      "    \u001b[32m# via torchtext\u001b[0m\u001b[0m\n",
      "typing-extensions==4.3.0\n",
      "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
      "urllib3==1.26.12\n",
      "    \u001b[32m# via\n",
      "    #   botocore\n",
      "    #   requests\u001b[0m\u001b[0m\n",
      "zipp==3.8.1\n",
      "    \u001b[32m# via importlib-metadata\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip-compile ./rnn_baseline/requirements.in > ./rnn_baseline/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17970395",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='train_rnn.py',\n",
    "                    source_dir = './rnn_baseline/',\n",
    "                    instance_count = 1,\n",
    "                    instance_type = 'ml.g4dn.xlarge',\n",
    "                    framework_version = '1.10',\n",
    "                    py_version='py38',\n",
    "                    role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be81882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-18 03:39:34 Starting - Starting the training job...\n",
      "2022-09-18 03:39:57 Starting - Preparing the instances for trainingProfilerReport-1663472373: InProgress\n",
      ".........\n",
      "2022-09-18 03:41:33 Downloading - Downloading input data\n",
      "2022-09-18 03:41:33 Training - Downloading the training image.....................\n",
      "2022-09-18 03:45:04 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-18 03:45:07,547 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-18 03:45:07,578 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-18 03:45:07,585 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-18 03:45:08,070 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs==21.4.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting awscli==1.25.76\u001b[0m\n",
      "\u001b[34mDownloading awscli-1.25.76-py3-none-any.whl (3.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 62.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting boto3==1.24.75\u001b[0m\n",
      "\u001b[34mDownloading boto3-1.24.75-py3-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.5/132.5 kB 24.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting botocore==1.27.75\u001b[0m\n",
      "\u001b[34mDownloading botocore-1.27.75-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 76.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting certifi==2022.9.14\u001b[0m\n",
      "\u001b[34mDownloading certifi-2022.9.14-py3-none-any.whl (162 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 28.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer==2.1.1\u001b[0m\n",
      "\u001b[34mDownloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama==0.4.4 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill==0.3.5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils==0.16 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta==0.2.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (0.2.0)\u001b[0m\n",
      "\u001b[34mCollecting idna==3.4\u001b[0m\n",
      "\u001b[34mDownloading idna-3.4-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 13.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata==4.12.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath==1.0.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting joblib==1.2.0\u001b[0m\n",
      "\u001b[34mDownloading joblib-1.2.0-py3-none-any.whl (297 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 31.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess==0.70.13 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (0.70.13)\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.23.3\u001b[0m\n",
      "\u001b[34mDownloading numpy-1.23.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 73.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging==21.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.4.4\u001b[0m\n",
      "\u001b[34mDownloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 90.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos==0.2.9 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 61)) (0.2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox==0.3.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 63)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft==1.7.6.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 65)) (1.7.6.5)\u001b[0m\n",
      "\u001b[34mCollecting protobuf==3.20.2\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 77.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict==0.1.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 71)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1==0.4.8 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 73)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing==3.0.9 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 75)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil==2.8.2 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 77)) (2.8.2)\u001b[0m\n",
      "\u001b[34mCollecting pytz==2022.2.1\u001b[0m\n",
      "\u001b[34mDownloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 500.6/500.6 kB 40.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml==5.4.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 83)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.28.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 85)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa==4.7.2 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 87)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer==0.6.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 89)) (0.6.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker==2.109.0\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.109.0.tar.gz (571 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 571.8/571.8 kB 54.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==1.1.2\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 52.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting scipy==1.9.1\u001b[0m\n",
      "\u001b[34mDownloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.4/43.4 MB 45.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 99)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sklearn==0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 105)) (0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 107)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl==3.1.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 109)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting torch==1.10.0\u001b[0m\n",
      "\u001b[34mDownloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 881.9/881.9 MB 1.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting torchtext==0.11.0\u001b[0m\n",
      "\u001b[34mDownloading torchtext-0.11.0-cp38-cp38-manylinux1_x86_64.whl (8.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/8.0 MB 104.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.64.1\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions==4.3.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 119)) (4.3.0)\u001b[0m\n",
      "\u001b[34mCollecting urllib3==1.26.12\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 30.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp==3.8.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 125)) (3.8.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.109.0-py2.py3-none-any.whl size=787690 sha256=9d48024299649e06d9e5c21a84d9e88c7dfa5be0968bcbdee3c1c70b4ab024cd\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/57/93/29/bf7214bd2d7d0514e155957ee2d54d5421c1ff75cf6af2227e\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, urllib3, tqdm, torch, protobuf, numpy, joblib, idna, charset-normalizer, certifi, scipy, pandas, botocore, torchtext, scikit-learn, boto3, awscli, sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pytz\u001b[0m\n",
      "\u001b[34mFound existing installation: pytz 2022.1\u001b[0m\n",
      "\u001b[34mUninstalling pytz-2022.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pytz-2022.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.10\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.10:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.10\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.63.0\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.63.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.63.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 1.10.2+cu113\u001b[0m\n",
      "\u001b[34mUninstalling torch-1.10.2+cu113:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-1.10.2+cu113\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.19.4\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.19.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.19.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.22.2\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.22.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.22.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: joblib\u001b[0m\n",
      "\u001b[34mFound existing installation: joblib 1.1.0\u001b[0m\n",
      "\u001b[34mUninstalling joblib-1.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled joblib-1.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: idna\u001b[0m\n",
      "\u001b[34mFound existing installation: idna 3.3\u001b[0m\n",
      "\u001b[34mUninstalling idna-3.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled idna-3.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: charset-normalizer\u001b[0m\n",
      "\u001b[34mFound existing installation: charset-normalizer 2.0.12\u001b[0m\n",
      "\u001b[34mUninstalling charset-normalizer-2.0.12:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled charset-normalizer-2.0.12\u001b[0m\n",
      "\u001b[34mAttempting uninstall: certifi\u001b[0m\n",
      "\u001b[34mFound existing installation: certifi 2022.6.15\u001b[0m\n",
      "\u001b[34mUninstalling certifi-2022.6.15:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled certifi-2022.6.15\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.8.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.8.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.8.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pandas\u001b[0m\n",
      "\u001b[34mFound existing installation: pandas 1.4.3\u001b[0m\n",
      "\u001b[34mUninstalling pandas-1.4.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pandas-1.4.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[34mFound existing installation: botocore 1.27.36\u001b[0m\n",
      "\u001b[34mUninstalling botocore-1.27.36:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled botocore-1.27.36\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.1.1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.1.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.1.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[34mFound existing installation: boto3 1.24.36\u001b[0m\n",
      "\u001b[34mUninstalling boto3-1.24.36:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled boto3-1.24.36\u001b[0m\n",
      "\u001b[34mAttempting uninstall: awscli\u001b[0m\n",
      "\u001b[34mFound existing installation: awscli 1.25.36\u001b[0m\n",
      "\u001b[34mUninstalling awscli-1.25.36:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled awscli-1.25.36\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.100.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.100.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.100.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-training 4.2.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed awscli-1.25.76 boto3-1.24.75 botocore-1.27.75 certifi-2022.9.14 charset-normalizer-2.1.1 idna-3.4 joblib-1.2.0 numpy-1.23.3 pandas-1.4.4 protobuf-3.20.2 pytz-2022.2.1 sagemaker-2.109.0 scikit-learn-1.1.2 scipy-1.9.1 torch-1.10.0 torchtext-0.11.0 tqdm-4.64.1 urllib3-1.26.12\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-09-18 03:46:19,426 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-09-18 03:46:19,426 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-09-18 03:46:19,517 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-09-18-03-39-33-268\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-556976497373/pytorch-training-2022-09-18-03-39-33-268/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_rnn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_rnn.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_rnn.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_rnn\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-556976497373/pytorch-training-2022-09-18-03-39-33-268/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-09-18-03-39-33-268\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-556976497373/pytorch-training-2022-09-18-03-39-33-268/source/sourcedir.tar.gz\",\"module_name\":\"train_rnn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_rnn.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_rnn.py\u001b[0m\n",
      "\u001b[34mcurrent epoch: 1\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/500434 (0%)] Loss: 4.057405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/500434 (2%)] Loss: 3.898205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/500434 (4%)] Loss: 3.639316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/500434 (6%)] Loss: 3.513347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/500434 (8%)] Loss: 3.491880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/500434 (10%)] Loss: 3.477610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [61440/500434 (12%)] Loss: 3.508563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [71680/500434 (14%)] Loss: 3.515231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [81920/500434 (16%)] Loss: 3.426704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [92160/500434 (18%)] Loss: 3.443478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [102400/500434 (20%)] Loss: 3.442863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [112640/500434 (22%)] Loss: 3.504910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [122880/500434 (25%)] Loss: 3.441718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [133120/500434 (27%)] Loss: 3.497076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [143360/500434 (29%)] Loss: 3.416993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [153600/500434 (31%)] Loss: 3.501538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [163840/500434 (33%)] Loss: 3.449270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [174080/500434 (35%)] Loss: 3.473472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [184320/500434 (37%)] Loss: 3.530840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [194560/500434 (39%)] Loss: 3.494579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [204800/500434 (41%)] Loss: 3.444205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [215040/500434 (43%)] Loss: 3.437055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [225280/500434 (45%)] Loss: 3.486469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [235520/500434 (47%)] Loss: 3.430015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [245760/500434 (49%)] Loss: 3.481286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [256000/500434 (51%)] Loss: 3.549564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [266240/500434 (53%)] Loss: 3.537149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [276480/500434 (55%)] Loss: 3.470390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [286720/500434 (57%)] Loss: 3.495215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [296960/500434 (59%)] Loss: 3.502332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [307200/500434 (61%)] Loss: 3.502642\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [317440/500434 (63%)] Loss: 3.491506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [327680/500434 (65%)] Loss: 3.500136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [337920/500434 (67%)] Loss: 3.508496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [348160/500434 (70%)] Loss: 3.483028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [358400/500434 (72%)] Loss: 3.494404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [368640/500434 (74%)] Loss: 3.435215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [378880/500434 (76%)] Loss: 3.425402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [389120/500434 (78%)] Loss: 3.431029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [399360/500434 (80%)] Loss: 3.480382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [409600/500434 (82%)] Loss: 3.542814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [419840/500434 (84%)] Loss: 3.485293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [430080/500434 (86%)] Loss: 3.470152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [440320/500434 (88%)] Loss: 3.484056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [450560/500434 (90%)] Loss: 3.413671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [460800/500434 (92%)] Loss: 3.511469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [471040/500434 (94%)] Loss: 3.462918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [481280/500434 (96%)] Loss: 3.424584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [491520/500434 (98%)] Loss: 3.468382\u001b[0m\n",
      "\u001b[34mcurrent epoch: 2\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/500434 (0%)] Loss: 3.471967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/500434 (2%)] Loss: 3.495338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/500434 (4%)] Loss: 3.486137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/500434 (6%)] Loss: 3.414135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40960/500434 (8%)] Loss: 3.535795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/500434 (10%)] Loss: 3.501001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [61440/500434 (12%)] Loss: 3.449669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [71680/500434 (14%)] Loss: 3.507673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [81920/500434 (16%)] Loss: 3.486237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [92160/500434 (18%)] Loss: 3.476757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [102400/500434 (20%)] Loss: 3.506837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [112640/500434 (22%)] Loss: 3.500567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [122880/500434 (25%)] Loss: 3.457846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [133120/500434 (27%)] Loss: 3.423025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [143360/500434 (29%)] Loss: 3.482107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [153600/500434 (31%)] Loss: 3.440538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [163840/500434 (33%)] Loss: 3.443644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [174080/500434 (35%)] Loss: 3.523329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [184320/500434 (37%)] Loss: 3.432773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [194560/500434 (39%)] Loss: 3.489881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [204800/500434 (41%)] Loss: 3.475540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [215040/500434 (43%)] Loss: 3.464225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [225280/500434 (45%)] Loss: 3.473282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [235520/500434 (47%)] Loss: 3.470304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [245760/500434 (49%)] Loss: 3.440635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [256000/500434 (51%)] Loss: 3.475510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [266240/500434 (53%)] Loss: 3.493358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [276480/500434 (55%)] Loss: 3.462633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [286720/500434 (57%)] Loss: 3.469100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [296960/500434 (59%)] Loss: 3.478657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [307200/500434 (61%)] Loss: 3.448266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [317440/500434 (63%)] Loss: 3.490026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [327680/500434 (65%)] Loss: 3.471930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [337920/500434 (67%)] Loss: 3.526596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [348160/500434 (70%)] Loss: 3.462555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [358400/500434 (72%)] Loss: 3.501084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [368640/500434 (74%)] Loss: 3.432726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [378880/500434 (76%)] Loss: 3.453997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [389120/500434 (78%)] Loss: 3.377275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [399360/500434 (80%)] Loss: 3.347754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [409600/500434 (82%)] Loss: 3.317668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [419840/500434 (84%)] Loss: 3.368198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [430080/500434 (86%)] Loss: 3.356938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [440320/500434 (88%)] Loss: 3.395783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [450560/500434 (90%)] Loss: 3.281642\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [460800/500434 (92%)] Loss: 3.286695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [471040/500434 (94%)] Loss: 3.253278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [481280/500434 (96%)] Loss: 3.221185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [491520/500434 (98%)] Loss: 3.297997\u001b[0m\n",
      "\u001b[34mcurrent epoch: 3\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [0/500434 (0%)] Loss: 3.202277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10240/500434 (2%)] Loss: 3.148102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [20480/500434 (4%)] Loss: 3.273074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30720/500434 (6%)] Loss: 3.195153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40960/500434 (8%)] Loss: 3.191624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/500434 (10%)] Loss: 3.184182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [61440/500434 (12%)] Loss: 3.117959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [71680/500434 (14%)] Loss: 3.210021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [81920/500434 (16%)] Loss: 3.067453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [92160/500434 (18%)] Loss: 3.163930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [102400/500434 (20%)] Loss: 3.079403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [112640/500434 (22%)] Loss: 3.223573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [122880/500434 (25%)] Loss: 3.106771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [133120/500434 (27%)] Loss: 3.058359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [143360/500434 (29%)] Loss: 3.078099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [153600/500434 (31%)] Loss: 3.029309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [163840/500434 (33%)] Loss: 3.023510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [174080/500434 (35%)] Loss: 2.955924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [184320/500434 (37%)] Loss: 3.030483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [194560/500434 (39%)] Loss: 2.987152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [204800/500434 (41%)] Loss: 2.957024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [215040/500434 (43%)] Loss: 3.007086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [225280/500434 (45%)] Loss: 2.926629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [235520/500434 (47%)] Loss: 3.034731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [245760/500434 (49%)] Loss: 2.926010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [256000/500434 (51%)] Loss: 2.929816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [266240/500434 (53%)] Loss: 2.903353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [276480/500434 (55%)] Loss: 2.825906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [286720/500434 (57%)] Loss: 3.007666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [296960/500434 (59%)] Loss: 2.873327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [307200/500434 (61%)] Loss: 2.898262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [317440/500434 (63%)] Loss: 2.993830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [327680/500434 (65%)] Loss: 2.943477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [337920/500434 (67%)] Loss: 2.920460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [348160/500434 (70%)] Loss: 2.828027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [358400/500434 (72%)] Loss: 2.873248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [368640/500434 (74%)] Loss: 2.863443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [378880/500434 (76%)] Loss: 2.796643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [389120/500434 (78%)] Loss: 2.913652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [399360/500434 (80%)] Loss: 2.913808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [409600/500434 (82%)] Loss: 2.846725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [419840/500434 (84%)] Loss: 2.913897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [430080/500434 (86%)] Loss: 2.785225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [440320/500434 (88%)] Loss: 2.850565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [450560/500434 (90%)] Loss: 2.825143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [460800/500434 (92%)] Loss: 2.773696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [471040/500434 (94%)] Loss: 2.808629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [481280/500434 (96%)] Loss: 2.766972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [491520/500434 (98%)] Loss: 2.649501\u001b[0m\n",
      "\u001b[34mcurrent epoch: 4\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [0/500434 (0%)] Loss: 2.874814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10240/500434 (2%)] Loss: 2.755586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [20480/500434 (4%)] Loss: 2.748180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30720/500434 (6%)] Loss: 2.797363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40960/500434 (8%)] Loss: 2.795155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/500434 (10%)] Loss: 2.854885\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [61440/500434 (12%)] Loss: 2.780301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [71680/500434 (14%)] Loss: 2.759612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [81920/500434 (16%)] Loss: 2.685080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [92160/500434 (18%)] Loss: 2.771440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [102400/500434 (20%)] Loss: 2.732311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [112640/500434 (22%)] Loss: 2.686395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [122880/500434 (25%)] Loss: 2.783590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [133120/500434 (27%)] Loss: 2.746675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [143360/500434 (29%)] Loss: 2.819495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [153600/500434 (31%)] Loss: 2.726661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [163840/500434 (33%)] Loss: 2.716009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [174080/500434 (35%)] Loss: 2.734596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [184320/500434 (37%)] Loss: 2.802506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [194560/500434 (39%)] Loss: 2.760311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [204800/500434 (41%)] Loss: 2.852530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [215040/500434 (43%)] Loss: 2.695232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [225280/500434 (45%)] Loss: 2.796019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [235520/500434 (47%)] Loss: 2.756867\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [245760/500434 (49%)] Loss: 2.752037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [256000/500434 (51%)] Loss: 2.659889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [266240/500434 (53%)] Loss: 2.714177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [276480/500434 (55%)] Loss: 2.787633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [286720/500434 (57%)] Loss: 2.693450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [296960/500434 (59%)] Loss: 2.683108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [307200/500434 (61%)] Loss: 2.684806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [317440/500434 (63%)] Loss: 2.665457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [327680/500434 (65%)] Loss: 2.735985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [337920/500434 (67%)] Loss: 2.641734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [348160/500434 (70%)] Loss: 2.675595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [358400/500434 (72%)] Loss: 2.706837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [368640/500434 (74%)] Loss: 2.602285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [378880/500434 (76%)] Loss: 2.913762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [389120/500434 (78%)] Loss: 2.690046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [399360/500434 (80%)] Loss: 2.675957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [409600/500434 (82%)] Loss: 2.661719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [419840/500434 (84%)] Loss: 2.590728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [430080/500434 (86%)] Loss: 2.681557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [440320/500434 (88%)] Loss: 2.718960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [450560/500434 (90%)] Loss: 2.574649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [460800/500434 (92%)] Loss: 2.603422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [471040/500434 (94%)] Loss: 2.736068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [481280/500434 (96%)] Loss: 2.550409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [491520/500434 (98%)] Loss: 2.609581\u001b[0m\n",
      "\u001b[34mcurrent epoch: 5\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [0/500434 (0%)] Loss: 2.599432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10240/500434 (2%)] Loss: 2.648192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [20480/500434 (4%)] Loss: 2.564863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30720/500434 (6%)] Loss: 2.535246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40960/500434 (8%)] Loss: 2.506174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/500434 (10%)] Loss: 2.597890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [61440/500434 (12%)] Loss: 2.558934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [71680/500434 (14%)] Loss: 2.516820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [81920/500434 (16%)] Loss: 2.643466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [92160/500434 (18%)] Loss: 2.643086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [102400/500434 (20%)] Loss: 2.581185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [112640/500434 (22%)] Loss: 2.515792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [122880/500434 (25%)] Loss: 2.563070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [133120/500434 (27%)] Loss: 2.622296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [143360/500434 (29%)] Loss: 2.616305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [153600/500434 (31%)] Loss: 2.680572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [163840/500434 (33%)] Loss: 2.579308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [174080/500434 (35%)] Loss: 2.610542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [184320/500434 (37%)] Loss: 2.910147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [194560/500434 (39%)] Loss: 2.514906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [204800/500434 (41%)] Loss: 2.637378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [215040/500434 (43%)] Loss: 2.562116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [225280/500434 (45%)] Loss: 2.496411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [235520/500434 (47%)] Loss: 2.569674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [245760/500434 (49%)] Loss: 2.623334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [256000/500434 (51%)] Loss: 2.627736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [266240/500434 (53%)] Loss: 2.612144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [276480/500434 (55%)] Loss: 2.548962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [286720/500434 (57%)] Loss: 2.497880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [296960/500434 (59%)] Loss: 2.523649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [307200/500434 (61%)] Loss: 2.569703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [317440/500434 (63%)] Loss: 2.655391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [327680/500434 (65%)] Loss: 2.543776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [337920/500434 (67%)] Loss: 2.603494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [348160/500434 (70%)] Loss: 2.547580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [358400/500434 (72%)] Loss: 2.590917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [368640/500434 (74%)] Loss: 2.570260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [378880/500434 (76%)] Loss: 2.631126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [389120/500434 (78%)] Loss: 2.555136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [399360/500434 (80%)] Loss: 2.633862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [409600/500434 (82%)] Loss: 2.667825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [419840/500434 (84%)] Loss: 2.536800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [430080/500434 (86%)] Loss: 2.469433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [440320/500434 (88%)] Loss: 2.609875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [450560/500434 (90%)] Loss: 2.479825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [460800/500434 (92%)] Loss: 2.465551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [471040/500434 (94%)] Loss: 2.454045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [481280/500434 (96%)] Loss: 2.530712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [491520/500434 (98%)] Loss: 2.456769\u001b[0m\n",
      "\u001b[34mcurrent epoch: 6\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [0/500434 (0%)] Loss: 2.577924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [10240/500434 (2%)] Loss: 2.519536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [20480/500434 (4%)] Loss: 2.494145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [30720/500434 (6%)] Loss: 2.496258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [40960/500434 (8%)] Loss: 2.513262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/500434 (10%)] Loss: 2.484375\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [61440/500434 (12%)] Loss: 2.477069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [71680/500434 (14%)] Loss: 2.582706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [81920/500434 (16%)] Loss: 2.517774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [92160/500434 (18%)] Loss: 2.479520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [102400/500434 (20%)] Loss: 2.391962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [112640/500434 (22%)] Loss: 2.516735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [122880/500434 (25%)] Loss: 2.488332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [133120/500434 (27%)] Loss: 2.496515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [143360/500434 (29%)] Loss: 2.398783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [153600/500434 (31%)] Loss: 2.571559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [163840/500434 (33%)] Loss: 2.392854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [174080/500434 (35%)] Loss: 2.487306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [184320/500434 (37%)] Loss: 2.497441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [194560/500434 (39%)] Loss: 2.443424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [204800/500434 (41%)] Loss: 2.462234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [215040/500434 (43%)] Loss: 2.424553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [225280/500434 (45%)] Loss: 2.548125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [235520/500434 (47%)] Loss: 2.341641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [245760/500434 (49%)] Loss: 2.392127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [256000/500434 (51%)] Loss: 2.472852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [266240/500434 (53%)] Loss: 2.572537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [276480/500434 (55%)] Loss: 2.512579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [286720/500434 (57%)] Loss: 2.405987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [296960/500434 (59%)] Loss: 2.395480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [307200/500434 (61%)] Loss: 2.408430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [317440/500434 (63%)] Loss: 2.450606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [327680/500434 (65%)] Loss: 2.397807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [337920/500434 (67%)] Loss: 2.374280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [348160/500434 (70%)] Loss: 2.479472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [358400/500434 (72%)] Loss: 2.497684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [368640/500434 (74%)] Loss: 2.462779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [378880/500434 (76%)] Loss: 2.448171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [389120/500434 (78%)] Loss: 2.453392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [399360/500434 (80%)] Loss: 2.370483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [409600/500434 (82%)] Loss: 2.462502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [419840/500434 (84%)] Loss: 2.443399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [430080/500434 (86%)] Loss: 2.522292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [440320/500434 (88%)] Loss: 2.350105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [450560/500434 (90%)] Loss: 2.484563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [460800/500434 (92%)] Loss: 2.504302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [471040/500434 (94%)] Loss: 2.431174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [481280/500434 (96%)] Loss: 2.373828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [491520/500434 (98%)] Loss: 2.523634\u001b[0m\n",
      "\u001b[34mcurrent epoch: 7\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [0/500434 (0%)] Loss: 2.409027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [10240/500434 (2%)] Loss: 2.299730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [20480/500434 (4%)] Loss: 2.406821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [30720/500434 (6%)] Loss: 2.406178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [40960/500434 (8%)] Loss: 2.363972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/500434 (10%)] Loss: 2.437882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [61440/500434 (12%)] Loss: 2.313609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [71680/500434 (14%)] Loss: 2.470733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [81920/500434 (16%)] Loss: 2.456822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [92160/500434 (18%)] Loss: 2.355999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [102400/500434 (20%)] Loss: 2.353817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [112640/500434 (22%)] Loss: 2.319252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [122880/500434 (25%)] Loss: 2.496347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [133120/500434 (27%)] Loss: 2.518128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [143360/500434 (29%)] Loss: 2.473408\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [153600/500434 (31%)] Loss: 2.462697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [163840/500434 (33%)] Loss: 2.379148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [174080/500434 (35%)] Loss: 2.378174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [184320/500434 (37%)] Loss: 2.381891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [194560/500434 (39%)] Loss: 2.207417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [204800/500434 (41%)] Loss: 2.333694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [215040/500434 (43%)] Loss: 2.359850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [225280/500434 (45%)] Loss: 2.483423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [235520/500434 (47%)] Loss: 2.375491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [245760/500434 (49%)] Loss: 2.344830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [256000/500434 (51%)] Loss: 2.309111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [266240/500434 (53%)] Loss: 2.354840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [276480/500434 (55%)] Loss: 2.365923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [286720/500434 (57%)] Loss: 2.434773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [296960/500434 (59%)] Loss: 2.442327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [307200/500434 (61%)] Loss: 2.381522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [317440/500434 (63%)] Loss: 2.396985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [327680/500434 (65%)] Loss: 2.372235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [337920/500434 (67%)] Loss: 2.424982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [348160/500434 (70%)] Loss: 2.370339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [358400/500434 (72%)] Loss: 2.364993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [368640/500434 (74%)] Loss: 2.402474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [378880/500434 (76%)] Loss: 2.348463\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [389120/500434 (78%)] Loss: 2.399858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [399360/500434 (80%)] Loss: 2.324349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [409600/500434 (82%)] Loss: 2.375460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [419840/500434 (84%)] Loss: 2.377923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [430080/500434 (86%)] Loss: 2.332329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [440320/500434 (88%)] Loss: 2.375822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [450560/500434 (90%)] Loss: 2.242846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [460800/500434 (92%)] Loss: 2.369609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [471040/500434 (94%)] Loss: 2.401897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [481280/500434 (96%)] Loss: 2.333894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [491520/500434 (98%)] Loss: 2.412368\u001b[0m\n",
      "\u001b[34mcurrent epoch: 8\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [0/500434 (0%)] Loss: 2.320925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [10240/500434 (2%)] Loss: 2.313701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [20480/500434 (4%)] Loss: 2.309801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [30720/500434 (6%)] Loss: 2.351059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [40960/500434 (8%)] Loss: 2.296971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/500434 (10%)] Loss: 2.390861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [61440/500434 (12%)] Loss: 2.464590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [71680/500434 (14%)] Loss: 2.451063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [81920/500434 (16%)] Loss: 2.271502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [92160/500434 (18%)] Loss: 2.412005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [102400/500434 (20%)] Loss: 2.295297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [112640/500434 (22%)] Loss: 2.461869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [122880/500434 (25%)] Loss: 2.331802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [133120/500434 (27%)] Loss: 2.357120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [143360/500434 (29%)] Loss: 2.578307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [153600/500434 (31%)] Loss: 2.306451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [163840/500434 (33%)] Loss: 2.468203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [174080/500434 (35%)] Loss: 2.345175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [184320/500434 (37%)] Loss: 2.406392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [194560/500434 (39%)] Loss: 2.338314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [204800/500434 (41%)] Loss: 2.211166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [215040/500434 (43%)] Loss: 2.378621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [225280/500434 (45%)] Loss: 2.394088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [235520/500434 (47%)] Loss: 2.366850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [245760/500434 (49%)] Loss: 2.378695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [256000/500434 (51%)] Loss: 2.230797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [266240/500434 (53%)] Loss: 2.339856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [276480/500434 (55%)] Loss: 2.312415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [286720/500434 (57%)] Loss: 2.348908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [296960/500434 (59%)] Loss: 2.382874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [307200/500434 (61%)] Loss: 2.407839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [317440/500434 (63%)] Loss: 2.389041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [327680/500434 (65%)] Loss: 2.248792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [337920/500434 (67%)] Loss: 2.234717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [348160/500434 (70%)] Loss: 2.298080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [358400/500434 (72%)] Loss: 2.292719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [368640/500434 (74%)] Loss: 2.185641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [378880/500434 (76%)] Loss: 2.333924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [389120/500434 (78%)] Loss: 2.227263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [399360/500434 (80%)] Loss: 2.314204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [409600/500434 (82%)] Loss: 2.226451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [419840/500434 (84%)] Loss: 2.361778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [430080/500434 (86%)] Loss: 2.382235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [440320/500434 (88%)] Loss: 2.322737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [450560/500434 (90%)] Loss: 2.322511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [460800/500434 (92%)] Loss: 2.225677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [471040/500434 (94%)] Loss: 2.248480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [481280/500434 (96%)] Loss: 2.389428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [491520/500434 (98%)] Loss: 2.355490\u001b[0m\n",
      "\u001b[34mcurrent epoch: 9\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [0/500434 (0%)] Loss: 2.200696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [10240/500434 (2%)] Loss: 2.216851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [20480/500434 (4%)] Loss: 2.367094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [30720/500434 (6%)] Loss: 2.257754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [40960/500434 (8%)] Loss: 2.285782\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/500434 (10%)] Loss: 2.304448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [61440/500434 (12%)] Loss: 2.230377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [71680/500434 (14%)] Loss: 2.286231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [81920/500434 (16%)] Loss: 2.246329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [92160/500434 (18%)] Loss: 2.263517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [102400/500434 (20%)] Loss: 2.269709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [112640/500434 (22%)] Loss: 2.350929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [122880/500434 (25%)] Loss: 2.365151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [133120/500434 (27%)] Loss: 2.303712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [143360/500434 (29%)] Loss: 2.272653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [153600/500434 (31%)] Loss: 2.334295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [163840/500434 (33%)] Loss: 2.245213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [174080/500434 (35%)] Loss: 2.311827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [184320/500434 (37%)] Loss: 2.304363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [194560/500434 (39%)] Loss: 2.276312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [204800/500434 (41%)] Loss: 2.274618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [215040/500434 (43%)] Loss: 2.302133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [225280/500434 (45%)] Loss: 2.292200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [235520/500434 (47%)] Loss: 2.262945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [245760/500434 (49%)] Loss: 2.179380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [256000/500434 (51%)] Loss: 2.233533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [266240/500434 (53%)] Loss: 2.299209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [276480/500434 (55%)] Loss: 2.271679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [286720/500434 (57%)] Loss: 2.208617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [296960/500434 (59%)] Loss: 2.230801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [307200/500434 (61%)] Loss: 2.198958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [317440/500434 (63%)] Loss: 2.236556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [327680/500434 (65%)] Loss: 2.285307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [337920/500434 (67%)] Loss: 2.301778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [348160/500434 (70%)] Loss: 2.255557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [358400/500434 (72%)] Loss: 2.274830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [368640/500434 (74%)] Loss: 2.260059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [378880/500434 (76%)] Loss: 2.233066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [389120/500434 (78%)] Loss: 2.179794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [399360/500434 (80%)] Loss: 2.266623\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [409600/500434 (82%)] Loss: 2.240382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [419840/500434 (84%)] Loss: 2.210764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [430080/500434 (86%)] Loss: 2.292398\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [440320/500434 (88%)] Loss: 2.261066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [450560/500434 (90%)] Loss: 2.277009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [460800/500434 (92%)] Loss: 2.191952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [471040/500434 (94%)] Loss: 2.282169\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [481280/500434 (96%)] Loss: 2.201982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [491520/500434 (98%)] Loss: 2.253395\u001b[0m\n",
      "\u001b[34mcurrent epoch: 10\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [0/500434 (0%)] Loss: 2.264068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [10240/500434 (2%)] Loss: 2.196980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [20480/500434 (4%)] Loss: 2.276186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [30720/500434 (6%)] Loss: 2.245076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [40960/500434 (8%)] Loss: 2.196576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/500434 (10%)] Loss: 2.266890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [61440/500434 (12%)] Loss: 2.149032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [71680/500434 (14%)] Loss: 2.158657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [81920/500434 (16%)] Loss: 2.157056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [92160/500434 (18%)] Loss: 2.132935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [102400/500434 (20%)] Loss: 2.207612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [112640/500434 (22%)] Loss: 2.261716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [122880/500434 (25%)] Loss: 2.265967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [133120/500434 (27%)] Loss: 2.279937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [143360/500434 (29%)] Loss: 2.171009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [153600/500434 (31%)] Loss: 2.263278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [163840/500434 (33%)] Loss: 2.373896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [174080/500434 (35%)] Loss: 2.312004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [184320/500434 (37%)] Loss: 2.189605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [194560/500434 (39%)] Loss: 2.286436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [204800/500434 (41%)] Loss: 2.239273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [215040/500434 (43%)] Loss: 2.208305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [225280/500434 (45%)] Loss: 2.346032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [235520/500434 (47%)] Loss: 2.139411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [245760/500434 (49%)] Loss: 2.192250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [256000/500434 (51%)] Loss: 2.278194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [266240/500434 (53%)] Loss: 2.390218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [276480/500434 (55%)] Loss: 2.414865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [286720/500434 (57%)] Loss: 2.393137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [296960/500434 (59%)] Loss: 2.249608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [307200/500434 (61%)] Loss: 2.273835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [317440/500434 (63%)] Loss: 2.367904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [327680/500434 (65%)] Loss: 2.318458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [337920/500434 (67%)] Loss: 2.355814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [348160/500434 (70%)] Loss: 2.311081\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [358400/500434 (72%)] Loss: 2.289870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [368640/500434 (74%)] Loss: 2.449361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [378880/500434 (76%)] Loss: 2.371175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [389120/500434 (78%)] Loss: 2.327986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [399360/500434 (80%)] Loss: 2.207976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [409600/500434 (82%)] Loss: 2.330867\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [419840/500434 (84%)] Loss: 2.198750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [430080/500434 (86%)] Loss: 2.252801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [440320/500434 (88%)] Loss: 2.345997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [450560/500434 (90%)] Loss: 2.364004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [460800/500434 (92%)] Loss: 2.237685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [471040/500434 (94%)] Loss: 2.228799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [481280/500434 (96%)] Loss: 2.194221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [491520/500434 (98%)] Loss: 2.432361\u001b[0m\n",
      "\u001b[34mcurrent epoch: 11\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [0/500434 (0%)] Loss: 2.326019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [10240/500434 (2%)] Loss: 2.343945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [20480/500434 (4%)] Loss: 2.335662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [30720/500434 (6%)] Loss: 2.242495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [40960/500434 (8%)] Loss: 2.259892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [51200/500434 (10%)] Loss: 2.222204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [61440/500434 (12%)] Loss: 2.226066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [71680/500434 (14%)] Loss: 2.228906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [81920/500434 (16%)] Loss: 2.258658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [92160/500434 (18%)] Loss: 2.267398\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [102400/500434 (20%)] Loss: 2.266164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [112640/500434 (22%)] Loss: 2.153373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [122880/500434 (25%)] Loss: 2.200072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [133120/500434 (27%)] Loss: 2.218782\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [143360/500434 (29%)] Loss: 2.287624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [153600/500434 (31%)] Loss: 2.123506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [163840/500434 (33%)] Loss: 2.216919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [174080/500434 (35%)] Loss: 2.255167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [184320/500434 (37%)] Loss: 2.264276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [194560/500434 (39%)] Loss: 2.230546\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [204800/500434 (41%)] Loss: 2.404056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [215040/500434 (43%)] Loss: 2.243489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [225280/500434 (45%)] Loss: 2.224586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [235520/500434 (47%)] Loss: 2.228151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [245760/500434 (49%)] Loss: 2.312185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [256000/500434 (51%)] Loss: 2.309519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [266240/500434 (53%)] Loss: 2.238101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [276480/500434 (55%)] Loss: 2.204416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [286720/500434 (57%)] Loss: 2.140146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [296960/500434 (59%)] Loss: 2.342994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [307200/500434 (61%)] Loss: 2.209715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [317440/500434 (63%)] Loss: 2.206059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [327680/500434 (65%)] Loss: 2.112970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [337920/500434 (67%)] Loss: 2.309845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [348160/500434 (70%)] Loss: 2.265225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [358400/500434 (72%)] Loss: 2.179589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [368640/500434 (74%)] Loss: 2.290142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [378880/500434 (76%)] Loss: 2.356519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [389120/500434 (78%)] Loss: 2.253621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [399360/500434 (80%)] Loss: 2.224639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [409600/500434 (82%)] Loss: 2.229863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [419840/500434 (84%)] Loss: 2.210425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [430080/500434 (86%)] Loss: 2.145823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [440320/500434 (88%)] Loss: 2.230649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [450560/500434 (90%)] Loss: 2.201597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [460800/500434 (92%)] Loss: 2.229154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [471040/500434 (94%)] Loss: 2.175761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [481280/500434 (96%)] Loss: 2.355927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [491520/500434 (98%)] Loss: 2.266853\u001b[0m\n",
      "\u001b[34mcurrent epoch: 12\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [0/500434 (0%)] Loss: 2.237136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [10240/500434 (2%)] Loss: 2.244980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [20480/500434 (4%)] Loss: 2.328510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [30720/500434 (6%)] Loss: 2.225590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [40960/500434 (8%)] Loss: 2.291316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [51200/500434 (10%)] Loss: 2.296764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [61440/500434 (12%)] Loss: 2.321878\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [71680/500434 (14%)] Loss: 2.284971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [81920/500434 (16%)] Loss: 2.161601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [92160/500434 (18%)] Loss: 2.219783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [102400/500434 (20%)] Loss: 2.231872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [112640/500434 (22%)] Loss: 2.214083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [122880/500434 (25%)] Loss: 2.269162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [133120/500434 (27%)] Loss: 2.207844\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [143360/500434 (29%)] Loss: 2.215746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [153600/500434 (31%)] Loss: 2.214299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [163840/500434 (33%)] Loss: 2.384094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [174080/500434 (35%)] Loss: 2.210287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [184320/500434 (37%)] Loss: 2.203126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [194560/500434 (39%)] Loss: 2.238368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [204800/500434 (41%)] Loss: 2.145919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [215040/500434 (43%)] Loss: 2.267230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [225280/500434 (45%)] Loss: 2.280964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [235520/500434 (47%)] Loss: 2.225887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [245760/500434 (49%)] Loss: 2.168096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [256000/500434 (51%)] Loss: 2.102764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [266240/500434 (53%)] Loss: 2.105943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [276480/500434 (55%)] Loss: 2.165051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [286720/500434 (57%)] Loss: 2.174386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [296960/500434 (59%)] Loss: 2.106130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [307200/500434 (61%)] Loss: 2.128523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [317440/500434 (63%)] Loss: 2.253510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [327680/500434 (65%)] Loss: 2.276201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [337920/500434 (67%)] Loss: 2.206413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [348160/500434 (70%)] Loss: 2.153264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [358400/500434 (72%)] Loss: 2.182701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [368640/500434 (74%)] Loss: 2.191323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [378880/500434 (76%)] Loss: 2.109755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [389120/500434 (78%)] Loss: 1.999918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [399360/500434 (80%)] Loss: 2.212870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [409600/500434 (82%)] Loss: 2.245565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [419840/500434 (84%)] Loss: 2.206710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [430080/500434 (86%)] Loss: 2.201326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [440320/500434 (88%)] Loss: 2.118178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [450560/500434 (90%)] Loss: 2.136193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [460800/500434 (92%)] Loss: 2.227312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [471040/500434 (94%)] Loss: 2.150444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [481280/500434 (96%)] Loss: 2.195666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [491520/500434 (98%)] Loss: 2.151377\u001b[0m\n",
      "\u001b[34mcurrent epoch: 13\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [0/500434 (0%)] Loss: 2.235664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [10240/500434 (2%)] Loss: 2.131708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [20480/500434 (4%)] Loss: 2.281263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [30720/500434 (6%)] Loss: 2.109274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [40960/500434 (8%)] Loss: 2.107470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [51200/500434 (10%)] Loss: 2.262174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [61440/500434 (12%)] Loss: 2.243057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [71680/500434 (14%)] Loss: 2.089525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [81920/500434 (16%)] Loss: 2.148168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [92160/500434 (18%)] Loss: 2.281419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [102400/500434 (20%)] Loss: 2.160834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [112640/500434 (22%)] Loss: 2.160838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [122880/500434 (25%)] Loss: 2.138835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [133120/500434 (27%)] Loss: 2.201467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [143360/500434 (29%)] Loss: 2.115958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [153600/500434 (31%)] Loss: 2.034399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [163840/500434 (33%)] Loss: 2.059732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [174080/500434 (35%)] Loss: 2.139135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [184320/500434 (37%)] Loss: 2.116325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [194560/500434 (39%)] Loss: 2.066908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [204800/500434 (41%)] Loss: 2.077800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [215040/500434 (43%)] Loss: 2.090467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [225280/500434 (45%)] Loss: 2.157843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [235520/500434 (47%)] Loss: 2.066232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [245760/500434 (49%)] Loss: 2.175642\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [256000/500434 (51%)] Loss: 2.133333\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [266240/500434 (53%)] Loss: 2.176518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [276480/500434 (55%)] Loss: 2.116617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [286720/500434 (57%)] Loss: 2.186832\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [296960/500434 (59%)] Loss: 2.140096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [307200/500434 (61%)] Loss: 1.988820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [317440/500434 (63%)] Loss: 2.056647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [327680/500434 (65%)] Loss: 2.125577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [337920/500434 (67%)] Loss: 2.161309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [348160/500434 (70%)] Loss: 2.145114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [358400/500434 (72%)] Loss: 2.146576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [368640/500434 (74%)] Loss: 2.080755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [378880/500434 (76%)] Loss: 2.161709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [389120/500434 (78%)] Loss: 2.135697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [399360/500434 (80%)] Loss: 2.079156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [409600/500434 (82%)] Loss: 2.125328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [419840/500434 (84%)] Loss: 2.179854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [430080/500434 (86%)] Loss: 2.155229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [440320/500434 (88%)] Loss: 2.223153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [450560/500434 (90%)] Loss: 2.042783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [460800/500434 (92%)] Loss: 2.105690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [471040/500434 (94%)] Loss: 2.220720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [481280/500434 (96%)] Loss: 2.008492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [491520/500434 (98%)] Loss: 2.183122\u001b[0m\n",
      "\u001b[34mcurrent epoch: 14\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [0/500434 (0%)] Loss: 2.179232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [10240/500434 (2%)] Loss: 2.069532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [20480/500434 (4%)] Loss: 2.103959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [30720/500434 (6%)] Loss: 2.116783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [40960/500434 (8%)] Loss: 2.057560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [51200/500434 (10%)] Loss: 2.157283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [61440/500434 (12%)] Loss: 2.105886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [71680/500434 (14%)] Loss: 2.058801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [81920/500434 (16%)] Loss: 2.088930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [92160/500434 (18%)] Loss: 2.043420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [102400/500434 (20%)] Loss: 2.045026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [112640/500434 (22%)] Loss: 2.062506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [122880/500434 (25%)] Loss: 2.063314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [133120/500434 (27%)] Loss: 1.996482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [143360/500434 (29%)] Loss: 2.192487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [153600/500434 (31%)] Loss: 2.097208\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [163840/500434 (33%)] Loss: 2.121441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [174080/500434 (35%)] Loss: 2.172159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [184320/500434 (37%)] Loss: 2.147228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [194560/500434 (39%)] Loss: 2.136400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [204800/500434 (41%)] Loss: 2.162908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [215040/500434 (43%)] Loss: 1.971989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [225280/500434 (45%)] Loss: 2.124826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [235520/500434 (47%)] Loss: 2.184141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [245760/500434 (49%)] Loss: 2.164358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [256000/500434 (51%)] Loss: 2.075038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [266240/500434 (53%)] Loss: 2.046447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [276480/500434 (55%)] Loss: 1.974894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [286720/500434 (57%)] Loss: 2.043886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [296960/500434 (59%)] Loss: 2.049748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [307200/500434 (61%)] Loss: 2.062529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [317440/500434 (63%)] Loss: 2.148865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [327680/500434 (65%)] Loss: 2.035792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [337920/500434 (67%)] Loss: 2.116953\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [348160/500434 (70%)] Loss: 2.248857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [358400/500434 (72%)] Loss: 2.126648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [368640/500434 (74%)] Loss: 2.130182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [378880/500434 (76%)] Loss: 2.056513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [389120/500434 (78%)] Loss: 2.149630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [399360/500434 (80%)] Loss: 2.035475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [409600/500434 (82%)] Loss: 2.069960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [419840/500434 (84%)] Loss: 2.092663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [430080/500434 (86%)] Loss: 2.098362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [440320/500434 (88%)] Loss: 2.028995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [450560/500434 (90%)] Loss: 2.101035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [460800/500434 (92%)] Loss: 2.225662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [471040/500434 (94%)] Loss: 2.119379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [481280/500434 (96%)] Loss: 2.118004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [491520/500434 (98%)] Loss: 2.140239\u001b[0m\n",
      "\u001b[34mcurrent epoch: 15\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [0/500434 (0%)] Loss: 2.054949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [10240/500434 (2%)] Loss: 2.081944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [20480/500434 (4%)] Loss: 2.133441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [30720/500434 (6%)] Loss: 2.188412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [40960/500434 (8%)] Loss: 2.075820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [51200/500434 (10%)] Loss: 2.070804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [61440/500434 (12%)] Loss: 2.139524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [71680/500434 (14%)] Loss: 2.000823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [81920/500434 (16%)] Loss: 2.024402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [92160/500434 (18%)] Loss: 2.100371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [102400/500434 (20%)] Loss: 2.126148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [112640/500434 (22%)] Loss: 2.113605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [122880/500434 (25%)] Loss: 2.173032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [133120/500434 (27%)] Loss: 2.044453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [143360/500434 (29%)] Loss: 1.954922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [153600/500434 (31%)] Loss: 2.143776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [163840/500434 (33%)] Loss: 2.047423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [174080/500434 (35%)] Loss: 2.012924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [184320/500434 (37%)] Loss: 2.031088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [194560/500434 (39%)] Loss: 1.985961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [204800/500434 (41%)] Loss: 2.006667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [215040/500434 (43%)] Loss: 1.971064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [225280/500434 (45%)] Loss: 2.003389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [235520/500434 (47%)] Loss: 2.041326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [245760/500434 (49%)] Loss: 2.024939\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [256000/500434 (51%)] Loss: 2.039879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [266240/500434 (53%)] Loss: 2.086530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [276480/500434 (55%)] Loss: 2.046555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [286720/500434 (57%)] Loss: 1.999884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [296960/500434 (59%)] Loss: 1.996797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [307200/500434 (61%)] Loss: 2.148810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [317440/500434 (63%)] Loss: 2.050269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [327680/500434 (65%)] Loss: 2.057113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [337920/500434 (67%)] Loss: 2.114083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [348160/500434 (70%)] Loss: 2.033292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [358400/500434 (72%)] Loss: 2.103453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [368640/500434 (74%)] Loss: 2.194760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [378880/500434 (76%)] Loss: 2.117776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [389120/500434 (78%)] Loss: 2.177356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [399360/500434 (80%)] Loss: 2.134872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [409600/500434 (82%)] Loss: 2.093887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [419840/500434 (84%)] Loss: 2.087184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [430080/500434 (86%)] Loss: 2.008430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [440320/500434 (88%)] Loss: 2.095234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [450560/500434 (90%)] Loss: 2.146094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [460800/500434 (92%)] Loss: 2.193551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [471040/500434 (94%)] Loss: 2.098504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [481280/500434 (96%)] Loss: 2.124171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [491520/500434 (98%)] Loss: 2.089133\u001b[0m\n",
      "\u001b[34mcurrent epoch: 16\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [0/500434 (0%)] Loss: 2.082269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [10240/500434 (2%)] Loss: 2.103386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [20480/500434 (4%)] Loss: 2.012235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [30720/500434 (6%)] Loss: 2.023210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [40960/500434 (8%)] Loss: 2.005560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [51200/500434 (10%)] Loss: 1.998082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [61440/500434 (12%)] Loss: 2.032333\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [71680/500434 (14%)] Loss: 2.010082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [81920/500434 (16%)] Loss: 1.899526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [92160/500434 (18%)] Loss: 2.114187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [102400/500434 (20%)] Loss: 2.054290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [112640/500434 (22%)] Loss: 2.075358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [122880/500434 (25%)] Loss: 2.093657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [133120/500434 (27%)] Loss: 2.001725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [143360/500434 (29%)] Loss: 2.085185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [153600/500434 (31%)] Loss: 2.044181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [163840/500434 (33%)] Loss: 2.087573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [174080/500434 (35%)] Loss: 2.014415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [184320/500434 (37%)] Loss: 2.054545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [194560/500434 (39%)] Loss: 2.019087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [204800/500434 (41%)] Loss: 1.932618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [215040/500434 (43%)] Loss: 1.965667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [225280/500434 (45%)] Loss: 2.046247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [235520/500434 (47%)] Loss: 2.046544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [245760/500434 (49%)] Loss: 1.977397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [256000/500434 (51%)] Loss: 2.148496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [266240/500434 (53%)] Loss: 2.051399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [276480/500434 (55%)] Loss: 2.077055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [286720/500434 (57%)] Loss: 2.136721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [296960/500434 (59%)] Loss: 2.002901\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [307200/500434 (61%)] Loss: 2.031829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [317440/500434 (63%)] Loss: 2.081636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [327680/500434 (65%)] Loss: 1.977400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [337920/500434 (67%)] Loss: 1.949551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [348160/500434 (70%)] Loss: 1.950205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [358400/500434 (72%)] Loss: 2.092481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [368640/500434 (74%)] Loss: 1.996207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [378880/500434 (76%)] Loss: 2.017699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [389120/500434 (78%)] Loss: 2.058295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [399360/500434 (80%)] Loss: 2.165273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [409600/500434 (82%)] Loss: 2.087420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [419840/500434 (84%)] Loss: 2.030289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [430080/500434 (86%)] Loss: 2.164811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [440320/500434 (88%)] Loss: 2.183399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [450560/500434 (90%)] Loss: 2.042284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [460800/500434 (92%)] Loss: 2.012446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [471040/500434 (94%)] Loss: 2.135488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [481280/500434 (96%)] Loss: 1.916369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [491520/500434 (98%)] Loss: 2.001606\u001b[0m\n",
      "\u001b[34mcurrent epoch: 17\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [0/500434 (0%)] Loss: 2.018376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [10240/500434 (2%)] Loss: 1.975975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [20480/500434 (4%)] Loss: 1.945672\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [30720/500434 (6%)] Loss: 1.964405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [40960/500434 (8%)] Loss: 1.972926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [51200/500434 (10%)] Loss: 2.079011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [61440/500434 (12%)] Loss: 2.025993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [71680/500434 (14%)] Loss: 2.016889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [81920/500434 (16%)] Loss: 2.014380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [92160/500434 (18%)] Loss: 1.968604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [102400/500434 (20%)] Loss: 2.022330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [112640/500434 (22%)] Loss: 1.909784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [122880/500434 (25%)] Loss: 2.019886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [133120/500434 (27%)] Loss: 1.972187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [143360/500434 (29%)] Loss: 2.060424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [153600/500434 (31%)] Loss: 2.095362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [163840/500434 (33%)] Loss: 2.061646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [174080/500434 (35%)] Loss: 1.966520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [184320/500434 (37%)] Loss: 1.981663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [194560/500434 (39%)] Loss: 2.121061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [204800/500434 (41%)] Loss: 1.975202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [215040/500434 (43%)] Loss: 2.082211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [225280/500434 (45%)] Loss: 2.070053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [235520/500434 (47%)] Loss: 2.020227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [245760/500434 (49%)] Loss: 1.948868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [256000/500434 (51%)] Loss: 2.043211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [266240/500434 (53%)] Loss: 2.135057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [276480/500434 (55%)] Loss: 1.977323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [286720/500434 (57%)] Loss: 2.064706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [296960/500434 (59%)] Loss: 2.098283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [307200/500434 (61%)] Loss: 2.035295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [317440/500434 (63%)] Loss: 2.043577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [327680/500434 (65%)] Loss: 2.074544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [337920/500434 (67%)] Loss: 1.980051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [348160/500434 (70%)] Loss: 2.092899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [358400/500434 (72%)] Loss: 1.978178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [368640/500434 (74%)] Loss: 2.095498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [378880/500434 (76%)] Loss: 2.028018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [389120/500434 (78%)] Loss: 1.949262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [399360/500434 (80%)] Loss: 1.958537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [409600/500434 (82%)] Loss: 1.969299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [419840/500434 (84%)] Loss: 1.954461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [430080/500434 (86%)] Loss: 1.903235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [440320/500434 (88%)] Loss: 2.021151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [450560/500434 (90%)] Loss: 2.070776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [460800/500434 (92%)] Loss: 1.888015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [471040/500434 (94%)] Loss: 2.015054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [481280/500434 (96%)] Loss: 1.928248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [491520/500434 (98%)] Loss: 2.007211\u001b[0m\n",
      "\u001b[34mcurrent epoch: 18\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [0/500434 (0%)] Loss: 2.032538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [10240/500434 (2%)] Loss: 1.974218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [20480/500434 (4%)] Loss: 2.068771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [30720/500434 (6%)] Loss: 2.059700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [40960/500434 (8%)] Loss: 1.957848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [51200/500434 (10%)] Loss: 2.083484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [61440/500434 (12%)] Loss: 2.009338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [71680/500434 (14%)] Loss: 1.985109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [81920/500434 (16%)] Loss: 2.002040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [92160/500434 (18%)] Loss: 1.920745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [102400/500434 (20%)] Loss: 1.991643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [112640/500434 (22%)] Loss: 2.004482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [122880/500434 (25%)] Loss: 2.009748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [133120/500434 (27%)] Loss: 1.974707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [143360/500434 (29%)] Loss: 2.002567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [153600/500434 (31%)] Loss: 1.914744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [163840/500434 (33%)] Loss: 2.081414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [174080/500434 (35%)] Loss: 2.018843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [184320/500434 (37%)] Loss: 2.008940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [194560/500434 (39%)] Loss: 1.972996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [204800/500434 (41%)] Loss: 1.979870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [215040/500434 (43%)] Loss: 2.093316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [225280/500434 (45%)] Loss: 1.871675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [235520/500434 (47%)] Loss: 1.876176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [245760/500434 (49%)] Loss: 2.011623\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [256000/500434 (51%)] Loss: 1.990096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [266240/500434 (53%)] Loss: 1.972400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [276480/500434 (55%)] Loss: 2.040168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [286720/500434 (57%)] Loss: 2.040769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [296960/500434 (59%)] Loss: 2.109969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [307200/500434 (61%)] Loss: 2.030813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [317440/500434 (63%)] Loss: 2.066480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [327680/500434 (65%)] Loss: 2.026009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [337920/500434 (67%)] Loss: 1.995994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [348160/500434 (70%)] Loss: 2.011575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [358400/500434 (72%)] Loss: 2.015707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [368640/500434 (74%)] Loss: 2.024478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [378880/500434 (76%)] Loss: 1.975712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [389120/500434 (78%)] Loss: 1.949857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [399360/500434 (80%)] Loss: 1.985537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [409600/500434 (82%)] Loss: 2.029963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [419840/500434 (84%)] Loss: 2.062042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [430080/500434 (86%)] Loss: 1.964499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [440320/500434 (88%)] Loss: 2.047364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [450560/500434 (90%)] Loss: 1.969336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [460800/500434 (92%)] Loss: 1.970506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [471040/500434 (94%)] Loss: 1.972265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [481280/500434 (96%)] Loss: 1.930668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [491520/500434 (98%)] Loss: 1.956777\u001b[0m\n",
      "\u001b[34mcurrent epoch: 19\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [0/500434 (0%)] Loss: 1.945247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [10240/500434 (2%)] Loss: 2.085109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [20480/500434 (4%)] Loss: 1.911946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [30720/500434 (6%)] Loss: 1.978678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [40960/500434 (8%)] Loss: 1.928383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [51200/500434 (10%)] Loss: 1.997209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [61440/500434 (12%)] Loss: 1.890996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [71680/500434 (14%)] Loss: 1.982958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [81920/500434 (16%)] Loss: 1.839528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [92160/500434 (18%)] Loss: 1.957071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [102400/500434 (20%)] Loss: 2.212643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [112640/500434 (22%)] Loss: 1.921011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [122880/500434 (25%)] Loss: 2.017555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [133120/500434 (27%)] Loss: 2.038319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [143360/500434 (29%)] Loss: 1.974302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [153600/500434 (31%)] Loss: 1.925709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [163840/500434 (33%)] Loss: 1.934115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [174080/500434 (35%)] Loss: 1.848446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [184320/500434 (37%)] Loss: 1.972456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [194560/500434 (39%)] Loss: 2.159651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [204800/500434 (41%)] Loss: 2.060084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [215040/500434 (43%)] Loss: 1.947290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [225280/500434 (45%)] Loss: 1.985719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [235520/500434 (47%)] Loss: 2.057425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [245760/500434 (49%)] Loss: 2.065786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [256000/500434 (51%)] Loss: 2.008370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [266240/500434 (53%)] Loss: 2.007164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [276480/500434 (55%)] Loss: 1.906713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [286720/500434 (57%)] Loss: 1.923173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [296960/500434 (59%)] Loss: 2.001827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [307200/500434 (61%)] Loss: 2.111689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [317440/500434 (63%)] Loss: 1.967503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [327680/500434 (65%)] Loss: 1.965565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [337920/500434 (67%)] Loss: 2.009495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [348160/500434 (70%)] Loss: 1.989557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [358400/500434 (72%)] Loss: 2.120375\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [368640/500434 (74%)] Loss: 2.049800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [378880/500434 (76%)] Loss: 1.908078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [389120/500434 (78%)] Loss: 1.981979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [399360/500434 (80%)] Loss: 1.891056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [409600/500434 (82%)] Loss: 1.989723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [419840/500434 (84%)] Loss: 1.960860\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [430080/500434 (86%)] Loss: 1.988077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [440320/500434 (88%)] Loss: 2.064803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [450560/500434 (90%)] Loss: 1.908248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [460800/500434 (92%)] Loss: 2.031389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [471040/500434 (94%)] Loss: 1.870594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [481280/500434 (96%)] Loss: 1.935685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [491520/500434 (98%)] Loss: 2.000968\u001b[0m\n",
      "\u001b[34mcurrent epoch: 20\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [0/500434 (0%)] Loss: 1.914021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [10240/500434 (2%)] Loss: 1.911389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [20480/500434 (4%)] Loss: 1.867105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [30720/500434 (6%)] Loss: 1.967037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [40960/500434 (8%)] Loss: 2.011115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [51200/500434 (10%)] Loss: 1.873641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [61440/500434 (12%)] Loss: 1.936262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [71680/500434 (14%)] Loss: 1.967800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [81920/500434 (16%)] Loss: 1.959993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [92160/500434 (18%)] Loss: 2.032781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [102400/500434 (20%)] Loss: 2.008611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [112640/500434 (22%)] Loss: 1.943269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [122880/500434 (25%)] Loss: 1.962713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [133120/500434 (27%)] Loss: 2.002100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [143360/500434 (29%)] Loss: 1.945868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [153600/500434 (31%)] Loss: 1.937557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [163840/500434 (33%)] Loss: 1.944291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [174080/500434 (35%)] Loss: 2.041119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [184320/500434 (37%)] Loss: 2.042598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [194560/500434 (39%)] Loss: 1.941504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [204800/500434 (41%)] Loss: 1.903242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [215040/500434 (43%)] Loss: 2.069019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [225280/500434 (45%)] Loss: 1.887307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [235520/500434 (47%)] Loss: 1.899334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [245760/500434 (49%)] Loss: 1.942669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [256000/500434 (51%)] Loss: 1.906485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [266240/500434 (53%)] Loss: 1.982624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [276480/500434 (55%)] Loss: 2.025346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [286720/500434 (57%)] Loss: 1.900492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [296960/500434 (59%)] Loss: 1.988206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [307200/500434 (61%)] Loss: 1.976946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [317440/500434 (63%)] Loss: 1.995414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [327680/500434 (65%)] Loss: 2.003611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [337920/500434 (67%)] Loss: 1.860127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [348160/500434 (70%)] Loss: 1.949804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [358400/500434 (72%)] Loss: 1.852734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [368640/500434 (74%)] Loss: 1.860610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [378880/500434 (76%)] Loss: 1.861851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [389120/500434 (78%)] Loss: 1.999275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [399360/500434 (80%)] Loss: 2.012477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [409600/500434 (82%)] Loss: 1.916721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [419840/500434 (84%)] Loss: 1.946951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [430080/500434 (86%)] Loss: 1.856360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [440320/500434 (88%)] Loss: 1.978150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [450560/500434 (90%)] Loss: 1.909458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [460800/500434 (92%)] Loss: 1.978826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [471040/500434 (94%)] Loss: 1.934147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [481280/500434 (96%)] Loss: 1.947359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [491520/500434 (98%)] Loss: 1.904631\u001b[0m\n",
      "\u001b[34mcurrent epoch: 21\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [0/500434 (0%)] Loss: 1.893260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [10240/500434 (2%)] Loss: 1.947026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [20480/500434 (4%)] Loss: 1.975258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [30720/500434 (6%)] Loss: 1.892165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [40960/500434 (8%)] Loss: 1.978152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [51200/500434 (10%)] Loss: 1.890666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [61440/500434 (12%)] Loss: 2.032476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [71680/500434 (14%)] Loss: 1.904277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [81920/500434 (16%)] Loss: 2.025456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [92160/500434 (18%)] Loss: 1.950785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [102400/500434 (20%)] Loss: 2.056703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [112640/500434 (22%)] Loss: 1.983707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [122880/500434 (25%)] Loss: 1.905277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [133120/500434 (27%)] Loss: 1.921780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [143360/500434 (29%)] Loss: 2.063860\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [153600/500434 (31%)] Loss: 2.094694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [163840/500434 (33%)] Loss: 1.914759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [174080/500434 (35%)] Loss: 1.903416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [184320/500434 (37%)] Loss: 1.933371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [194560/500434 (39%)] Loss: 1.951349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [204800/500434 (41%)] Loss: 1.898148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [215040/500434 (43%)] Loss: 1.917551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [225280/500434 (45%)] Loss: 2.024034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [235520/500434 (47%)] Loss: 1.951731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [245760/500434 (49%)] Loss: 1.905619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [256000/500434 (51%)] Loss: 1.910232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [266240/500434 (53%)] Loss: 1.949467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [276480/500434 (55%)] Loss: 2.044324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [286720/500434 (57%)] Loss: 1.914631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [296960/500434 (59%)] Loss: 1.906562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [307200/500434 (61%)] Loss: 2.040181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [317440/500434 (63%)] Loss: 2.014799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [327680/500434 (65%)] Loss: 2.064902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [337920/500434 (67%)] Loss: 1.969866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [348160/500434 (70%)] Loss: 1.914718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [358400/500434 (72%)] Loss: 1.916957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [368640/500434 (74%)] Loss: 1.843032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [378880/500434 (76%)] Loss: 1.927236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [389120/500434 (78%)] Loss: 2.006113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [399360/500434 (80%)] Loss: 1.939629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [409600/500434 (82%)] Loss: 1.991825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [419840/500434 (84%)] Loss: 1.955877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [430080/500434 (86%)] Loss: 1.865056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [440320/500434 (88%)] Loss: 1.908013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [450560/500434 (90%)] Loss: 1.958490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [460800/500434 (92%)] Loss: 1.799481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [471040/500434 (94%)] Loss: 1.902326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [481280/500434 (96%)] Loss: 1.870419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [491520/500434 (98%)] Loss: 1.854149\u001b[0m\n",
      "\u001b[34mcurrent epoch: 22\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [0/500434 (0%)] Loss: 1.936510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [10240/500434 (2%)] Loss: 1.886463\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [20480/500434 (4%)] Loss: 1.886420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [30720/500434 (6%)] Loss: 2.034426\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [40960/500434 (8%)] Loss: 2.032748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [51200/500434 (10%)] Loss: 1.915967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [61440/500434 (12%)] Loss: 1.969416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [71680/500434 (14%)] Loss: 1.940243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [81920/500434 (16%)] Loss: 2.018417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [92160/500434 (18%)] Loss: 2.108610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [102400/500434 (20%)] Loss: 2.048856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [112640/500434 (22%)] Loss: 1.942585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [122880/500434 (25%)] Loss: 1.824365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [133120/500434 (27%)] Loss: 1.939022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [143360/500434 (29%)] Loss: 1.859855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [153600/500434 (31%)] Loss: 1.839660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [163840/500434 (33%)] Loss: 1.986116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [174080/500434 (35%)] Loss: 1.880490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [184320/500434 (37%)] Loss: 1.881350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [194560/500434 (39%)] Loss: 1.905608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [204800/500434 (41%)] Loss: 1.940323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [215040/500434 (43%)] Loss: 1.880650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [225280/500434 (45%)] Loss: 1.996771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [235520/500434 (47%)] Loss: 1.908489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [245760/500434 (49%)] Loss: 1.927805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [256000/500434 (51%)] Loss: 1.944609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [266240/500434 (53%)] Loss: 1.966263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [276480/500434 (55%)] Loss: 1.880349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [286720/500434 (57%)] Loss: 2.011308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [296960/500434 (59%)] Loss: 1.891362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [307200/500434 (61%)] Loss: 1.791745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [317440/500434 (63%)] Loss: 1.921629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [327680/500434 (65%)] Loss: 1.971328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [337920/500434 (67%)] Loss: 2.034913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [348160/500434 (70%)] Loss: 1.991862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [358400/500434 (72%)] Loss: 1.903314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [368640/500434 (74%)] Loss: 1.878626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [378880/500434 (76%)] Loss: 1.811100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [389120/500434 (78%)] Loss: 1.909799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [399360/500434 (80%)] Loss: 2.018795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [409600/500434 (82%)] Loss: 1.890879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [419840/500434 (84%)] Loss: 1.872826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [430080/500434 (86%)] Loss: 1.917484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [440320/500434 (88%)] Loss: 1.907843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [450560/500434 (90%)] Loss: 1.913539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [460800/500434 (92%)] Loss: 1.968097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [471040/500434 (94%)] Loss: 1.973796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [481280/500434 (96%)] Loss: 1.834478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [491520/500434 (98%)] Loss: 1.886914\u001b[0m\n",
      "\u001b[34mcurrent epoch: 23\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [0/500434 (0%)] Loss: 1.857911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [10240/500434 (2%)] Loss: 1.904327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [20480/500434 (4%)] Loss: 1.891529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [30720/500434 (6%)] Loss: 1.863741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [40960/500434 (8%)] Loss: 2.029027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [51200/500434 (10%)] Loss: 1.804463\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [61440/500434 (12%)] Loss: 1.953344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [71680/500434 (14%)] Loss: 1.993471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [81920/500434 (16%)] Loss: 1.925923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [92160/500434 (18%)] Loss: 1.946890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [102400/500434 (20%)] Loss: 1.920539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [112640/500434 (22%)] Loss: 1.929709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [122880/500434 (25%)] Loss: 2.006252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [133120/500434 (27%)] Loss: 1.998116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [143360/500434 (29%)] Loss: 1.912255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [153600/500434 (31%)] Loss: 1.855903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [163840/500434 (33%)] Loss: 2.042223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [174080/500434 (35%)] Loss: 2.039964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [184320/500434 (37%)] Loss: 1.858357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [194560/500434 (39%)] Loss: 1.982136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [204800/500434 (41%)] Loss: 1.985929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [215040/500434 (43%)] Loss: 2.039367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [225280/500434 (45%)] Loss: 1.928441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [235520/500434 (47%)] Loss: 1.963936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [245760/500434 (49%)] Loss: 1.921299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [256000/500434 (51%)] Loss: 2.120419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [266240/500434 (53%)] Loss: 1.923934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [276480/500434 (55%)] Loss: 1.838884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [286720/500434 (57%)] Loss: 1.840360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [296960/500434 (59%)] Loss: 1.909244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [307200/500434 (61%)] Loss: 2.019038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [317440/500434 (63%)] Loss: 1.881335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [327680/500434 (65%)] Loss: 1.909124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [337920/500434 (67%)] Loss: 1.938092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [348160/500434 (70%)] Loss: 2.043622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [358400/500434 (72%)] Loss: 1.964146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [368640/500434 (74%)] Loss: 1.893102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [378880/500434 (76%)] Loss: 2.010071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [389120/500434 (78%)] Loss: 1.811925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [399360/500434 (80%)] Loss: 1.974373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [409600/500434 (82%)] Loss: 1.883824\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [419840/500434 (84%)] Loss: 1.836524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [430080/500434 (86%)] Loss: 1.899044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [440320/500434 (88%)] Loss: 1.972928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [450560/500434 (90%)] Loss: 1.942402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [460800/500434 (92%)] Loss: 1.855033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [471040/500434 (94%)] Loss: 1.905722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [481280/500434 (96%)] Loss: 2.063184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [491520/500434 (98%)] Loss: 1.874246\u001b[0m\n",
      "\u001b[34mcurrent epoch: 24\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [0/500434 (0%)] Loss: 1.893284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [10240/500434 (2%)] Loss: 1.852788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [20480/500434 (4%)] Loss: 1.911339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [30720/500434 (6%)] Loss: 1.819471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [40960/500434 (8%)] Loss: 1.887837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [51200/500434 (10%)] Loss: 1.918424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [61440/500434 (12%)] Loss: 1.890373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [71680/500434 (14%)] Loss: 1.938280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [81920/500434 (16%)] Loss: 1.951375\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [92160/500434 (18%)] Loss: 1.872480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [102400/500434 (20%)] Loss: 1.861069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [112640/500434 (22%)] Loss: 1.896392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [122880/500434 (25%)] Loss: 1.825274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [133120/500434 (27%)] Loss: 1.859446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [143360/500434 (29%)] Loss: 1.947441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [153600/500434 (31%)] Loss: 1.842265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [163840/500434 (33%)] Loss: 1.827477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [174080/500434 (35%)] Loss: 1.962317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [184320/500434 (37%)] Loss: 1.799817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [194560/500434 (39%)] Loss: 1.850634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [204800/500434 (41%)] Loss: 1.955144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [215040/500434 (43%)] Loss: 1.901507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [225280/500434 (45%)] Loss: 1.903825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [235520/500434 (47%)] Loss: 1.921304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [245760/500434 (49%)] Loss: 2.045452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [256000/500434 (51%)] Loss: 1.978756\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [266240/500434 (53%)] Loss: 1.883813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [276480/500434 (55%)] Loss: 1.863313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [286720/500434 (57%)] Loss: 1.858441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [296960/500434 (59%)] Loss: 1.897510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [307200/500434 (61%)] Loss: 1.691607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [317440/500434 (63%)] Loss: 1.874589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [327680/500434 (65%)] Loss: 1.897743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [337920/500434 (67%)] Loss: 1.932121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [348160/500434 (70%)] Loss: 1.954749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [358400/500434 (72%)] Loss: 1.818289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [368640/500434 (74%)] Loss: 1.848154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [378880/500434 (76%)] Loss: 1.860032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [389120/500434 (78%)] Loss: 1.892100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [399360/500434 (80%)] Loss: 1.984098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [409600/500434 (82%)] Loss: 1.912112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [419840/500434 (84%)] Loss: 2.065558\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [430080/500434 (86%)] Loss: 2.030302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [440320/500434 (88%)] Loss: 1.798275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [450560/500434 (90%)] Loss: 1.830404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [460800/500434 (92%)] Loss: 1.840673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [471040/500434 (94%)] Loss: 1.926126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [481280/500434 (96%)] Loss: 1.930161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [491520/500434 (98%)] Loss: 1.854562\u001b[0m\n",
      "\u001b[34mcurrent epoch: 25\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [0/500434 (0%)] Loss: 2.003412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [10240/500434 (2%)] Loss: 1.800388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [20480/500434 (4%)] Loss: 1.838858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [30720/500434 (6%)] Loss: 1.876256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [40960/500434 (8%)] Loss: 1.899687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [51200/500434 (10%)] Loss: 1.909253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [61440/500434 (12%)] Loss: 1.811956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [71680/500434 (14%)] Loss: 1.843722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [81920/500434 (16%)] Loss: 1.873465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [92160/500434 (18%)] Loss: 1.874596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [102400/500434 (20%)] Loss: 1.799882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [112640/500434 (22%)] Loss: 1.784820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [122880/500434 (25%)] Loss: 1.951350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [133120/500434 (27%)] Loss: 1.828507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [143360/500434 (29%)] Loss: 1.881053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [153600/500434 (31%)] Loss: 1.990385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [163840/500434 (33%)] Loss: 1.932807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [174080/500434 (35%)] Loss: 2.031133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [184320/500434 (37%)] Loss: 1.850249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [194560/500434 (39%)] Loss: 1.897836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [204800/500434 (41%)] Loss: 1.817822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [215040/500434 (43%)] Loss: 1.859989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [225280/500434 (45%)] Loss: 2.184133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [235520/500434 (47%)] Loss: 1.956730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [245760/500434 (49%)] Loss: 1.815859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [256000/500434 (51%)] Loss: 1.884655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [266240/500434 (53%)] Loss: 1.865416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [276480/500434 (55%)] Loss: 1.847423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [286720/500434 (57%)] Loss: 1.881409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [296960/500434 (59%)] Loss: 1.806156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [307200/500434 (61%)] Loss: 1.893739\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [317440/500434 (63%)] Loss: 1.736742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [327680/500434 (65%)] Loss: 1.911088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [337920/500434 (67%)] Loss: 1.913499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [348160/500434 (70%)] Loss: 1.821835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [358400/500434 (72%)] Loss: 1.970014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [368640/500434 (74%)] Loss: 1.815985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [378880/500434 (76%)] Loss: 1.954975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [389120/500434 (78%)] Loss: 1.896977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [399360/500434 (80%)] Loss: 1.883462\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [409600/500434 (82%)] Loss: 1.908370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [419840/500434 (84%)] Loss: 1.843184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [430080/500434 (86%)] Loss: 1.855615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [440320/500434 (88%)] Loss: 1.981090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [450560/500434 (90%)] Loss: 1.929189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [460800/500434 (92%)] Loss: 1.887194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [471040/500434 (94%)] Loss: 1.953483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [481280/500434 (96%)] Loss: 1.903312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [491520/500434 (98%)] Loss: 2.007314\u001b[0m\n",
      "\u001b[34mcurrent epoch: 26\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [0/500434 (0%)] Loss: 1.801629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [10240/500434 (2%)] Loss: 1.919759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [20480/500434 (4%)] Loss: 1.877458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [30720/500434 (6%)] Loss: 1.799938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [40960/500434 (8%)] Loss: 1.984791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [51200/500434 (10%)] Loss: 1.809505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [61440/500434 (12%)] Loss: 1.875866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [71680/500434 (14%)] Loss: 1.868652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [81920/500434 (16%)] Loss: 1.830614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [92160/500434 (18%)] Loss: 1.820062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [102400/500434 (20%)] Loss: 1.898761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [112640/500434 (22%)] Loss: 1.870294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [122880/500434 (25%)] Loss: 1.812488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [133120/500434 (27%)] Loss: 1.857085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [143360/500434 (29%)] Loss: 1.888929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [153600/500434 (31%)] Loss: 1.941802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [163840/500434 (33%)] Loss: 1.800013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [174080/500434 (35%)] Loss: 1.960042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [184320/500434 (37%)] Loss: 1.775394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [194560/500434 (39%)] Loss: 1.837691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [204800/500434 (41%)] Loss: 1.861966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [215040/500434 (43%)] Loss: 1.788800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [225280/500434 (45%)] Loss: 1.858160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [235520/500434 (47%)] Loss: 1.817187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [245760/500434 (49%)] Loss: 1.847696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [256000/500434 (51%)] Loss: 1.819821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [266240/500434 (53%)] Loss: 1.782878\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [276480/500434 (55%)] Loss: 1.966789\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [286720/500434 (57%)] Loss: 1.898975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [296960/500434 (59%)] Loss: 1.847342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [307200/500434 (61%)] Loss: 1.852681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [317440/500434 (63%)] Loss: 1.893185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [327680/500434 (65%)] Loss: 1.843150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [337920/500434 (67%)] Loss: 1.860597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [348160/500434 (70%)] Loss: 1.890859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [358400/500434 (72%)] Loss: 1.914893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [368640/500434 (74%)] Loss: 1.965991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [378880/500434 (76%)] Loss: 1.850788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [389120/500434 (78%)] Loss: 1.994780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [399360/500434 (80%)] Loss: 1.785652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [409600/500434 (82%)] Loss: 1.815781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [419840/500434 (84%)] Loss: 1.857944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [430080/500434 (86%)] Loss: 1.908556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [440320/500434 (88%)] Loss: 1.769219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [450560/500434 (90%)] Loss: 1.826163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [460800/500434 (92%)] Loss: 1.840570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [471040/500434 (94%)] Loss: 1.914841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [481280/500434 (96%)] Loss: 1.920853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [491520/500434 (98%)] Loss: 1.840731\u001b[0m\n",
      "\u001b[34mcurrent epoch: 27\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [0/500434 (0%)] Loss: 1.864687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [10240/500434 (2%)] Loss: 1.791115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [20480/500434 (4%)] Loss: 1.868219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [30720/500434 (6%)] Loss: 1.859648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [40960/500434 (8%)] Loss: 1.743652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [51200/500434 (10%)] Loss: 1.851825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [61440/500434 (12%)] Loss: 1.810273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [71680/500434 (14%)] Loss: 1.834545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [81920/500434 (16%)] Loss: 1.823051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [92160/500434 (18%)] Loss: 1.832583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [102400/500434 (20%)] Loss: 1.775152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [112640/500434 (22%)] Loss: 1.829099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [122880/500434 (25%)] Loss: 2.052316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [133120/500434 (27%)] Loss: 2.003258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [143360/500434 (29%)] Loss: 1.801579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [153600/500434 (31%)] Loss: 1.809291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [163840/500434 (33%)] Loss: 1.886803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [174080/500434 (35%)] Loss: 1.851441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [184320/500434 (37%)] Loss: 1.821200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [194560/500434 (39%)] Loss: 1.867816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [204800/500434 (41%)] Loss: 1.912231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [215040/500434 (43%)] Loss: 1.784090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [225280/500434 (45%)] Loss: 1.903486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [235520/500434 (47%)] Loss: 1.860075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [245760/500434 (49%)] Loss: 1.848786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [256000/500434 (51%)] Loss: 1.794160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [266240/500434 (53%)] Loss: 1.949928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [276480/500434 (55%)] Loss: 1.829494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [286720/500434 (57%)] Loss: 1.818900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [296960/500434 (59%)] Loss: 1.879419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [307200/500434 (61%)] Loss: 1.836061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [317440/500434 (63%)] Loss: 1.874162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [327680/500434 (65%)] Loss: 1.874335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [337920/500434 (67%)] Loss: 1.869503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [348160/500434 (70%)] Loss: 1.897243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [358400/500434 (72%)] Loss: 1.901216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [368640/500434 (74%)] Loss: 1.854236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [378880/500434 (76%)] Loss: 1.874745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [389120/500434 (78%)] Loss: 1.896365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [399360/500434 (80%)] Loss: 1.866360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [409600/500434 (82%)] Loss: 1.845803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [419840/500434 (84%)] Loss: 1.852010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [430080/500434 (86%)] Loss: 1.800580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [440320/500434 (88%)] Loss: 1.883911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [450560/500434 (90%)] Loss: 2.009906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [460800/500434 (92%)] Loss: 1.897529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [471040/500434 (94%)] Loss: 1.844839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [481280/500434 (96%)] Loss: 1.840224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [491520/500434 (98%)] Loss: 1.845974\u001b[0m\n",
      "\u001b[34mcurrent epoch: 28\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [0/500434 (0%)] Loss: 1.874473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [10240/500434 (2%)] Loss: 1.838911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [20480/500434 (4%)] Loss: 1.761744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [30720/500434 (6%)] Loss: 1.738251\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [40960/500434 (8%)] Loss: 1.893270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [51200/500434 (10%)] Loss: 1.818520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [61440/500434 (12%)] Loss: 1.880806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [71680/500434 (14%)] Loss: 1.813855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [81920/500434 (16%)] Loss: 1.909984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [92160/500434 (18%)] Loss: 1.848779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [102400/500434 (20%)] Loss: 1.835907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [112640/500434 (22%)] Loss: 1.821804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [122880/500434 (25%)] Loss: 1.937149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [133120/500434 (27%)] Loss: 1.875836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [143360/500434 (29%)] Loss: 1.751696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [153600/500434 (31%)] Loss: 1.768672\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [163840/500434 (33%)] Loss: 1.813973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [174080/500434 (35%)] Loss: 1.808584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [184320/500434 (37%)] Loss: 2.022089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [194560/500434 (39%)] Loss: 1.924055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [204800/500434 (41%)] Loss: 1.780435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [215040/500434 (43%)] Loss: 1.820563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [225280/500434 (45%)] Loss: 1.851973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [235520/500434 (47%)] Loss: 1.948052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [245760/500434 (49%)] Loss: 1.771009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [256000/500434 (51%)] Loss: 1.886116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [266240/500434 (53%)] Loss: 1.764735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [276480/500434 (55%)] Loss: 1.921700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [286720/500434 (57%)] Loss: 1.825955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [296960/500434 (59%)] Loss: 1.894006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [307200/500434 (61%)] Loss: 1.817214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [317440/500434 (63%)] Loss: 1.857195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [327680/500434 (65%)] Loss: 1.949917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [337920/500434 (67%)] Loss: 1.823456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [348160/500434 (70%)] Loss: 1.843213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [358400/500434 (72%)] Loss: 1.889581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [368640/500434 (74%)] Loss: 1.801079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [378880/500434 (76%)] Loss: 1.912004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [389120/500434 (78%)] Loss: 1.912336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [399360/500434 (80%)] Loss: 1.782744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [409600/500434 (82%)] Loss: 1.795357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [419840/500434 (84%)] Loss: 1.879153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [430080/500434 (86%)] Loss: 1.891933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [440320/500434 (88%)] Loss: 1.834342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [450560/500434 (90%)] Loss: 1.825612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [460800/500434 (92%)] Loss: 1.747938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [471040/500434 (94%)] Loss: 1.831100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [481280/500434 (96%)] Loss: 1.796708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [491520/500434 (98%)] Loss: 1.934089\u001b[0m\n",
      "\u001b[34mcurrent epoch: 29\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [0/500434 (0%)] Loss: 1.785593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [10240/500434 (2%)] Loss: 1.821650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [20480/500434 (4%)] Loss: 1.809736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [30720/500434 (6%)] Loss: 1.887226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [40960/500434 (8%)] Loss: 1.765253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [51200/500434 (10%)] Loss: 1.835439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [61440/500434 (12%)] Loss: 1.922681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [71680/500434 (14%)] Loss: 1.857601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [81920/500434 (16%)] Loss: 1.859250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [92160/500434 (18%)] Loss: 1.886943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [102400/500434 (20%)] Loss: 1.953470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [112640/500434 (22%)] Loss: 1.814380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [122880/500434 (25%)] Loss: 1.881273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [133120/500434 (27%)] Loss: 1.845898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [143360/500434 (29%)] Loss: 1.865087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [153600/500434 (31%)] Loss: 1.759995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [163840/500434 (33%)] Loss: 1.877274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [174080/500434 (35%)] Loss: 1.768317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [184320/500434 (37%)] Loss: 1.761612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [194560/500434 (39%)] Loss: 1.818253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [204800/500434 (41%)] Loss: 1.805423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [215040/500434 (43%)] Loss: 1.799799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [225280/500434 (45%)] Loss: 1.849388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [235520/500434 (47%)] Loss: 1.892603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [245760/500434 (49%)] Loss: 1.876083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [256000/500434 (51%)] Loss: 1.858924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [266240/500434 (53%)] Loss: 1.906512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [276480/500434 (55%)] Loss: 1.811671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [286720/500434 (57%)] Loss: 1.877319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [296960/500434 (59%)] Loss: 1.730179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [307200/500434 (61%)] Loss: 1.772216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [317440/500434 (63%)] Loss: 1.843504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [327680/500434 (65%)] Loss: 1.845076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [337920/500434 (67%)] Loss: 1.785885\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [348160/500434 (70%)] Loss: 1.761017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [358400/500434 (72%)] Loss: 1.902732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [368640/500434 (74%)] Loss: 1.929891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [378880/500434 (76%)] Loss: 1.882961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [389120/500434 (78%)] Loss: 1.769150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [399360/500434 (80%)] Loss: 1.911702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [409600/500434 (82%)] Loss: 1.776244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [419840/500434 (84%)] Loss: 1.858854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [430080/500434 (86%)] Loss: 1.798197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [440320/500434 (88%)] Loss: 1.744866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [450560/500434 (90%)] Loss: 1.832163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [460800/500434 (92%)] Loss: 1.786707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [471040/500434 (94%)] Loss: 1.858085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [481280/500434 (96%)] Loss: 1.714159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [491520/500434 (98%)] Loss: 1.818265\u001b[0m\n",
      "\u001b[34mcurrent epoch: 30\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [0/500434 (0%)] Loss: 1.825106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [10240/500434 (2%)] Loss: 1.860276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [20480/500434 (4%)] Loss: 1.879395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [30720/500434 (6%)] Loss: 1.838584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [40960/500434 (8%)] Loss: 1.806622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [51200/500434 (10%)] Loss: 1.820504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [61440/500434 (12%)] Loss: 1.810395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [71680/500434 (14%)] Loss: 1.769631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [81920/500434 (16%)] Loss: 1.849682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [92160/500434 (18%)] Loss: 1.814527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [102400/500434 (20%)] Loss: 1.730247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [112640/500434 (22%)] Loss: 1.894863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [122880/500434 (25%)] Loss: 1.820192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [133120/500434 (27%)] Loss: 1.774717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [143360/500434 (29%)] Loss: 1.879716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [153600/500434 (31%)] Loss: 1.714557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [163840/500434 (33%)] Loss: 1.783988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [174080/500434 (35%)] Loss: 1.958213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [184320/500434 (37%)] Loss: 1.732139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [194560/500434 (39%)] Loss: 1.784239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [204800/500434 (41%)] Loss: 1.878888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [215040/500434 (43%)] Loss: 1.755037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [225280/500434 (45%)] Loss: 1.712050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [235520/500434 (47%)] Loss: 1.789869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [245760/500434 (49%)] Loss: 1.760829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [256000/500434 (51%)] Loss: 1.825505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [266240/500434 (53%)] Loss: 1.762390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [276480/500434 (55%)] Loss: 1.824553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [286720/500434 (57%)] Loss: 1.825176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [296960/500434 (59%)] Loss: 1.811126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [307200/500434 (61%)] Loss: 1.751905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [317440/500434 (63%)] Loss: 1.786600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [327680/500434 (65%)] Loss: 1.781030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [337920/500434 (67%)] Loss: 1.820200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [348160/500434 (70%)] Loss: 1.714173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [358400/500434 (72%)] Loss: 1.874848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [368640/500434 (74%)] Loss: 1.735560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [378880/500434 (76%)] Loss: 1.824188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [389120/500434 (78%)] Loss: 1.817722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [399360/500434 (80%)] Loss: 1.798838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [409600/500434 (82%)] Loss: 1.833568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [419840/500434 (84%)] Loss: 1.742514\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [430080/500434 (86%)] Loss: 1.764089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [440320/500434 (88%)] Loss: 1.719322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [450560/500434 (90%)] Loss: 1.669317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [460800/500434 (92%)] Loss: 1.952789\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [471040/500434 (94%)] Loss: 1.819063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [481280/500434 (96%)] Loss: 1.754024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [491520/500434 (98%)] Loss: 1.804003\u001b[0m\n",
      "\u001b[34mcurrent epoch: 31\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [0/500434 (0%)] Loss: 1.733275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [10240/500434 (2%)] Loss: 1.868148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [20480/500434 (4%)] Loss: 1.785771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [30720/500434 (6%)] Loss: 1.804289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [40960/500434 (8%)] Loss: 1.796729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [51200/500434 (10%)] Loss: 1.786771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [61440/500434 (12%)] Loss: 1.920372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [71680/500434 (14%)] Loss: 1.811128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [81920/500434 (16%)] Loss: 1.839709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [92160/500434 (18%)] Loss: 1.716677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [102400/500434 (20%)] Loss: 1.775052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [112640/500434 (22%)] Loss: 1.737419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [122880/500434 (25%)] Loss: 1.817411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [133120/500434 (27%)] Loss: 1.791055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [143360/500434 (29%)] Loss: 1.878898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [153600/500434 (31%)] Loss: 1.693428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [163840/500434 (33%)] Loss: 1.707631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [174080/500434 (35%)] Loss: 1.729272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [184320/500434 (37%)] Loss: 1.884661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [194560/500434 (39%)] Loss: 1.825725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [204800/500434 (41%)] Loss: 1.825175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [215040/500434 (43%)] Loss: 1.887595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [225280/500434 (45%)] Loss: 1.749689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [235520/500434 (47%)] Loss: 1.709937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [245760/500434 (49%)] Loss: 1.854248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [256000/500434 (51%)] Loss: 1.810597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [266240/500434 (53%)] Loss: 1.783034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [276480/500434 (55%)] Loss: 1.871156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [286720/500434 (57%)] Loss: 1.826113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [296960/500434 (59%)] Loss: 1.734143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [307200/500434 (61%)] Loss: 1.635313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [317440/500434 (63%)] Loss: 1.786123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [327680/500434 (65%)] Loss: 1.724094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [337920/500434 (67%)] Loss: 1.789613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [348160/500434 (70%)] Loss: 1.707696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [358400/500434 (72%)] Loss: 1.845636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [368640/500434 (74%)] Loss: 1.851738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [378880/500434 (76%)] Loss: 1.720037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [389120/500434 (78%)] Loss: 1.829027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [399360/500434 (80%)] Loss: 1.785742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [409600/500434 (82%)] Loss: 1.877523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [419840/500434 (84%)] Loss: 1.813653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [430080/500434 (86%)] Loss: 1.773289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [440320/500434 (88%)] Loss: 1.854392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [450560/500434 (90%)] Loss: 1.835464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [460800/500434 (92%)] Loss: 1.802452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [471040/500434 (94%)] Loss: 1.817769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [481280/500434 (96%)] Loss: 1.671003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [491520/500434 (98%)] Loss: 1.782363\u001b[0m\n",
      "\u001b[34mcurrent epoch: 32\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [0/500434 (0%)] Loss: 1.675736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [10240/500434 (2%)] Loss: 1.756352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [20480/500434 (4%)] Loss: 1.789356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [30720/500434 (6%)] Loss: 1.741413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [40960/500434 (8%)] Loss: 1.754160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [51200/500434 (10%)] Loss: 1.831523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [61440/500434 (12%)] Loss: 1.725482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [71680/500434 (14%)] Loss: 1.673147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [81920/500434 (16%)] Loss: 1.838523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [92160/500434 (18%)] Loss: 1.823175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [102400/500434 (20%)] Loss: 1.775979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [112640/500434 (22%)] Loss: 1.835866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [122880/500434 (25%)] Loss: 1.619321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [133120/500434 (27%)] Loss: 1.647159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [143360/500434 (29%)] Loss: 1.684865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [153600/500434 (31%)] Loss: 1.778391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [163840/500434 (33%)] Loss: 1.797674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [174080/500434 (35%)] Loss: 1.698790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [184320/500434 (37%)] Loss: 1.797736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [194560/500434 (39%)] Loss: 1.812387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [204800/500434 (41%)] Loss: 1.753045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [215040/500434 (43%)] Loss: 1.750953\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [225280/500434 (45%)] Loss: 1.786335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [235520/500434 (47%)] Loss: 1.743714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [245760/500434 (49%)] Loss: 1.783172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [256000/500434 (51%)] Loss: 1.795439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [266240/500434 (53%)] Loss: 1.790861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [276480/500434 (55%)] Loss: 1.746232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [286720/500434 (57%)] Loss: 1.837881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [296960/500434 (59%)] Loss: 1.767118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [307200/500434 (61%)] Loss: 1.803203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [317440/500434 (63%)] Loss: 1.759709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [327680/500434 (65%)] Loss: 1.883471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [337920/500434 (67%)] Loss: 1.794545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [348160/500434 (70%)] Loss: 1.734700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [358400/500434 (72%)] Loss: 1.811070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [368640/500434 (74%)] Loss: 1.757507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [378880/500434 (76%)] Loss: 1.777525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [389120/500434 (78%)] Loss: 1.696579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [399360/500434 (80%)] Loss: 1.773441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [409600/500434 (82%)] Loss: 1.662262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [419840/500434 (84%)] Loss: 1.806033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [430080/500434 (86%)] Loss: 1.785087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [440320/500434 (88%)] Loss: 1.954578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [450560/500434 (90%)] Loss: 1.678078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [460800/500434 (92%)] Loss: 1.810281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [471040/500434 (94%)] Loss: 1.865728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [481280/500434 (96%)] Loss: 1.709378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [491520/500434 (98%)] Loss: 1.764430\u001b[0m\n",
      "\u001b[34mcurrent epoch: 33\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [0/500434 (0%)] Loss: 1.744622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [10240/500434 (2%)] Loss: 1.792985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [20480/500434 (4%)] Loss: 1.780367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [30720/500434 (6%)] Loss: 1.712836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [40960/500434 (8%)] Loss: 1.760277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [51200/500434 (10%)] Loss: 1.706270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [61440/500434 (12%)] Loss: 1.747584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [71680/500434 (14%)] Loss: 1.614578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [81920/500434 (16%)] Loss: 1.762148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [92160/500434 (18%)] Loss: 1.677537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [102400/500434 (20%)] Loss: 1.757302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [112640/500434 (22%)] Loss: 1.675319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [122880/500434 (25%)] Loss: 1.726319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [133120/500434 (27%)] Loss: 1.734541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [143360/500434 (29%)] Loss: 1.738521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [153600/500434 (31%)] Loss: 1.782635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [163840/500434 (33%)] Loss: 1.878938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [174080/500434 (35%)] Loss: 1.795106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [184320/500434 (37%)] Loss: 1.661459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [194560/500434 (39%)] Loss: 1.756770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [204800/500434 (41%)] Loss: 1.928252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [215040/500434 (43%)] Loss: 1.709541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [225280/500434 (45%)] Loss: 1.873242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [235520/500434 (47%)] Loss: 1.860922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [245760/500434 (49%)] Loss: 1.660318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [256000/500434 (51%)] Loss: 1.789984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [266240/500434 (53%)] Loss: 1.738844\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [276480/500434 (55%)] Loss: 1.682550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [286720/500434 (57%)] Loss: 1.731348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [296960/500434 (59%)] Loss: 1.729286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [307200/500434 (61%)] Loss: 1.786876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [317440/500434 (63%)] Loss: 1.712500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [327680/500434 (65%)] Loss: 1.773957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [337920/500434 (67%)] Loss: 1.682443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [348160/500434 (70%)] Loss: 1.711421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [358400/500434 (72%)] Loss: 1.594118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [368640/500434 (74%)] Loss: 1.729518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [378880/500434 (76%)] Loss: 1.728197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [389120/500434 (78%)] Loss: 1.785962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [399360/500434 (80%)] Loss: 1.692806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [409600/500434 (82%)] Loss: 1.709005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [419840/500434 (84%)] Loss: 1.822991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [430080/500434 (86%)] Loss: 1.920388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [440320/500434 (88%)] Loss: 1.731022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [450560/500434 (90%)] Loss: 1.634510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [460800/500434 (92%)] Loss: 1.814864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [471040/500434 (94%)] Loss: 1.714096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [481280/500434 (96%)] Loss: 1.704555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [491520/500434 (98%)] Loss: 1.812532\u001b[0m\n",
      "\u001b[34mcurrent epoch: 34\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [0/500434 (0%)] Loss: 1.769825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [10240/500434 (2%)] Loss: 1.683565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [20480/500434 (4%)] Loss: 1.817319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [30720/500434 (6%)] Loss: 1.778187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [40960/500434 (8%)] Loss: 1.995726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [51200/500434 (10%)] Loss: 1.670903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [61440/500434 (12%)] Loss: 1.741211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [71680/500434 (14%)] Loss: 1.781257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [81920/500434 (16%)] Loss: 1.752775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [92160/500434 (18%)] Loss: 1.718498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [102400/500434 (20%)] Loss: 1.684472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [112640/500434 (22%)] Loss: 1.749702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [122880/500434 (25%)] Loss: 1.719657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [133120/500434 (27%)] Loss: 1.811730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [143360/500434 (29%)] Loss: 1.774518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [153600/500434 (31%)] Loss: 1.883242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [163840/500434 (33%)] Loss: 1.769741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [174080/500434 (35%)] Loss: 1.747595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [184320/500434 (37%)] Loss: 1.731535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [194560/500434 (39%)] Loss: 1.720170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [204800/500434 (41%)] Loss: 1.777332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [215040/500434 (43%)] Loss: 1.719493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [225280/500434 (45%)] Loss: 1.710563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [235520/500434 (47%)] Loss: 1.789699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [245760/500434 (49%)] Loss: 1.821697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [256000/500434 (51%)] Loss: 1.815649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [266240/500434 (53%)] Loss: 1.818094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [276480/500434 (55%)] Loss: 1.678762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [286720/500434 (57%)] Loss: 1.805558\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [296960/500434 (59%)] Loss: 1.744017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [307200/500434 (61%)] Loss: 1.802542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [317440/500434 (63%)] Loss: 1.722227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [327680/500434 (65%)] Loss: 1.804088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [337920/500434 (67%)] Loss: 1.850802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [348160/500434 (70%)] Loss: 1.776853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [358400/500434 (72%)] Loss: 1.764851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [368640/500434 (74%)] Loss: 1.671908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [378880/500434 (76%)] Loss: 1.755367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [389120/500434 (78%)] Loss: 1.824834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [399360/500434 (80%)] Loss: 1.757582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [409600/500434 (82%)] Loss: 1.781296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [419840/500434 (84%)] Loss: 1.761315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [430080/500434 (86%)] Loss: 1.832752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [440320/500434 (88%)] Loss: 1.758388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [450560/500434 (90%)] Loss: 1.719207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [460800/500434 (92%)] Loss: 1.767335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [471040/500434 (94%)] Loss: 1.793612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [481280/500434 (96%)] Loss: 1.726995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [491520/500434 (98%)] Loss: 1.661937\u001b[0m\n",
      "\u001b[34mcurrent epoch: 35\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [0/500434 (0%)] Loss: 1.669602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [10240/500434 (2%)] Loss: 1.645171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [20480/500434 (4%)] Loss: 1.721649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [30720/500434 (6%)] Loss: 1.711922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [40960/500434 (8%)] Loss: 1.683214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [51200/500434 (10%)] Loss: 1.781905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [61440/500434 (12%)] Loss: 1.706129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [71680/500434 (14%)] Loss: 1.838933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [81920/500434 (16%)] Loss: 1.720568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [92160/500434 (18%)] Loss: 1.754150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [102400/500434 (20%)] Loss: 1.688480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [112640/500434 (22%)] Loss: 1.784187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [122880/500434 (25%)] Loss: 1.792406\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [133120/500434 (27%)] Loss: 1.783955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [143360/500434 (29%)] Loss: 1.787132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [153600/500434 (31%)] Loss: 1.706586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [163840/500434 (33%)] Loss: 1.721277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [174080/500434 (35%)] Loss: 1.641841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [184320/500434 (37%)] Loss: 1.727582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [194560/500434 (39%)] Loss: 1.813638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [204800/500434 (41%)] Loss: 1.672248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [215040/500434 (43%)] Loss: 1.748179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [225280/500434 (45%)] Loss: 1.796144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [235520/500434 (47%)] Loss: 1.801541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [245760/500434 (49%)] Loss: 1.789486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [256000/500434 (51%)] Loss: 1.800288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [266240/500434 (53%)] Loss: 1.765695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [276480/500434 (55%)] Loss: 1.815089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [286720/500434 (57%)] Loss: 1.792321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [296960/500434 (59%)] Loss: 1.824653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [307200/500434 (61%)] Loss: 1.831090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [317440/500434 (63%)] Loss: 1.739458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [327680/500434 (65%)] Loss: 1.706160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [337920/500434 (67%)] Loss: 1.676187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [348160/500434 (70%)] Loss: 1.703017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [358400/500434 (72%)] Loss: 1.810354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [368640/500434 (74%)] Loss: 1.800927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [378880/500434 (76%)] Loss: 1.779092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [389120/500434 (78%)] Loss: 1.792433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [399360/500434 (80%)] Loss: 1.799330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [409600/500434 (82%)] Loss: 1.723649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [419840/500434 (84%)] Loss: 1.740754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [430080/500434 (86%)] Loss: 1.662706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [440320/500434 (88%)] Loss: 1.731234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [450560/500434 (90%)] Loss: 1.686578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [460800/500434 (92%)] Loss: 1.842011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [471040/500434 (94%)] Loss: 1.714487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [481280/500434 (96%)] Loss: 1.789021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [491520/500434 (98%)] Loss: 1.692132\u001b[0m\n",
      "\u001b[34mcurrent epoch: 36\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [0/500434 (0%)] Loss: 1.707857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [10240/500434 (2%)] Loss: 1.584845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [20480/500434 (4%)] Loss: 1.656686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [30720/500434 (6%)] Loss: 1.777772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [40960/500434 (8%)] Loss: 1.713065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [51200/500434 (10%)] Loss: 1.708568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [61440/500434 (12%)] Loss: 1.762632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [71680/500434 (14%)] Loss: 1.788980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [81920/500434 (16%)] Loss: 1.716021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [92160/500434 (18%)] Loss: 1.642471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [102400/500434 (20%)] Loss: 1.779469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [112640/500434 (22%)] Loss: 1.658389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [122880/500434 (25%)] Loss: 1.650532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [133120/500434 (27%)] Loss: 1.736460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [143360/500434 (29%)] Loss: 1.637364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [153600/500434 (31%)] Loss: 1.614357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [163840/500434 (33%)] Loss: 1.635231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [174080/500434 (35%)] Loss: 1.712877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [184320/500434 (37%)] Loss: 1.726359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [194560/500434 (39%)] Loss: 1.658933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [204800/500434 (41%)] Loss: 1.928887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [215040/500434 (43%)] Loss: 1.859328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [225280/500434 (45%)] Loss: 1.719787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [235520/500434 (47%)] Loss: 1.709901\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [245760/500434 (49%)] Loss: 1.705633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [256000/500434 (51%)] Loss: 1.689071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [266240/500434 (53%)] Loss: 1.723332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [276480/500434 (55%)] Loss: 1.754838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [286720/500434 (57%)] Loss: 1.724862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [296960/500434 (59%)] Loss: 1.645422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [307200/500434 (61%)] Loss: 1.726836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [317440/500434 (63%)] Loss: 1.691923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [327680/500434 (65%)] Loss: 1.757328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [337920/500434 (67%)] Loss: 1.734364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [348160/500434 (70%)] Loss: 1.827881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [358400/500434 (72%)] Loss: 1.704595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [368640/500434 (74%)] Loss: 1.762014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [378880/500434 (76%)] Loss: 1.679531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [389120/500434 (78%)] Loss: 1.593092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [399360/500434 (80%)] Loss: 1.725548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [409600/500434 (82%)] Loss: 1.684474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [419840/500434 (84%)] Loss: 1.712445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [430080/500434 (86%)] Loss: 1.748245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [440320/500434 (88%)] Loss: 1.745532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [450560/500434 (90%)] Loss: 1.684616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [460800/500434 (92%)] Loss: 1.781710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [471040/500434 (94%)] Loss: 1.724219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [481280/500434 (96%)] Loss: 1.604225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [491520/500434 (98%)] Loss: 1.613914\u001b[0m\n",
      "\u001b[34mcurrent epoch: 37\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [0/500434 (0%)] Loss: 1.643271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [10240/500434 (2%)] Loss: 1.748145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [20480/500434 (4%)] Loss: 1.652853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [30720/500434 (6%)] Loss: 1.775220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [40960/500434 (8%)] Loss: 1.735162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [51200/500434 (10%)] Loss: 1.767949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [61440/500434 (12%)] Loss: 1.752935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [71680/500434 (14%)] Loss: 1.799496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [81920/500434 (16%)] Loss: 1.728634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [92160/500434 (18%)] Loss: 1.657542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [102400/500434 (20%)] Loss: 1.764801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [112640/500434 (22%)] Loss: 1.709960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [122880/500434 (25%)] Loss: 1.715559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [133120/500434 (27%)] Loss: 1.760261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [143360/500434 (29%)] Loss: 1.622526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [153600/500434 (31%)] Loss: 1.661171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [163840/500434 (33%)] Loss: 1.771088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [174080/500434 (35%)] Loss: 1.659796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [184320/500434 (37%)] Loss: 1.756543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [194560/500434 (39%)] Loss: 1.883421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [204800/500434 (41%)] Loss: 1.654172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [215040/500434 (43%)] Loss: 1.811143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [225280/500434 (45%)] Loss: 1.708323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [235520/500434 (47%)] Loss: 1.660468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [245760/500434 (49%)] Loss: 1.606944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [256000/500434 (51%)] Loss: 1.730030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [266240/500434 (53%)] Loss: 1.632784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [276480/500434 (55%)] Loss: 1.757946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [286720/500434 (57%)] Loss: 1.806057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [296960/500434 (59%)] Loss: 1.668492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [307200/500434 (61%)] Loss: 1.831985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [317440/500434 (63%)] Loss: 1.792511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [327680/500434 (65%)] Loss: 1.661288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [337920/500434 (67%)] Loss: 1.808761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [348160/500434 (70%)] Loss: 1.767365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [358400/500434 (72%)] Loss: 1.705892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [368640/500434 (74%)] Loss: 1.775541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [378880/500434 (76%)] Loss: 1.786500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [389120/500434 (78%)] Loss: 1.760021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [399360/500434 (80%)] Loss: 1.804905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [409600/500434 (82%)] Loss: 1.587052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [419840/500434 (84%)] Loss: 1.695582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [430080/500434 (86%)] Loss: 1.759230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [440320/500434 (88%)] Loss: 1.684032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [450560/500434 (90%)] Loss: 2.170084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [460800/500434 (92%)] Loss: 1.756115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [471040/500434 (94%)] Loss: 1.596482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [481280/500434 (96%)] Loss: 1.800002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [491520/500434 (98%)] Loss: 1.799219\u001b[0m\n",
      "\u001b[34mcurrent epoch: 38\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [0/500434 (0%)] Loss: 1.796445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [10240/500434 (2%)] Loss: 1.733445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [20480/500434 (4%)] Loss: 1.661720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [30720/500434 (6%)] Loss: 1.661394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [40960/500434 (8%)] Loss: 1.686806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [51200/500434 (10%)] Loss: 1.726093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [61440/500434 (12%)] Loss: 1.693016\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [71680/500434 (14%)] Loss: 1.720125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [81920/500434 (16%)] Loss: 1.636156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [92160/500434 (18%)] Loss: 1.673004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [102400/500434 (20%)] Loss: 1.573959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [112640/500434 (22%)] Loss: 1.657992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [122880/500434 (25%)] Loss: 1.603036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [133120/500434 (27%)] Loss: 1.689956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [143360/500434 (29%)] Loss: 1.616403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [153600/500434 (31%)] Loss: 1.813475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [163840/500434 (33%)] Loss: 1.804981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [174080/500434 (35%)] Loss: 1.791082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [184320/500434 (37%)] Loss: 1.770023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [194560/500434 (39%)] Loss: 1.749859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [204800/500434 (41%)] Loss: 1.669750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [215040/500434 (43%)] Loss: 1.672312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [225280/500434 (45%)] Loss: 1.648317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [235520/500434 (47%)] Loss: 1.845455\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [245760/500434 (49%)] Loss: 1.787894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [256000/500434 (51%)] Loss: 1.752448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [266240/500434 (53%)] Loss: 1.808045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [276480/500434 (55%)] Loss: 1.764112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [286720/500434 (57%)] Loss: 1.747009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [296960/500434 (59%)] Loss: 1.669500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [307200/500434 (61%)] Loss: 1.632575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [317440/500434 (63%)] Loss: 1.664340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [327680/500434 (65%)] Loss: 1.711066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [337920/500434 (67%)] Loss: 1.741932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [348160/500434 (70%)] Loss: 1.708189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [358400/500434 (72%)] Loss: 1.675993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [368640/500434 (74%)] Loss: 1.681060\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [378880/500434 (76%)] Loss: 1.719173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [389120/500434 (78%)] Loss: 1.720752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [399360/500434 (80%)] Loss: 1.604900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [409600/500434 (82%)] Loss: 1.728245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [419840/500434 (84%)] Loss: 1.677075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [430080/500434 (86%)] Loss: 1.674762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [440320/500434 (88%)] Loss: 1.717562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [450560/500434 (90%)] Loss: 1.702876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [460800/500434 (92%)] Loss: 1.713928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [471040/500434 (94%)] Loss: 1.690537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [481280/500434 (96%)] Loss: 1.698101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [491520/500434 (98%)] Loss: 1.739371\u001b[0m\n",
      "\u001b[34mcurrent epoch: 39\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [0/500434 (0%)] Loss: 1.720172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [10240/500434 (2%)] Loss: 1.609579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [20480/500434 (4%)] Loss: 1.691654\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [30720/500434 (6%)] Loss: 1.656760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [40960/500434 (8%)] Loss: 1.736265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [51200/500434 (10%)] Loss: 1.712067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [61440/500434 (12%)] Loss: 1.728796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [71680/500434 (14%)] Loss: 1.637442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [81920/500434 (16%)] Loss: 1.816629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [92160/500434 (18%)] Loss: 1.713765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [102400/500434 (20%)] Loss: 1.728687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [112640/500434 (22%)] Loss: 1.708901\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [122880/500434 (25%)] Loss: 1.742708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [133120/500434 (27%)] Loss: 1.728544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [143360/500434 (29%)] Loss: 1.708589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [153600/500434 (31%)] Loss: 1.577336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [163840/500434 (33%)] Loss: 1.755278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [174080/500434 (35%)] Loss: 1.789536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [184320/500434 (37%)] Loss: 1.767957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [194560/500434 (39%)] Loss: 1.792331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [204800/500434 (41%)] Loss: 1.808438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [215040/500434 (43%)] Loss: 1.674836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [225280/500434 (45%)] Loss: 1.717788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [235520/500434 (47%)] Loss: 1.619230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [245760/500434 (49%)] Loss: 1.650255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [256000/500434 (51%)] Loss: 1.702838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [266240/500434 (53%)] Loss: 1.713146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [276480/500434 (55%)] Loss: 1.738639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [286720/500434 (57%)] Loss: 1.754064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [296960/500434 (59%)] Loss: 1.700976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [307200/500434 (61%)] Loss: 1.662304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [317440/500434 (63%)] Loss: 1.796200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [327680/500434 (65%)] Loss: 1.620910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [337920/500434 (67%)] Loss: 1.599340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [348160/500434 (70%)] Loss: 1.615242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [358400/500434 (72%)] Loss: 1.630826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [368640/500434 (74%)] Loss: 1.608864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [378880/500434 (76%)] Loss: 1.801214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [389120/500434 (78%)] Loss: 1.580117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [399360/500434 (80%)] Loss: 1.770852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [409600/500434 (82%)] Loss: 1.677993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [419840/500434 (84%)] Loss: 1.526946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [430080/500434 (86%)] Loss: 1.697519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [440320/500434 (88%)] Loss: 1.593049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [450560/500434 (90%)] Loss: 1.714882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [460800/500434 (92%)] Loss: 1.697174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [471040/500434 (94%)] Loss: 1.637900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [481280/500434 (96%)] Loss: 1.640801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [491520/500434 (98%)] Loss: 1.649364\u001b[0m\n",
      "\u001b[34mcurrent epoch: 40\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [0/500434 (0%)] Loss: 1.688410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [10240/500434 (2%)] Loss: 1.688552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [20480/500434 (4%)] Loss: 1.705380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [30720/500434 (6%)] Loss: 1.772152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [40960/500434 (8%)] Loss: 1.752954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [51200/500434 (10%)] Loss: 1.767112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [61440/500434 (12%)] Loss: 1.714012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [71680/500434 (14%)] Loss: 1.625900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [81920/500434 (16%)] Loss: 1.668075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [92160/500434 (18%)] Loss: 1.618560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [102400/500434 (20%)] Loss: 1.681921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [112640/500434 (22%)] Loss: 1.718177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [122880/500434 (25%)] Loss: 1.677204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [133120/500434 (27%)] Loss: 1.659819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [143360/500434 (29%)] Loss: 1.714674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [153600/500434 (31%)] Loss: 1.567174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [163840/500434 (33%)] Loss: 1.750467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [174080/500434 (35%)] Loss: 1.626618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [184320/500434 (37%)] Loss: 1.621904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [194560/500434 (39%)] Loss: 1.700424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [204800/500434 (41%)] Loss: 1.777159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [215040/500434 (43%)] Loss: 1.695519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [225280/500434 (45%)] Loss: 1.612012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [235520/500434 (47%)] Loss: 1.704578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [245760/500434 (49%)] Loss: 1.609053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [256000/500434 (51%)] Loss: 1.735035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [266240/500434 (53%)] Loss: 1.654674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [276480/500434 (55%)] Loss: 1.629396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [286720/500434 (57%)] Loss: 1.774436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [296960/500434 (59%)] Loss: 1.654036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [307200/500434 (61%)] Loss: 1.623577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [317440/500434 (63%)] Loss: 1.721997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [327680/500434 (65%)] Loss: 1.629428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [337920/500434 (67%)] Loss: 1.668825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [348160/500434 (70%)] Loss: 1.655094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [358400/500434 (72%)] Loss: 1.703845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [368640/500434 (74%)] Loss: 1.644064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [378880/500434 (76%)] Loss: 1.641440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [389120/500434 (78%)] Loss: 1.743390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [399360/500434 (80%)] Loss: 1.690065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [409600/500434 (82%)] Loss: 1.700888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [419840/500434 (84%)] Loss: 1.733019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [430080/500434 (86%)] Loss: 1.764692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [440320/500434 (88%)] Loss: 1.595531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [450560/500434 (90%)] Loss: 1.664506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [460800/500434 (92%)] Loss: 1.705083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [471040/500434 (94%)] Loss: 1.749504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [481280/500434 (96%)] Loss: 1.735232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [491520/500434 (98%)] Loss: 1.640249\u001b[0m\n",
      "\u001b[34mcurrent epoch: 41\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [0/500434 (0%)] Loss: 1.633529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [10240/500434 (2%)] Loss: 1.630209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [20480/500434 (4%)] Loss: 1.726135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [30720/500434 (6%)] Loss: 1.658502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [40960/500434 (8%)] Loss: 1.664106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [51200/500434 (10%)] Loss: 1.680019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [61440/500434 (12%)] Loss: 1.808976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [71680/500434 (14%)] Loss: 1.714951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [81920/500434 (16%)] Loss: 1.732528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [92160/500434 (18%)] Loss: 1.791847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [102400/500434 (20%)] Loss: 1.728685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [112640/500434 (22%)] Loss: 1.674109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [122880/500434 (25%)] Loss: 1.669575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [133120/500434 (27%)] Loss: 1.653118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [143360/500434 (29%)] Loss: 1.567017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [153600/500434 (31%)] Loss: 1.628075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [163840/500434 (33%)] Loss: 1.671888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [174080/500434 (35%)] Loss: 1.687088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [184320/500434 (37%)] Loss: 1.683972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [194560/500434 (39%)] Loss: 1.698507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [204800/500434 (41%)] Loss: 1.762563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [215040/500434 (43%)] Loss: 1.762370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [225280/500434 (45%)] Loss: 1.700516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [235520/500434 (47%)] Loss: 1.871956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [245760/500434 (49%)] Loss: 1.703770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [256000/500434 (51%)] Loss: 1.726756\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [266240/500434 (53%)] Loss: 1.721649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [276480/500434 (55%)] Loss: 1.670895\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [286720/500434 (57%)] Loss: 1.778692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [296960/500434 (59%)] Loss: 1.641072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [307200/500434 (61%)] Loss: 1.557670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [317440/500434 (63%)] Loss: 1.676230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [327680/500434 (65%)] Loss: 1.822245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [337920/500434 (67%)] Loss: 1.832148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [348160/500434 (70%)] Loss: 1.646295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [358400/500434 (72%)] Loss: 1.812078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [368640/500434 (74%)] Loss: 1.668962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [378880/500434 (76%)] Loss: 1.540259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [389120/500434 (78%)] Loss: 1.778877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [399360/500434 (80%)] Loss: 1.722507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [409600/500434 (82%)] Loss: 1.638615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [419840/500434 (84%)] Loss: 1.798979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [430080/500434 (86%)] Loss: 1.576059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [440320/500434 (88%)] Loss: 1.614024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [450560/500434 (90%)] Loss: 1.666448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [460800/500434 (92%)] Loss: 1.617325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [471040/500434 (94%)] Loss: 1.696655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [481280/500434 (96%)] Loss: 1.624897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [491520/500434 (98%)] Loss: 1.563756\u001b[0m\n",
      "\u001b[34mcurrent epoch: 42\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [0/500434 (0%)] Loss: 1.698330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [10240/500434 (2%)] Loss: 1.753079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [20480/500434 (4%)] Loss: 1.714537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [30720/500434 (6%)] Loss: 1.721986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [40960/500434 (8%)] Loss: 1.671821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [51200/500434 (10%)] Loss: 1.599829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [61440/500434 (12%)] Loss: 1.711584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [71680/500434 (14%)] Loss: 1.691671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [81920/500434 (16%)] Loss: 1.631202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [92160/500434 (18%)] Loss: 1.602669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [102400/500434 (20%)] Loss: 1.753495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [112640/500434 (22%)] Loss: 1.657352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [122880/500434 (25%)] Loss: 1.690938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [133120/500434 (27%)] Loss: 1.598078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [143360/500434 (29%)] Loss: 1.656080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [153600/500434 (31%)] Loss: 1.703115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [163840/500434 (33%)] Loss: 1.759992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [174080/500434 (35%)] Loss: 1.611626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [184320/500434 (37%)] Loss: 1.716196\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [194560/500434 (39%)] Loss: 1.821671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [204800/500434 (41%)] Loss: 1.641636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [215040/500434 (43%)] Loss: 1.620282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [225280/500434 (45%)] Loss: 1.649215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [235520/500434 (47%)] Loss: 1.640985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [245760/500434 (49%)] Loss: 1.750440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [256000/500434 (51%)] Loss: 1.729582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [266240/500434 (53%)] Loss: 1.616352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [276480/500434 (55%)] Loss: 1.665459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [286720/500434 (57%)] Loss: 1.760770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [296960/500434 (59%)] Loss: 1.641306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [307200/500434 (61%)] Loss: 1.648942\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [317440/500434 (63%)] Loss: 1.624097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [327680/500434 (65%)] Loss: 1.677735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [337920/500434 (67%)] Loss: 1.584365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [348160/500434 (70%)] Loss: 1.613403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [358400/500434 (72%)] Loss: 1.744842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [368640/500434 (74%)] Loss: 1.609147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [378880/500434 (76%)] Loss: 1.592845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [389120/500434 (78%)] Loss: 1.673674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [399360/500434 (80%)] Loss: 1.573471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [409600/500434 (82%)] Loss: 1.737558\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [419840/500434 (84%)] Loss: 1.721273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [430080/500434 (86%)] Loss: 1.623697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [440320/500434 (88%)] Loss: 1.776779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [450560/500434 (90%)] Loss: 1.632232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [460800/500434 (92%)] Loss: 1.757766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [471040/500434 (94%)] Loss: 1.667018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [481280/500434 (96%)] Loss: 1.602688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [491520/500434 (98%)] Loss: 1.697753\u001b[0m\n",
      "\u001b[34mcurrent epoch: 43\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [0/500434 (0%)] Loss: 1.897112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [10240/500434 (2%)] Loss: 1.678468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [20480/500434 (4%)] Loss: 1.635580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [30720/500434 (6%)] Loss: 1.862119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [40960/500434 (8%)] Loss: 1.601994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [51200/500434 (10%)] Loss: 1.675723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [61440/500434 (12%)] Loss: 1.631981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [71680/500434 (14%)] Loss: 1.614492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [81920/500434 (16%)] Loss: 1.589082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [92160/500434 (18%)] Loss: 1.746649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [102400/500434 (20%)] Loss: 1.641218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [112640/500434 (22%)] Loss: 1.641728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [122880/500434 (25%)] Loss: 1.616047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [133120/500434 (27%)] Loss: 1.702989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [143360/500434 (29%)] Loss: 1.675544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [153600/500434 (31%)] Loss: 1.702498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [163840/500434 (33%)] Loss: 1.639868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [174080/500434 (35%)] Loss: 1.657798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [184320/500434 (37%)] Loss: 1.640367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [194560/500434 (39%)] Loss: 1.782754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [204800/500434 (41%)] Loss: 1.689262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [215040/500434 (43%)] Loss: 1.736566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [225280/500434 (45%)] Loss: 1.626764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [235520/500434 (47%)] Loss: 1.739686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [245760/500434 (49%)] Loss: 1.741973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [256000/500434 (51%)] Loss: 1.667742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [266240/500434 (53%)] Loss: 1.795890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [276480/500434 (55%)] Loss: 1.635394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [286720/500434 (57%)] Loss: 1.583993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [296960/500434 (59%)] Loss: 1.747475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [307200/500434 (61%)] Loss: 1.590571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [317440/500434 (63%)] Loss: 1.675923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [327680/500434 (65%)] Loss: 1.602678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [337920/500434 (67%)] Loss: 1.698473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [348160/500434 (70%)] Loss: 1.662406\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [358400/500434 (72%)] Loss: 1.729421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [368640/500434 (74%)] Loss: 1.712777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [378880/500434 (76%)] Loss: 1.625463\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [389120/500434 (78%)] Loss: 1.638262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [399360/500434 (80%)] Loss: 1.850994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [409600/500434 (82%)] Loss: 1.556371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [419840/500434 (84%)] Loss: 1.751195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [430080/500434 (86%)] Loss: 1.677937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [440320/500434 (88%)] Loss: 1.747122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [450560/500434 (90%)] Loss: 1.696581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [460800/500434 (92%)] Loss: 1.662057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [471040/500434 (94%)] Loss: 1.513968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [481280/500434 (96%)] Loss: 1.593544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [491520/500434 (98%)] Loss: 1.762836\u001b[0m\n",
      "\u001b[34mcurrent epoch: 44\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [0/500434 (0%)] Loss: 1.582850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [10240/500434 (2%)] Loss: 1.610513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [20480/500434 (4%)] Loss: 1.651161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [30720/500434 (6%)] Loss: 1.744271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [40960/500434 (8%)] Loss: 1.591972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [51200/500434 (10%)] Loss: 1.737194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [61440/500434 (12%)] Loss: 1.550235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [71680/500434 (14%)] Loss: 1.735014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [81920/500434 (16%)] Loss: 1.662082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [92160/500434 (18%)] Loss: 1.696058\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [102400/500434 (20%)] Loss: 1.608184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [112640/500434 (22%)] Loss: 1.716283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [122880/500434 (25%)] Loss: 1.714097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [133120/500434 (27%)] Loss: 1.679599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [143360/500434 (29%)] Loss: 1.615407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [153600/500434 (31%)] Loss: 1.587991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [163840/500434 (33%)] Loss: 1.696685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [174080/500434 (35%)] Loss: 1.713164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [184320/500434 (37%)] Loss: 1.614012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [194560/500434 (39%)] Loss: 1.597663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [204800/500434 (41%)] Loss: 1.727998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [215040/500434 (43%)] Loss: 1.767448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [225280/500434 (45%)] Loss: 1.895807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [235520/500434 (47%)] Loss: 1.763615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [245760/500434 (49%)] Loss: 1.680659\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [256000/500434 (51%)] Loss: 1.739359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [266240/500434 (53%)] Loss: 1.509549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [276480/500434 (55%)] Loss: 1.661023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [286720/500434 (57%)] Loss: 1.709289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [296960/500434 (59%)] Loss: 1.718469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [307200/500434 (61%)] Loss: 1.580464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [317440/500434 (63%)] Loss: 1.611151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [327680/500434 (65%)] Loss: 1.661352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [337920/500434 (67%)] Loss: 1.630862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [348160/500434 (70%)] Loss: 1.598779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [358400/500434 (72%)] Loss: 1.839322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [368640/500434 (74%)] Loss: 1.839773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [378880/500434 (76%)] Loss: 1.793688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [389120/500434 (78%)] Loss: 1.607890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [399360/500434 (80%)] Loss: 1.676538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [409600/500434 (82%)] Loss: 1.744326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [419840/500434 (84%)] Loss: 1.688729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [430080/500434 (86%)] Loss: 1.757851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [440320/500434 (88%)] Loss: 1.654702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [450560/500434 (90%)] Loss: 1.670666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [460800/500434 (92%)] Loss: 1.752268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [471040/500434 (94%)] Loss: 1.571460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [481280/500434 (96%)] Loss: 1.580675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [491520/500434 (98%)] Loss: 1.725350\u001b[0m\n",
      "\u001b[34mcurrent epoch: 45\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [0/500434 (0%)] Loss: 1.687078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [10240/500434 (2%)] Loss: 1.742644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [20480/500434 (4%)] Loss: 1.660073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [30720/500434 (6%)] Loss: 1.695518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [40960/500434 (8%)] Loss: 1.535921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [51200/500434 (10%)] Loss: 1.592370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [61440/500434 (12%)] Loss: 1.610940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [71680/500434 (14%)] Loss: 1.633185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [81920/500434 (16%)] Loss: 1.660632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [92160/500434 (18%)] Loss: 1.651029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [102400/500434 (20%)] Loss: 1.770421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [112640/500434 (22%)] Loss: 1.584539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [122880/500434 (25%)] Loss: 1.624646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [133120/500434 (27%)] Loss: 1.559940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [143360/500434 (29%)] Loss: 1.771809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [153600/500434 (31%)] Loss: 1.520486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [163840/500434 (33%)] Loss: 1.558182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [174080/500434 (35%)] Loss: 1.614487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [184320/500434 (37%)] Loss: 1.682072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [194560/500434 (39%)] Loss: 1.537873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [204800/500434 (41%)] Loss: 1.712048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [215040/500434 (43%)] Loss: 1.613020\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [225280/500434 (45%)] Loss: 1.613004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [235520/500434 (47%)] Loss: 1.608635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [245760/500434 (49%)] Loss: 1.594459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [256000/500434 (51%)] Loss: 1.765315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [266240/500434 (53%)] Loss: 1.668033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [276480/500434 (55%)] Loss: 1.613070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [286720/500434 (57%)] Loss: 1.504948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [296960/500434 (59%)] Loss: 1.696331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [307200/500434 (61%)] Loss: 1.617692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [317440/500434 (63%)] Loss: 1.671146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [327680/500434 (65%)] Loss: 1.704451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [337920/500434 (67%)] Loss: 1.675007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [348160/500434 (70%)] Loss: 1.619578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [358400/500434 (72%)] Loss: 1.725049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [368640/500434 (74%)] Loss: 1.587905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [378880/500434 (76%)] Loss: 1.584967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [389120/500434 (78%)] Loss: 1.639177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [399360/500434 (80%)] Loss: 1.660328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [409600/500434 (82%)] Loss: 1.600038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [419840/500434 (84%)] Loss: 1.621266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [430080/500434 (86%)] Loss: 1.676755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [440320/500434 (88%)] Loss: 1.592726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [450560/500434 (90%)] Loss: 1.709288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [460800/500434 (92%)] Loss: 1.600712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [471040/500434 (94%)] Loss: 1.724617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [481280/500434 (96%)] Loss: 1.706358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [491520/500434 (98%)] Loss: 1.556267\u001b[0m\n",
      "\u001b[34mcurrent epoch: 46\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [0/500434 (0%)] Loss: 1.568510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [10240/500434 (2%)] Loss: 1.591220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [20480/500434 (4%)] Loss: 1.583034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [30720/500434 (6%)] Loss: 1.599792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [40960/500434 (8%)] Loss: 1.632915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [51200/500434 (10%)] Loss: 1.703465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [61440/500434 (12%)] Loss: 1.709693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [71680/500434 (14%)] Loss: 1.573205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [81920/500434 (16%)] Loss: 1.696098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [92160/500434 (18%)] Loss: 1.666290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [102400/500434 (20%)] Loss: 1.775545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [112640/500434 (22%)] Loss: 1.576478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [122880/500434 (25%)] Loss: 1.617875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [133120/500434 (27%)] Loss: 1.635509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [143360/500434 (29%)] Loss: 1.560722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [153600/500434 (31%)] Loss: 1.628002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [163840/500434 (33%)] Loss: 1.693189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [174080/500434 (35%)] Loss: 1.642819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [184320/500434 (37%)] Loss: 1.643741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [194560/500434 (39%)] Loss: 1.741536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [204800/500434 (41%)] Loss: 1.582990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [215040/500434 (43%)] Loss: 1.608543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [225280/500434 (45%)] Loss: 1.612469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [235520/500434 (47%)] Loss: 1.637481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [245760/500434 (49%)] Loss: 1.604633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [256000/500434 (51%)] Loss: 1.606167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [266240/500434 (53%)] Loss: 1.641125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [276480/500434 (55%)] Loss: 1.662618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [286720/500434 (57%)] Loss: 1.661303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [296960/500434 (59%)] Loss: 1.708568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [307200/500434 (61%)] Loss: 1.568273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [317440/500434 (63%)] Loss: 1.687212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [327680/500434 (65%)] Loss: 1.609093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [337920/500434 (67%)] Loss: 1.625148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [348160/500434 (70%)] Loss: 1.599217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [358400/500434 (72%)] Loss: 1.690769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [368640/500434 (74%)] Loss: 1.639356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [378880/500434 (76%)] Loss: 1.666342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [389120/500434 (78%)] Loss: 1.635720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [399360/500434 (80%)] Loss: 1.669877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [409600/500434 (82%)] Loss: 1.585641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [419840/500434 (84%)] Loss: 1.596272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [430080/500434 (86%)] Loss: 1.640056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [440320/500434 (88%)] Loss: 1.639134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [450560/500434 (90%)] Loss: 1.708180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [460800/500434 (92%)] Loss: 1.731158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [471040/500434 (94%)] Loss: 1.632030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [481280/500434 (96%)] Loss: 1.634682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [491520/500434 (98%)] Loss: 1.705753\u001b[0m\n",
      "\u001b[34mcurrent epoch: 47\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [0/500434 (0%)] Loss: 1.597244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [10240/500434 (2%)] Loss: 1.629378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [20480/500434 (4%)] Loss: 1.668753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [30720/500434 (6%)] Loss: 1.672121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [40960/500434 (8%)] Loss: 1.620198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [51200/500434 (10%)] Loss: 1.518748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [61440/500434 (12%)] Loss: 1.777751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [71680/500434 (14%)] Loss: 1.732072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [81920/500434 (16%)] Loss: 1.584194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [92160/500434 (18%)] Loss: 1.657713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [102400/500434 (20%)] Loss: 1.651767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [112640/500434 (22%)] Loss: 1.635885\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [122880/500434 (25%)] Loss: 1.675416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [133120/500434 (27%)] Loss: 1.610547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [143360/500434 (29%)] Loss: 1.673574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [153600/500434 (31%)] Loss: 1.604433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [163840/500434 (33%)] Loss: 1.694163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [174080/500434 (35%)] Loss: 1.589298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [184320/500434 (37%)] Loss: 1.596236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [194560/500434 (39%)] Loss: 1.623652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [204800/500434 (41%)] Loss: 1.703027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [215040/500434 (43%)] Loss: 1.733719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [225280/500434 (45%)] Loss: 1.650294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [235520/500434 (47%)] Loss: 1.703271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [245760/500434 (49%)] Loss: 1.549096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [256000/500434 (51%)] Loss: 1.575356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [266240/500434 (53%)] Loss: 1.629037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [276480/500434 (55%)] Loss: 1.731223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [286720/500434 (57%)] Loss: 1.683483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [296960/500434 (59%)] Loss: 1.628857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [307200/500434 (61%)] Loss: 1.558329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [317440/500434 (63%)] Loss: 1.560706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [327680/500434 (65%)] Loss: 1.587891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [337920/500434 (67%)] Loss: 1.704449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [348160/500434 (70%)] Loss: 1.671293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [358400/500434 (72%)] Loss: 1.747868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [368640/500434 (74%)] Loss: 1.570038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [378880/500434 (76%)] Loss: 1.640422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [389120/500434 (78%)] Loss: 1.592648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [399360/500434 (80%)] Loss: 1.623619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [409600/500434 (82%)] Loss: 1.610486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [419840/500434 (84%)] Loss: 1.633180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [430080/500434 (86%)] Loss: 1.663005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [440320/500434 (88%)] Loss: 1.623577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [450560/500434 (90%)] Loss: 1.588263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [460800/500434 (92%)] Loss: 1.623162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [471040/500434 (94%)] Loss: 1.603932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [481280/500434 (96%)] Loss: 1.545698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [491520/500434 (98%)] Loss: 1.667493\u001b[0m\n",
      "\u001b[34mcurrent epoch: 48\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [0/500434 (0%)] Loss: 1.622068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [10240/500434 (2%)] Loss: 1.550766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [20480/500434 (4%)] Loss: 1.552425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [30720/500434 (6%)] Loss: 1.780401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [40960/500434 (8%)] Loss: 1.598631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [51200/500434 (10%)] Loss: 1.686456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [61440/500434 (12%)] Loss: 1.757759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [71680/500434 (14%)] Loss: 1.756120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [81920/500434 (16%)] Loss: 1.554947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [92160/500434 (18%)] Loss: 1.659611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [102400/500434 (20%)] Loss: 1.649693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [112640/500434 (22%)] Loss: 1.616037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [122880/500434 (25%)] Loss: 1.568059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [133120/500434 (27%)] Loss: 1.561191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [143360/500434 (29%)] Loss: 1.662675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [153600/500434 (31%)] Loss: 1.590695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [163840/500434 (33%)] Loss: 1.651372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [174080/500434 (35%)] Loss: 1.805131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [184320/500434 (37%)] Loss: 1.619066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [194560/500434 (39%)] Loss: 1.667731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [204800/500434 (41%)] Loss: 1.578802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [215040/500434 (43%)] Loss: 1.693481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [225280/500434 (45%)] Loss: 1.560689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [235520/500434 (47%)] Loss: 1.601860\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [245760/500434 (49%)] Loss: 1.681224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [256000/500434 (51%)] Loss: 1.670734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [266240/500434 (53%)] Loss: 1.685246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [276480/500434 (55%)] Loss: 1.581468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [286720/500434 (57%)] Loss: 1.732043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [296960/500434 (59%)] Loss: 1.589611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [307200/500434 (61%)] Loss: 1.607509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [317440/500434 (63%)] Loss: 1.615229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [327680/500434 (65%)] Loss: 1.615343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [337920/500434 (67%)] Loss: 1.567048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [348160/500434 (70%)] Loss: 1.709896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [358400/500434 (72%)] Loss: 1.607357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [368640/500434 (74%)] Loss: 1.623550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [378880/500434 (76%)] Loss: 1.623311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [389120/500434 (78%)] Loss: 1.656275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [399360/500434 (80%)] Loss: 1.623050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [409600/500434 (82%)] Loss: 1.771717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [419840/500434 (84%)] Loss: 1.576373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [430080/500434 (86%)] Loss: 1.587192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [440320/500434 (88%)] Loss: 1.687488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [450560/500434 (90%)] Loss: 1.650373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [460800/500434 (92%)] Loss: 1.819701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [471040/500434 (94%)] Loss: 1.562313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [481280/500434 (96%)] Loss: 1.616618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [491520/500434 (98%)] Loss: 1.633248\u001b[0m\n",
      "\u001b[34mcurrent epoch: 49\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [0/500434 (0%)] Loss: 1.554773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [10240/500434 (2%)] Loss: 1.688624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [20480/500434 (4%)] Loss: 1.684595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [30720/500434 (6%)] Loss: 1.621677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [40960/500434 (8%)] Loss: 1.654180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [51200/500434 (10%)] Loss: 1.698036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [61440/500434 (12%)] Loss: 1.609812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [71680/500434 (14%)] Loss: 1.639818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [81920/500434 (16%)] Loss: 1.671551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [92160/500434 (18%)] Loss: 1.617456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [102400/500434 (20%)] Loss: 1.616741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [112640/500434 (22%)] Loss: 1.601064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [122880/500434 (25%)] Loss: 1.851228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [133120/500434 (27%)] Loss: 1.638693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [143360/500434 (29%)] Loss: 1.630720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [153600/500434 (31%)] Loss: 1.567829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [163840/500434 (33%)] Loss: 1.737348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [174080/500434 (35%)] Loss: 1.683528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [184320/500434 (37%)] Loss: 1.650560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [194560/500434 (39%)] Loss: 1.638789\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [204800/500434 (41%)] Loss: 1.600361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [215040/500434 (43%)] Loss: 1.648239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [225280/500434 (45%)] Loss: 1.725052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [235520/500434 (47%)] Loss: 1.581792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [245760/500434 (49%)] Loss: 1.564138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [256000/500434 (51%)] Loss: 1.586161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [266240/500434 (53%)] Loss: 1.556821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [276480/500434 (55%)] Loss: 1.564326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [286720/500434 (57%)] Loss: 1.633473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [296960/500434 (59%)] Loss: 1.633731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [307200/500434 (61%)] Loss: 1.569275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [317440/500434 (63%)] Loss: 1.645777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [327680/500434 (65%)] Loss: 1.755119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [337920/500434 (67%)] Loss: 1.644505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [348160/500434 (70%)] Loss: 1.619608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [358400/500434 (72%)] Loss: 1.703011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [368640/500434 (74%)] Loss: 1.645936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [378880/500434 (76%)] Loss: 1.610727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [389120/500434 (78%)] Loss: 1.838884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [399360/500434 (80%)] Loss: 1.619689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [409600/500434 (82%)] Loss: 1.705315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [419840/500434 (84%)] Loss: 1.577675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [430080/500434 (86%)] Loss: 1.641650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [440320/500434 (88%)] Loss: 1.681697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [450560/500434 (90%)] Loss: 1.615572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [460800/500434 (92%)] Loss: 1.657819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [471040/500434 (94%)] Loss: 1.583304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [481280/500434 (96%)] Loss: 1.532177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [491520/500434 (98%)] Loss: 1.628796\u001b[0m\n",
      "\u001b[34mcurrent epoch: 50\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [0/500434 (0%)] Loss: 1.541645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [10240/500434 (2%)] Loss: 1.565139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [20480/500434 (4%)] Loss: 1.677414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [30720/500434 (6%)] Loss: 1.563327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [40960/500434 (8%)] Loss: 1.633347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [51200/500434 (10%)] Loss: 1.545062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [61440/500434 (12%)] Loss: 1.607576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [71680/500434 (14%)] Loss: 1.614160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [81920/500434 (16%)] Loss: 1.622017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [92160/500434 (18%)] Loss: 1.606968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [102400/500434 (20%)] Loss: 1.676538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [112640/500434 (22%)] Loss: 1.558739\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [122880/500434 (25%)] Loss: 1.589013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [133120/500434 (27%)] Loss: 1.581832\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [143360/500434 (29%)] Loss: 1.637226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [153600/500434 (31%)] Loss: 1.634272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [163840/500434 (33%)] Loss: 1.616050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [174080/500434 (35%)] Loss: 1.618354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [184320/500434 (37%)] Loss: 1.671613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [194560/500434 (39%)] Loss: 1.550616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [204800/500434 (41%)] Loss: 1.695008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [215040/500434 (43%)] Loss: 1.693004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [225280/500434 (45%)] Loss: 1.580522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [235520/500434 (47%)] Loss: 1.668198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [245760/500434 (49%)] Loss: 1.658328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [256000/500434 (51%)] Loss: 1.585742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [266240/500434 (53%)] Loss: 1.624449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [276480/500434 (55%)] Loss: 1.564339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [286720/500434 (57%)] Loss: 1.628982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [296960/500434 (59%)] Loss: 1.588781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [307200/500434 (61%)] Loss: 1.557180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [317440/500434 (63%)] Loss: 1.564500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [327680/500434 (65%)] Loss: 1.647168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [337920/500434 (67%)] Loss: 1.550263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [348160/500434 (70%)] Loss: 1.517889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [358400/500434 (72%)] Loss: 1.681658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [368640/500434 (74%)] Loss: 1.650615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [378880/500434 (76%)] Loss: 1.582020\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [389120/500434 (78%)] Loss: 1.638433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [399360/500434 (80%)] Loss: 1.659669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [409600/500434 (82%)] Loss: 1.587196\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [419840/500434 (84%)] Loss: 1.525905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [430080/500434 (86%)] Loss: 1.673856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [440320/500434 (88%)] Loss: 1.622244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [450560/500434 (90%)] Loss: 1.676857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [460800/500434 (92%)] Loss: 1.562415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [471040/500434 (94%)] Loss: 1.557449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [481280/500434 (96%)] Loss: 1.575803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [491520/500434 (98%)] Loss: 1.587228\u001b[0m\n",
      "\u001b[34mcurrent epoch: 51\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [0/500434 (0%)] Loss: 1.544265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [10240/500434 (2%)] Loss: 1.549825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [20480/500434 (4%)] Loss: 1.639160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [30720/500434 (6%)] Loss: 1.586122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [40960/500434 (8%)] Loss: 1.619459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [51200/500434 (10%)] Loss: 1.599750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [61440/500434 (12%)] Loss: 1.591038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [71680/500434 (14%)] Loss: 1.540222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [81920/500434 (16%)] Loss: 1.775595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [92160/500434 (18%)] Loss: 1.589667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [102400/500434 (20%)] Loss: 1.624332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [112640/500434 (22%)] Loss: 1.628025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [122880/500434 (25%)] Loss: 1.567109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [133120/500434 (27%)] Loss: 1.648806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [143360/500434 (29%)] Loss: 1.574428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [153600/500434 (31%)] Loss: 1.568581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [163840/500434 (33%)] Loss: 1.562296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [174080/500434 (35%)] Loss: 1.723893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [184320/500434 (37%)] Loss: 1.532841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [194560/500434 (39%)] Loss: 1.566377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [204800/500434 (41%)] Loss: 1.688041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [215040/500434 (43%)] Loss: 1.587154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [225280/500434 (45%)] Loss: 1.623473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [235520/500434 (47%)] Loss: 1.532103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [245760/500434 (49%)] Loss: 1.668332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [256000/500434 (51%)] Loss: 1.511479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [266240/500434 (53%)] Loss: 1.613166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [276480/500434 (55%)] Loss: 1.668372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [286720/500434 (57%)] Loss: 1.608334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [296960/500434 (59%)] Loss: 1.551880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [307200/500434 (61%)] Loss: 1.618017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [317440/500434 (63%)] Loss: 1.624486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [327680/500434 (65%)] Loss: 1.475577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [337920/500434 (67%)] Loss: 1.653991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [348160/500434 (70%)] Loss: 1.735692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [358400/500434 (72%)] Loss: 1.585337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [368640/500434 (74%)] Loss: 1.636161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [378880/500434 (76%)] Loss: 1.480498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [389120/500434 (78%)] Loss: 1.701358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [399360/500434 (80%)] Loss: 1.663654\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [409600/500434 (82%)] Loss: 1.584500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [419840/500434 (84%)] Loss: 1.732360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [430080/500434 (86%)] Loss: 1.734935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [440320/500434 (88%)] Loss: 1.725816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [450560/500434 (90%)] Loss: 1.693433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [460800/500434 (92%)] Loss: 1.644913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [471040/500434 (94%)] Loss: 1.659862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [481280/500434 (96%)] Loss: 1.601033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [491520/500434 (98%)] Loss: 1.572439\u001b[0m\n",
      "\u001b[34mcurrent epoch: 52\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [0/500434 (0%)] Loss: 1.599432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [10240/500434 (2%)] Loss: 1.578192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [20480/500434 (4%)] Loss: 1.547309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [30720/500434 (6%)] Loss: 1.682468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [40960/500434 (8%)] Loss: 1.617628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [51200/500434 (10%)] Loss: 1.604118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [61440/500434 (12%)] Loss: 1.521131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [71680/500434 (14%)] Loss: 1.668029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [81920/500434 (16%)] Loss: 1.562198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [92160/500434 (18%)] Loss: 1.768690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [102400/500434 (20%)] Loss: 1.578843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [112640/500434 (22%)] Loss: 1.644231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [122880/500434 (25%)] Loss: 1.561515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [133120/500434 (27%)] Loss: 1.601446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [143360/500434 (29%)] Loss: 1.608487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [153600/500434 (31%)] Loss: 1.612073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [163840/500434 (33%)] Loss: 1.551349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [174080/500434 (35%)] Loss: 1.633166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [184320/500434 (37%)] Loss: 1.603116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [194560/500434 (39%)] Loss: 1.608809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [204800/500434 (41%)] Loss: 1.446517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [215040/500434 (43%)] Loss: 1.594450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [225280/500434 (45%)] Loss: 1.590302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [235520/500434 (47%)] Loss: 1.619000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [245760/500434 (49%)] Loss: 1.491394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [256000/500434 (51%)] Loss: 1.582278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [266240/500434 (53%)] Loss: 1.717819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [276480/500434 (55%)] Loss: 1.640560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [286720/500434 (57%)] Loss: 1.627143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [296960/500434 (59%)] Loss: 1.608403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [307200/500434 (61%)] Loss: 1.573413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [317440/500434 (63%)] Loss: 1.607697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [327680/500434 (65%)] Loss: 1.564366\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [337920/500434 (67%)] Loss: 1.670096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [348160/500434 (70%)] Loss: 1.749618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [358400/500434 (72%)] Loss: 1.570861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [368640/500434 (74%)] Loss: 1.549200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [378880/500434 (76%)] Loss: 1.651304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [389120/500434 (78%)] Loss: 1.617095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [399360/500434 (80%)] Loss: 1.701344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [409600/500434 (82%)] Loss: 1.531480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [419840/500434 (84%)] Loss: 1.502986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [430080/500434 (86%)] Loss: 1.560067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [440320/500434 (88%)] Loss: 1.579129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [450560/500434 (90%)] Loss: 1.621811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [460800/500434 (92%)] Loss: 1.526379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [471040/500434 (94%)] Loss: 1.551588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [481280/500434 (96%)] Loss: 1.566823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [491520/500434 (98%)] Loss: 1.660457\u001b[0m\n",
      "\u001b[34mcurrent epoch: 53\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [0/500434 (0%)] Loss: 1.604907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [10240/500434 (2%)] Loss: 1.601000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [20480/500434 (4%)] Loss: 1.520041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [30720/500434 (6%)] Loss: 1.570691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [40960/500434 (8%)] Loss: 1.518703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [51200/500434 (10%)] Loss: 1.574274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [61440/500434 (12%)] Loss: 1.618405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [71680/500434 (14%)] Loss: 1.553681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [81920/500434 (16%)] Loss: 1.672029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [92160/500434 (18%)] Loss: 1.591200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [102400/500434 (20%)] Loss: 1.668909\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [112640/500434 (22%)] Loss: 1.584965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [122880/500434 (25%)] Loss: 1.771778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [133120/500434 (27%)] Loss: 1.591879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [143360/500434 (29%)] Loss: 1.503517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [153600/500434 (31%)] Loss: 1.600310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [163840/500434 (33%)] Loss: 1.576084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [174080/500434 (35%)] Loss: 1.638415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [184320/500434 (37%)] Loss: 1.733967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [194560/500434 (39%)] Loss: 1.574161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [204800/500434 (41%)] Loss: 1.700629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [215040/500434 (43%)] Loss: 1.607381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [225280/500434 (45%)] Loss: 1.681789\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [235520/500434 (47%)] Loss: 1.545208\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [245760/500434 (49%)] Loss: 1.682657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [256000/500434 (51%)] Loss: 1.510883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [266240/500434 (53%)] Loss: 1.631774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [276480/500434 (55%)] Loss: 1.571517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [286720/500434 (57%)] Loss: 1.592298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [296960/500434 (59%)] Loss: 1.719854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [307200/500434 (61%)] Loss: 1.688136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [317440/500434 (63%)] Loss: 1.575753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [327680/500434 (65%)] Loss: 1.687005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [337920/500434 (67%)] Loss: 1.749166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [348160/500434 (70%)] Loss: 1.744684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [358400/500434 (72%)] Loss: 1.779074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [368640/500434 (74%)] Loss: 1.596791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [378880/500434 (76%)] Loss: 1.746383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [389120/500434 (78%)] Loss: 1.593659\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [399360/500434 (80%)] Loss: 1.616825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [409600/500434 (82%)] Loss: 1.590580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [419840/500434 (84%)] Loss: 1.539022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [430080/500434 (86%)] Loss: 1.522246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [440320/500434 (88%)] Loss: 1.658127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [450560/500434 (90%)] Loss: 1.675202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [460800/500434 (92%)] Loss: 1.543087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [471040/500434 (94%)] Loss: 1.534106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [481280/500434 (96%)] Loss: 1.628727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [491520/500434 (98%)] Loss: 1.618968\u001b[0m\n",
      "\u001b[34mcurrent epoch: 54\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [0/500434 (0%)] Loss: 1.521103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [10240/500434 (2%)] Loss: 1.649210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [20480/500434 (4%)] Loss: 1.629094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [30720/500434 (6%)] Loss: 1.562491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [40960/500434 (8%)] Loss: 1.637631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [51200/500434 (10%)] Loss: 1.587166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [61440/500434 (12%)] Loss: 1.697695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [71680/500434 (14%)] Loss: 1.474818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [81920/500434 (16%)] Loss: 1.740560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [92160/500434 (18%)] Loss: 1.595738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [102400/500434 (20%)] Loss: 1.571148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [112640/500434 (22%)] Loss: 1.519474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [122880/500434 (25%)] Loss: 1.603806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [133120/500434 (27%)] Loss: 1.634691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [143360/500434 (29%)] Loss: 1.545545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [153600/500434 (31%)] Loss: 1.609580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [163840/500434 (33%)] Loss: 1.615626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [174080/500434 (35%)] Loss: 1.607892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [184320/500434 (37%)] Loss: 1.705853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [194560/500434 (39%)] Loss: 1.553619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [204800/500434 (41%)] Loss: 1.499830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [215040/500434 (43%)] Loss: 1.554354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [225280/500434 (45%)] Loss: 1.580053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [235520/500434 (47%)] Loss: 1.666694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [245760/500434 (49%)] Loss: 1.464072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [256000/500434 (51%)] Loss: 1.620801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [266240/500434 (53%)] Loss: 1.459328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [276480/500434 (55%)] Loss: 1.522425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [286720/500434 (57%)] Loss: 1.539492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [296960/500434 (59%)] Loss: 1.628800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [307200/500434 (61%)] Loss: 1.579452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [317440/500434 (63%)] Loss: 1.534714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [327680/500434 (65%)] Loss: 1.504781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [337920/500434 (67%)] Loss: 1.739338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [348160/500434 (70%)] Loss: 1.583263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [358400/500434 (72%)] Loss: 1.583178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [368640/500434 (74%)] Loss: 1.598127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [378880/500434 (76%)] Loss: 1.602981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [389120/500434 (78%)] Loss: 1.732970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [399360/500434 (80%)] Loss: 1.536831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [409600/500434 (82%)] Loss: 1.510918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [419840/500434 (84%)] Loss: 1.569371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [430080/500434 (86%)] Loss: 1.601135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [440320/500434 (88%)] Loss: 1.586360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [450560/500434 (90%)] Loss: 1.560861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [460800/500434 (92%)] Loss: 1.622702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [471040/500434 (94%)] Loss: 1.576651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [481280/500434 (96%)] Loss: 1.445313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [491520/500434 (98%)] Loss: 1.579835\u001b[0m\n",
      "\u001b[34mcurrent epoch: 55\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [0/500434 (0%)] Loss: 1.676256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [10240/500434 (2%)] Loss: 1.649034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [20480/500434 (4%)] Loss: 1.487576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [30720/500434 (6%)] Loss: 1.421574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [40960/500434 (8%)] Loss: 1.625277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [51200/500434 (10%)] Loss: 1.588867\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [61440/500434 (12%)] Loss: 1.538740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [71680/500434 (14%)] Loss: 1.599155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [81920/500434 (16%)] Loss: 1.664752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [92160/500434 (18%)] Loss: 1.629237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [102400/500434 (20%)] Loss: 1.561926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [112640/500434 (22%)] Loss: 1.535216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [122880/500434 (25%)] Loss: 1.570417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [133120/500434 (27%)] Loss: 1.541191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [143360/500434 (29%)] Loss: 1.556531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [153600/500434 (31%)] Loss: 1.528781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [163840/500434 (33%)] Loss: 1.707610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [174080/500434 (35%)] Loss: 1.617436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [184320/500434 (37%)] Loss: 1.723346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [194560/500434 (39%)] Loss: 1.703017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [204800/500434 (41%)] Loss: 1.555801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [215040/500434 (43%)] Loss: 1.721981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [225280/500434 (45%)] Loss: 1.547816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [235520/500434 (47%)] Loss: 1.642660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [245760/500434 (49%)] Loss: 1.615677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [256000/500434 (51%)] Loss: 1.585711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [266240/500434 (53%)] Loss: 1.594768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [276480/500434 (55%)] Loss: 1.501489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [286720/500434 (57%)] Loss: 1.556596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [296960/500434 (59%)] Loss: 1.471583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [307200/500434 (61%)] Loss: 1.600533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [317440/500434 (63%)] Loss: 1.565738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [327680/500434 (65%)] Loss: 1.639244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [337920/500434 (67%)] Loss: 1.631604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [348160/500434 (70%)] Loss: 1.702417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [358400/500434 (72%)] Loss: 1.571833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [368640/500434 (74%)] Loss: 1.675073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [378880/500434 (76%)] Loss: 1.686388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [389120/500434 (78%)] Loss: 1.623652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [399360/500434 (80%)] Loss: 1.655919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [409600/500434 (82%)] Loss: 1.629144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [419840/500434 (84%)] Loss: 1.596844\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [430080/500434 (86%)] Loss: 1.567155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [440320/500434 (88%)] Loss: 1.581943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [450560/500434 (90%)] Loss: 1.550156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [460800/500434 (92%)] Loss: 1.632710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [471040/500434 (94%)] Loss: 1.650101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [481280/500434 (96%)] Loss: 1.526330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [491520/500434 (98%)] Loss: 1.526717\u001b[0m\n",
      "\u001b[34mcurrent epoch: 56\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [0/500434 (0%)] Loss: 1.528904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [10240/500434 (2%)] Loss: 1.503158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [20480/500434 (4%)] Loss: 1.621268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [30720/500434 (6%)] Loss: 1.508861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [40960/500434 (8%)] Loss: 1.623283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [51200/500434 (10%)] Loss: 1.615497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [61440/500434 (12%)] Loss: 1.670391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [71680/500434 (14%)] Loss: 1.642222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [81920/500434 (16%)] Loss: 1.655254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [92160/500434 (18%)] Loss: 1.565613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [102400/500434 (20%)] Loss: 1.525928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [112640/500434 (22%)] Loss: 1.578746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [122880/500434 (25%)] Loss: 1.521733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [133120/500434 (27%)] Loss: 1.626591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [143360/500434 (29%)] Loss: 1.706171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [153600/500434 (31%)] Loss: 1.500084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [163840/500434 (33%)] Loss: 1.539662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [174080/500434 (35%)] Loss: 1.597099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [184320/500434 (37%)] Loss: 1.628785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [194560/500434 (39%)] Loss: 1.601174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [204800/500434 (41%)] Loss: 1.569030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [215040/500434 (43%)] Loss: 1.688596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [225280/500434 (45%)] Loss: 1.676362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [235520/500434 (47%)] Loss: 1.630568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [245760/500434 (49%)] Loss: 1.574150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [256000/500434 (51%)] Loss: 1.467140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [266240/500434 (53%)] Loss: 1.550811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [276480/500434 (55%)] Loss: 1.488340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [286720/500434 (57%)] Loss: 1.539784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [296960/500434 (59%)] Loss: 1.545262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [307200/500434 (61%)] Loss: 1.588146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [317440/500434 (63%)] Loss: 1.585037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [327680/500434 (65%)] Loss: 1.558716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [337920/500434 (67%)] Loss: 1.616470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [348160/500434 (70%)] Loss: 1.619941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [358400/500434 (72%)] Loss: 1.567029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [368640/500434 (74%)] Loss: 1.538753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [378880/500434 (76%)] Loss: 1.621555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [389120/500434 (78%)] Loss: 1.550098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [399360/500434 (80%)] Loss: 1.738081\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [409600/500434 (82%)] Loss: 1.567384\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [419840/500434 (84%)] Loss: 1.602814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [430080/500434 (86%)] Loss: 1.534024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [440320/500434 (88%)] Loss: 1.587931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [450560/500434 (90%)] Loss: 1.744246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [460800/500434 (92%)] Loss: 1.556615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [471040/500434 (94%)] Loss: 1.549150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [481280/500434 (96%)] Loss: 1.517058\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [491520/500434 (98%)] Loss: 1.648359\u001b[0m\n",
      "\u001b[34mcurrent epoch: 57\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [0/500434 (0%)] Loss: 1.553993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [10240/500434 (2%)] Loss: 1.553495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [20480/500434 (4%)] Loss: 1.662766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [30720/500434 (6%)] Loss: 1.659312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [40960/500434 (8%)] Loss: 1.523241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [51200/500434 (10%)] Loss: 1.597501\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [61440/500434 (12%)] Loss: 1.628727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [71680/500434 (14%)] Loss: 1.557914\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [81920/500434 (16%)] Loss: 1.570741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [92160/500434 (18%)] Loss: 1.542748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [102400/500434 (20%)] Loss: 1.612253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [112640/500434 (22%)] Loss: 1.539589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [122880/500434 (25%)] Loss: 1.601032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [133120/500434 (27%)] Loss: 1.584013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [143360/500434 (29%)] Loss: 1.498667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [153600/500434 (31%)] Loss: 1.630808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [163840/500434 (33%)] Loss: 1.486035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [174080/500434 (35%)] Loss: 1.584765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [184320/500434 (37%)] Loss: 1.383899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [194560/500434 (39%)] Loss: 1.577737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [204800/500434 (41%)] Loss: 1.581839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [215040/500434 (43%)] Loss: 1.654482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [225280/500434 (45%)] Loss: 1.489742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [235520/500434 (47%)] Loss: 1.559690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [245760/500434 (49%)] Loss: 1.633665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [256000/500434 (51%)] Loss: 1.575055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [266240/500434 (53%)] Loss: 1.665252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [276480/500434 (55%)] Loss: 1.568892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [286720/500434 (57%)] Loss: 1.498587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [296960/500434 (59%)] Loss: 1.691891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [307200/500434 (61%)] Loss: 1.716849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [317440/500434 (63%)] Loss: 1.559960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [327680/500434 (65%)] Loss: 1.538101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [337920/500434 (67%)] Loss: 1.604177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [348160/500434 (70%)] Loss: 1.654742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [358400/500434 (72%)] Loss: 1.544107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [368640/500434 (74%)] Loss: 1.566836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [378880/500434 (76%)] Loss: 1.617887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [389120/500434 (78%)] Loss: 1.498674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [399360/500434 (80%)] Loss: 1.585439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [409600/500434 (82%)] Loss: 1.520471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [419840/500434 (84%)] Loss: 1.506700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [430080/500434 (86%)] Loss: 1.673627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [440320/500434 (88%)] Loss: 1.574699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [450560/500434 (90%)] Loss: 1.621238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [460800/500434 (92%)] Loss: 1.737416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [471040/500434 (94%)] Loss: 1.613912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [481280/500434 (96%)] Loss: 1.658081\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [491520/500434 (98%)] Loss: 1.635802\u001b[0m\n",
      "\u001b[34mcurrent epoch: 58\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [0/500434 (0%)] Loss: 1.574781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [10240/500434 (2%)] Loss: 1.579921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [20480/500434 (4%)] Loss: 1.691305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [30720/500434 (6%)] Loss: 1.537478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [40960/500434 (8%)] Loss: 1.574017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [51200/500434 (10%)] Loss: 1.596233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [61440/500434 (12%)] Loss: 1.513291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [71680/500434 (14%)] Loss: 1.573067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [81920/500434 (16%)] Loss: 1.726236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [92160/500434 (18%)] Loss: 1.651615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [102400/500434 (20%)] Loss: 1.463960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [112640/500434 (22%)] Loss: 1.621528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [122880/500434 (25%)] Loss: 1.553046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [133120/500434 (27%)] Loss: 1.541162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [143360/500434 (29%)] Loss: 1.650870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [153600/500434 (31%)] Loss: 1.562426\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [163840/500434 (33%)] Loss: 1.525761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [174080/500434 (35%)] Loss: 1.526334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [184320/500434 (37%)] Loss: 1.504202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [194560/500434 (39%)] Loss: 1.553326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [204800/500434 (41%)] Loss: 1.599458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [215040/500434 (43%)] Loss: 1.669056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [225280/500434 (45%)] Loss: 1.551553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [235520/500434 (47%)] Loss: 1.536858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [245760/500434 (49%)] Loss: 1.536858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [256000/500434 (51%)] Loss: 1.552405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [266240/500434 (53%)] Loss: 1.520453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [276480/500434 (55%)] Loss: 1.565351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [286720/500434 (57%)] Loss: 1.597862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [296960/500434 (59%)] Loss: 1.616211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [307200/500434 (61%)] Loss: 1.511578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [317440/500434 (63%)] Loss: 1.630543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [327680/500434 (65%)] Loss: 1.474374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [337920/500434 (67%)] Loss: 1.502784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [348160/500434 (70%)] Loss: 1.562299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [358400/500434 (72%)] Loss: 1.611053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [368640/500434 (74%)] Loss: 1.631950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [378880/500434 (76%)] Loss: 1.695576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [389120/500434 (78%)] Loss: 1.512768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [399360/500434 (80%)] Loss: 1.548779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [409600/500434 (82%)] Loss: 1.481341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [419840/500434 (84%)] Loss: 1.610354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [430080/500434 (86%)] Loss: 1.466721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [440320/500434 (88%)] Loss: 1.581801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [450560/500434 (90%)] Loss: 1.531023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [460800/500434 (92%)] Loss: 1.611934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [471040/500434 (94%)] Loss: 1.473632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [481280/500434 (96%)] Loss: 1.587284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [491520/500434 (98%)] Loss: 1.522000\u001b[0m\n",
      "\u001b[34mcurrent epoch: 59\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [0/500434 (0%)] Loss: 1.545580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [10240/500434 (2%)] Loss: 1.615443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [20480/500434 (4%)] Loss: 1.585602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [30720/500434 (6%)] Loss: 1.513805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [40960/500434 (8%)] Loss: 1.721567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [51200/500434 (10%)] Loss: 1.563580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [61440/500434 (12%)] Loss: 1.519643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [71680/500434 (14%)] Loss: 1.544421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [81920/500434 (16%)] Loss: 1.466036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [92160/500434 (18%)] Loss: 1.456605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [102400/500434 (20%)] Loss: 1.606712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [112640/500434 (22%)] Loss: 1.540963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [122880/500434 (25%)] Loss: 1.531374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [133120/500434 (27%)] Loss: 1.419395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [143360/500434 (29%)] Loss: 1.616205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [153600/500434 (31%)] Loss: 1.492179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [163840/500434 (33%)] Loss: 1.616910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [174080/500434 (35%)] Loss: 1.627404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [184320/500434 (37%)] Loss: 1.491918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [194560/500434 (39%)] Loss: 1.603617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [204800/500434 (41%)] Loss: 1.566137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [215040/500434 (43%)] Loss: 1.649279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [225280/500434 (45%)] Loss: 1.567136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [235520/500434 (47%)] Loss: 1.598135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [245760/500434 (49%)] Loss: 1.520197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [256000/500434 (51%)] Loss: 1.539636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [266240/500434 (53%)] Loss: 1.508713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [276480/500434 (55%)] Loss: 1.562769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [286720/500434 (57%)] Loss: 1.487934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [296960/500434 (59%)] Loss: 1.612826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [307200/500434 (61%)] Loss: 1.593313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [317440/500434 (63%)] Loss: 1.605025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [327680/500434 (65%)] Loss: 1.512506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [337920/500434 (67%)] Loss: 1.487126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [348160/500434 (70%)] Loss: 1.607512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [358400/500434 (72%)] Loss: 1.564429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [368640/500434 (74%)] Loss: 1.342596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [378880/500434 (76%)] Loss: 1.585527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [389120/500434 (78%)] Loss: 1.554918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [399360/500434 (80%)] Loss: 1.498446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [409600/500434 (82%)] Loss: 1.564465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [419840/500434 (84%)] Loss: 1.476530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [430080/500434 (86%)] Loss: 1.490200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [440320/500434 (88%)] Loss: 1.564242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [450560/500434 (90%)] Loss: 1.682796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [460800/500434 (92%)] Loss: 1.472039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [471040/500434 (94%)] Loss: 1.487697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [481280/500434 (96%)] Loss: 1.669740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [491520/500434 (98%)] Loss: 1.569041\u001b[0m\n",
      "\u001b[34mcurrent epoch: 60\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [0/500434 (0%)] Loss: 1.537299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [10240/500434 (2%)] Loss: 1.564887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [20480/500434 (4%)] Loss: 1.516878\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [30720/500434 (6%)] Loss: 1.534777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [40960/500434 (8%)] Loss: 1.598296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [51200/500434 (10%)] Loss: 1.541083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [61440/500434 (12%)] Loss: 1.548765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [71680/500434 (14%)] Loss: 1.559477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [81920/500434 (16%)] Loss: 1.609194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [92160/500434 (18%)] Loss: 1.522115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [102400/500434 (20%)] Loss: 1.527332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [112640/500434 (22%)] Loss: 1.529033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [122880/500434 (25%)] Loss: 1.455404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [133120/500434 (27%)] Loss: 1.550376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [143360/500434 (29%)] Loss: 1.580686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [153600/500434 (31%)] Loss: 1.604502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [163840/500434 (33%)] Loss: 1.554241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [174080/500434 (35%)] Loss: 1.662239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [184320/500434 (37%)] Loss: 1.565146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [194560/500434 (39%)] Loss: 1.612213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [204800/500434 (41%)] Loss: 1.534778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [215040/500434 (43%)] Loss: 1.470309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [225280/500434 (45%)] Loss: 1.629355\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [235520/500434 (47%)] Loss: 1.588820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [245760/500434 (49%)] Loss: 1.559927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [256000/500434 (51%)] Loss: 1.496637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [266240/500434 (53%)] Loss: 1.641637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [276480/500434 (55%)] Loss: 1.463984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [286720/500434 (57%)] Loss: 1.544003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [296960/500434 (59%)] Loss: 1.610239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [307200/500434 (61%)] Loss: 1.545517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [317440/500434 (63%)] Loss: 1.574386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [327680/500434 (65%)] Loss: 1.620091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [337920/500434 (67%)] Loss: 1.509733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [348160/500434 (70%)] Loss: 1.562335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [358400/500434 (72%)] Loss: 1.548098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [368640/500434 (74%)] Loss: 1.470886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [378880/500434 (76%)] Loss: 1.558915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [389120/500434 (78%)] Loss: 1.635136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [399360/500434 (80%)] Loss: 1.495296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [409600/500434 (82%)] Loss: 1.531092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [419840/500434 (84%)] Loss: 1.657028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [430080/500434 (86%)] Loss: 1.591206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [440320/500434 (88%)] Loss: 1.644068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [450560/500434 (90%)] Loss: 1.574089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [460800/500434 (92%)] Loss: 1.579033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [471040/500434 (94%)] Loss: 1.548632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [481280/500434 (96%)] Loss: 1.526162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [491520/500434 (98%)] Loss: 1.543007\u001b[0m\n",
      "\u001b[34mcurrent epoch: 61\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [0/500434 (0%)] Loss: 1.570485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [10240/500434 (2%)] Loss: 1.520688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [20480/500434 (4%)] Loss: 1.560076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [30720/500434 (6%)] Loss: 1.541889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [40960/500434 (8%)] Loss: 1.588086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [51200/500434 (10%)] Loss: 1.465735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [61440/500434 (12%)] Loss: 1.574177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [71680/500434 (14%)] Loss: 1.495064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [81920/500434 (16%)] Loss: 1.608433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [92160/500434 (18%)] Loss: 1.525828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [102400/500434 (20%)] Loss: 1.589557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [112640/500434 (22%)] Loss: 1.472888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [122880/500434 (25%)] Loss: 1.445256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [133120/500434 (27%)] Loss: 1.559410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [143360/500434 (29%)] Loss: 1.624034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [153600/500434 (31%)] Loss: 1.690117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [163840/500434 (33%)] Loss: 1.504338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [174080/500434 (35%)] Loss: 1.517633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [184320/500434 (37%)] Loss: 1.494496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [194560/500434 (39%)] Loss: 1.561116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [204800/500434 (41%)] Loss: 1.512099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [215040/500434 (43%)] Loss: 1.626778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [225280/500434 (45%)] Loss: 1.492553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [235520/500434 (47%)] Loss: 1.465871\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [245760/500434 (49%)] Loss: 1.575323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [256000/500434 (51%)] Loss: 1.632384\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [266240/500434 (53%)] Loss: 1.521280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [276480/500434 (55%)] Loss: 1.511754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [286720/500434 (57%)] Loss: 1.480328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [296960/500434 (59%)] Loss: 1.526599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [307200/500434 (61%)] Loss: 1.569021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [317440/500434 (63%)] Loss: 1.544146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [327680/500434 (65%)] Loss: 1.671258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [337920/500434 (67%)] Loss: 1.585566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [348160/500434 (70%)] Loss: 1.462183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [358400/500434 (72%)] Loss: 1.585908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [368640/500434 (74%)] Loss: 1.547543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [378880/500434 (76%)] Loss: 1.549100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [389120/500434 (78%)] Loss: 1.475585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [399360/500434 (80%)] Loss: 1.540099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [409600/500434 (82%)] Loss: 1.731944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [419840/500434 (84%)] Loss: 1.613486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [430080/500434 (86%)] Loss: 1.461950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [440320/500434 (88%)] Loss: 1.574370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [450560/500434 (90%)] Loss: 1.531966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [460800/500434 (92%)] Loss: 1.521038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [471040/500434 (94%)] Loss: 1.519125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [481280/500434 (96%)] Loss: 1.608856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [491520/500434 (98%)] Loss: 1.562686\u001b[0m\n",
      "\u001b[34mcurrent epoch: 62\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [0/500434 (0%)] Loss: 1.479927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [10240/500434 (2%)] Loss: 1.630409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [20480/500434 (4%)] Loss: 1.616749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [30720/500434 (6%)] Loss: 1.504601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [40960/500434 (8%)] Loss: 1.504231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [51200/500434 (10%)] Loss: 1.566148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [61440/500434 (12%)] Loss: 1.544321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [71680/500434 (14%)] Loss: 1.528279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [81920/500434 (16%)] Loss: 1.627976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [92160/500434 (18%)] Loss: 1.484785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [102400/500434 (20%)] Loss: 1.626974\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [112640/500434 (22%)] Loss: 1.485524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [122880/500434 (25%)] Loss: 1.607618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [133120/500434 (27%)] Loss: 1.642124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [143360/500434 (29%)] Loss: 1.652760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [153600/500434 (31%)] Loss: 1.504928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [163840/500434 (33%)] Loss: 1.614455\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [174080/500434 (35%)] Loss: 1.648319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [184320/500434 (37%)] Loss: 28.010433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [194560/500434 (39%)] Loss: 1.662367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [204800/500434 (41%)] Loss: 1.678698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [215040/500434 (43%)] Loss: 1.906518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [225280/500434 (45%)] Loss: 1.612992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [235520/500434 (47%)] Loss: 1.594816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [245760/500434 (49%)] Loss: 1.562969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [256000/500434 (51%)] Loss: 1.495131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [266240/500434 (53%)] Loss: 1.627397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [276480/500434 (55%)] Loss: 1.509327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [286720/500434 (57%)] Loss: 1.620233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [296960/500434 (59%)] Loss: 1.548144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [307200/500434 (61%)] Loss: 1.508618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [317440/500434 (63%)] Loss: 1.626293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [327680/500434 (65%)] Loss: 1.586186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [337920/500434 (67%)] Loss: 1.598647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [348160/500434 (70%)] Loss: 1.511289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [358400/500434 (72%)] Loss: 1.566732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [368640/500434 (74%)] Loss: 1.518867\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [378880/500434 (76%)] Loss: 1.531970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [389120/500434 (78%)] Loss: 1.517263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [399360/500434 (80%)] Loss: 1.613136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [409600/500434 (82%)] Loss: 1.507649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [419840/500434 (84%)] Loss: 1.464116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [430080/500434 (86%)] Loss: 1.632364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [440320/500434 (88%)] Loss: 1.417166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [450560/500434 (90%)] Loss: 1.596772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [460800/500434 (92%)] Loss: 1.601362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [471040/500434 (94%)] Loss: 1.567926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [481280/500434 (96%)] Loss: 1.500280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [491520/500434 (98%)] Loss: 1.596195\u001b[0m\n",
      "\u001b[34mcurrent epoch: 63\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [0/500434 (0%)] Loss: 1.608512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [10240/500434 (2%)] Loss: 1.657102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [20480/500434 (4%)] Loss: 1.544513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [30720/500434 (6%)] Loss: 1.590831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [40960/500434 (8%)] Loss: 1.591233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [51200/500434 (10%)] Loss: 1.712799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [61440/500434 (12%)] Loss: 1.603293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [71680/500434 (14%)] Loss: 1.474012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [81920/500434 (16%)] Loss: 1.674160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [92160/500434 (18%)] Loss: 1.637899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [102400/500434 (20%)] Loss: 1.498300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [112640/500434 (22%)] Loss: 1.547253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [122880/500434 (25%)] Loss: 1.626626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [133120/500434 (27%)] Loss: 1.541923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [143360/500434 (29%)] Loss: 1.482291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [153600/500434 (31%)] Loss: 1.538841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [163840/500434 (33%)] Loss: 1.427390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [174080/500434 (35%)] Loss: 1.530804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [184320/500434 (37%)] Loss: 1.523374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [194560/500434 (39%)] Loss: 1.687900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [204800/500434 (41%)] Loss: 1.513513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [215040/500434 (43%)] Loss: 1.537065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [225280/500434 (45%)] Loss: 1.583181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [235520/500434 (47%)] Loss: 1.628375\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [245760/500434 (49%)] Loss: 1.556801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [256000/500434 (51%)] Loss: 1.573633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [266240/500434 (53%)] Loss: 1.437985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [276480/500434 (55%)] Loss: 1.517968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [286720/500434 (57%)] Loss: 1.512908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [296960/500434 (59%)] Loss: 1.502006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [307200/500434 (61%)] Loss: 1.619927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [317440/500434 (63%)] Loss: 1.528315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [327680/500434 (65%)] Loss: 1.482036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [337920/500434 (67%)] Loss: 1.575993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [348160/500434 (70%)] Loss: 1.638612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [358400/500434 (72%)] Loss: 1.552530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [368640/500434 (74%)] Loss: 1.568915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [378880/500434 (76%)] Loss: 1.649088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [389120/500434 (78%)] Loss: 1.560880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [399360/500434 (80%)] Loss: 1.598537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [409600/500434 (82%)] Loss: 1.526903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [419840/500434 (84%)] Loss: 1.514714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [430080/500434 (86%)] Loss: 1.526761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [440320/500434 (88%)] Loss: 1.508285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [450560/500434 (90%)] Loss: 1.555085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [460800/500434 (92%)] Loss: 1.539379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [471040/500434 (94%)] Loss: 1.543724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [481280/500434 (96%)] Loss: 1.596321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [491520/500434 (98%)] Loss: 1.626224\u001b[0m\n",
      "\u001b[34mcurrent epoch: 64\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [0/500434 (0%)] Loss: 1.494941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [10240/500434 (2%)] Loss: 1.601233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [20480/500434 (4%)] Loss: 1.533325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [30720/500434 (6%)] Loss: 1.501448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [40960/500434 (8%)] Loss: 1.482532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [51200/500434 (10%)] Loss: 1.484619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [61440/500434 (12%)] Loss: 1.589009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [71680/500434 (14%)] Loss: 1.537014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [81920/500434 (16%)] Loss: 1.578816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [92160/500434 (18%)] Loss: 1.495005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [102400/500434 (20%)] Loss: 1.560786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [112640/500434 (22%)] Loss: 1.668723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [122880/500434 (25%)] Loss: 1.522354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [133120/500434 (27%)] Loss: 1.574989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [143360/500434 (29%)] Loss: 1.452088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [153600/500434 (31%)] Loss: 1.458564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [163840/500434 (33%)] Loss: 1.717140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [174080/500434 (35%)] Loss: 1.614434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [184320/500434 (37%)] Loss: 1.619763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [194560/500434 (39%)] Loss: 1.591852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [204800/500434 (41%)] Loss: 1.595611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [215040/500434 (43%)] Loss: 1.566043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [225280/500434 (45%)] Loss: 1.585791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [235520/500434 (47%)] Loss: 1.638710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [245760/500434 (49%)] Loss: 1.567781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [256000/500434 (51%)] Loss: 1.446965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [266240/500434 (53%)] Loss: 1.477886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [276480/500434 (55%)] Loss: 1.490496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [286720/500434 (57%)] Loss: 1.531548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [296960/500434 (59%)] Loss: 1.543964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [307200/500434 (61%)] Loss: 1.573231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [317440/500434 (63%)] Loss: 1.585792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [327680/500434 (65%)] Loss: 1.577942\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [337920/500434 (67%)] Loss: 1.542269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [348160/500434 (70%)] Loss: 1.532576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [358400/500434 (72%)] Loss: 1.488683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [368640/500434 (74%)] Loss: 1.537219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [378880/500434 (76%)] Loss: 1.518581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [389120/500434 (78%)] Loss: 1.428070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [399360/500434 (80%)] Loss: 1.561045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [409600/500434 (82%)] Loss: 1.548982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [419840/500434 (84%)] Loss: 1.711136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [430080/500434 (86%)] Loss: 1.456629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [440320/500434 (88%)] Loss: 1.602973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [450560/500434 (90%)] Loss: 1.545883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [460800/500434 (92%)] Loss: 1.570843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [471040/500434 (94%)] Loss: 1.580456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [481280/500434 (96%)] Loss: 1.558484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [491520/500434 (98%)] Loss: 1.506219\u001b[0m\n",
      "\u001b[34mcurrent epoch: 65\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [0/500434 (0%)] Loss: 1.520282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [10240/500434 (2%)] Loss: 1.512835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [20480/500434 (4%)] Loss: 1.397603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [30720/500434 (6%)] Loss: 1.493079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [40960/500434 (8%)] Loss: 1.533670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [51200/500434 (10%)] Loss: 1.575676\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [61440/500434 (12%)] Loss: 1.646260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [71680/500434 (14%)] Loss: 1.524371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [81920/500434 (16%)] Loss: 1.618456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [92160/500434 (18%)] Loss: 1.547690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [102400/500434 (20%)] Loss: 1.469955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [112640/500434 (22%)] Loss: 1.577246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [122880/500434 (25%)] Loss: 1.490072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [133120/500434 (27%)] Loss: 1.516931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [143360/500434 (29%)] Loss: 1.533061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [153600/500434 (31%)] Loss: 1.522526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [163840/500434 (33%)] Loss: 1.544912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [174080/500434 (35%)] Loss: 1.484865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [184320/500434 (37%)] Loss: 1.521510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [194560/500434 (39%)] Loss: 1.546179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [204800/500434 (41%)] Loss: 1.527247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [215040/500434 (43%)] Loss: 1.487149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [225280/500434 (45%)] Loss: 1.523471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [235520/500434 (47%)] Loss: 1.603269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [245760/500434 (49%)] Loss: 1.797225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [256000/500434 (51%)] Loss: 1.530018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [266240/500434 (53%)] Loss: 1.390463\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [276480/500434 (55%)] Loss: 1.506446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [286720/500434 (57%)] Loss: 1.515493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [296960/500434 (59%)] Loss: 1.505815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [307200/500434 (61%)] Loss: 1.583012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [317440/500434 (63%)] Loss: 1.538934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [327680/500434 (65%)] Loss: 1.552149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [337920/500434 (67%)] Loss: 1.616687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [348160/500434 (70%)] Loss: 1.573368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [358400/500434 (72%)] Loss: 1.552747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [368640/500434 (74%)] Loss: 1.526892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [378880/500434 (76%)] Loss: 1.512950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [389120/500434 (78%)] Loss: 1.466070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [399360/500434 (80%)] Loss: 1.545356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [409600/500434 (82%)] Loss: 1.643627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [419840/500434 (84%)] Loss: 1.547520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [430080/500434 (86%)] Loss: 1.525262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [440320/500434 (88%)] Loss: 1.483284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [450560/500434 (90%)] Loss: 1.389321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [460800/500434 (92%)] Loss: 1.501752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [471040/500434 (94%)] Loss: 1.404824\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [481280/500434 (96%)] Loss: 1.526518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [491520/500434 (98%)] Loss: 1.564755\u001b[0m\n",
      "\u001b[34mcurrent epoch: 66\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [0/500434 (0%)] Loss: 1.604246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [10240/500434 (2%)] Loss: 1.535518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [20480/500434 (4%)] Loss: 1.566258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [30720/500434 (6%)] Loss: 1.617067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [40960/500434 (8%)] Loss: 1.545167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [51200/500434 (10%)] Loss: 1.482414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [61440/500434 (12%)] Loss: 1.652026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [71680/500434 (14%)] Loss: 1.487207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [81920/500434 (16%)] Loss: 1.538194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [92160/500434 (18%)] Loss: 1.491177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [102400/500434 (20%)] Loss: 1.514381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [112640/500434 (22%)] Loss: 1.434167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [122880/500434 (25%)] Loss: 1.524875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [133120/500434 (27%)] Loss: 1.485932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [143360/500434 (29%)] Loss: 1.609380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [153600/500434 (31%)] Loss: 1.567773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [163840/500434 (33%)] Loss: 1.490460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [174080/500434 (35%)] Loss: 1.602481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [184320/500434 (37%)] Loss: 1.515095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [194560/500434 (39%)] Loss: 1.612066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [204800/500434 (41%)] Loss: 1.498175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [215040/500434 (43%)] Loss: 1.575044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [225280/500434 (45%)] Loss: 1.544314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [235520/500434 (47%)] Loss: 1.630365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [245760/500434 (49%)] Loss: 1.602083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [256000/500434 (51%)] Loss: 1.467016\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [266240/500434 (53%)] Loss: 1.562065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [276480/500434 (55%)] Loss: 1.565591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [286720/500434 (57%)] Loss: 1.555660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [296960/500434 (59%)] Loss: 1.552227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [307200/500434 (61%)] Loss: 1.533019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [317440/500434 (63%)] Loss: 1.562833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [327680/500434 (65%)] Loss: 1.570357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [337920/500434 (67%)] Loss: 1.639604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [348160/500434 (70%)] Loss: 1.535810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [358400/500434 (72%)] Loss: 1.536345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [368640/500434 (74%)] Loss: 1.726538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [378880/500434 (76%)] Loss: 1.539242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [389120/500434 (78%)] Loss: 1.509440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [399360/500434 (80%)] Loss: 1.538911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [409600/500434 (82%)] Loss: 1.453800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [419840/500434 (84%)] Loss: 1.470676\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [430080/500434 (86%)] Loss: 1.475094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [440320/500434 (88%)] Loss: 1.608649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [450560/500434 (90%)] Loss: 1.535417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [460800/500434 (92%)] Loss: 1.654986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [471040/500434 (94%)] Loss: 1.486304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [481280/500434 (96%)] Loss: 1.471319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [491520/500434 (98%)] Loss: 1.609015\u001b[0m\n",
      "\u001b[34mcurrent epoch: 67\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [0/500434 (0%)] Loss: 1.565578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [10240/500434 (2%)] Loss: 1.595358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [20480/500434 (4%)] Loss: 1.472763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [30720/500434 (6%)] Loss: 1.574937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [40960/500434 (8%)] Loss: 1.468507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [51200/500434 (10%)] Loss: 1.461218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [61440/500434 (12%)] Loss: 1.583142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [71680/500434 (14%)] Loss: 1.529909\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [81920/500434 (16%)] Loss: 1.465943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [92160/500434 (18%)] Loss: 1.678112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [102400/500434 (20%)] Loss: 1.507562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [112640/500434 (22%)] Loss: 1.531813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [122880/500434 (25%)] Loss: 1.499213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [133120/500434 (27%)] Loss: 1.525612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [143360/500434 (29%)] Loss: 1.467782\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [153600/500434 (31%)] Loss: 1.553744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [163840/500434 (33%)] Loss: 1.456135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [174080/500434 (35%)] Loss: 1.507916\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [184320/500434 (37%)] Loss: 1.587811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [194560/500434 (39%)] Loss: 1.491817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [204800/500434 (41%)] Loss: 1.290452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [215040/500434 (43%)] Loss: 1.469457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [225280/500434 (45%)] Loss: 1.557053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [235520/500434 (47%)] Loss: 1.573479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [245760/500434 (49%)] Loss: 1.712451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [256000/500434 (51%)] Loss: 1.617419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [266240/500434 (53%)] Loss: 1.657919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [276480/500434 (55%)] Loss: 1.452260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [286720/500434 (57%)] Loss: 1.587734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [296960/500434 (59%)] Loss: 1.467048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [307200/500434 (61%)] Loss: 1.597448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [317440/500434 (63%)] Loss: 1.418680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [327680/500434 (65%)] Loss: 1.569319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [337920/500434 (67%)] Loss: 1.494306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [348160/500434 (70%)] Loss: 1.604295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [358400/500434 (72%)] Loss: 1.539481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [368640/500434 (74%)] Loss: 1.450047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [378880/500434 (76%)] Loss: 1.474075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [389120/500434 (78%)] Loss: 1.559204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [399360/500434 (80%)] Loss: 1.481290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [409600/500434 (82%)] Loss: 1.520278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [419840/500434 (84%)] Loss: 1.449822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [430080/500434 (86%)] Loss: 1.429940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [440320/500434 (88%)] Loss: 1.592598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [450560/500434 (90%)] Loss: 1.653434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [460800/500434 (92%)] Loss: 1.560434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [471040/500434 (94%)] Loss: 1.458688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [481280/500434 (96%)] Loss: 1.596574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [491520/500434 (98%)] Loss: 1.486574\u001b[0m\n",
      "\u001b[34mcurrent epoch: 68\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [0/500434 (0%)] Loss: 1.445557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [10240/500434 (2%)] Loss: 1.535144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [20480/500434 (4%)] Loss: 1.555953\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [30720/500434 (6%)] Loss: 1.614970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [40960/500434 (8%)] Loss: 1.559618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [51200/500434 (10%)] Loss: 1.525244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [61440/500434 (12%)] Loss: 1.547840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [71680/500434 (14%)] Loss: 1.501563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [81920/500434 (16%)] Loss: 1.426685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [92160/500434 (18%)] Loss: 1.574447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [102400/500434 (20%)] Loss: 1.554970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [112640/500434 (22%)] Loss: 1.528549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [122880/500434 (25%)] Loss: 1.492500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [133120/500434 (27%)] Loss: 1.560835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [143360/500434 (29%)] Loss: 1.526670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [153600/500434 (31%)] Loss: 1.515021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [163840/500434 (33%)] Loss: 1.484294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [174080/500434 (35%)] Loss: 1.524193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [184320/500434 (37%)] Loss: 1.493886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [194560/500434 (39%)] Loss: 1.558549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [204800/500434 (41%)] Loss: 1.536644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [215040/500434 (43%)] Loss: 1.538295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [225280/500434 (45%)] Loss: 1.709109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [235520/500434 (47%)] Loss: 1.473552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [245760/500434 (49%)] Loss: 1.469217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [256000/500434 (51%)] Loss: 1.585180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [266240/500434 (53%)] Loss: 1.425182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [276480/500434 (55%)] Loss: 1.584908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [286720/500434 (57%)] Loss: 1.455772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [296960/500434 (59%)] Loss: 1.544265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [307200/500434 (61%)] Loss: 1.467590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [317440/500434 (63%)] Loss: 1.480612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [327680/500434 (65%)] Loss: 1.707788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [337920/500434 (67%)] Loss: 1.613308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [348160/500434 (70%)] Loss: 1.520078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [358400/500434 (72%)] Loss: 1.578677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [368640/500434 (74%)] Loss: 1.545871\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [378880/500434 (76%)] Loss: 1.577008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [389120/500434 (78%)] Loss: 1.679379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [399360/500434 (80%)] Loss: 1.573439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [409600/500434 (82%)] Loss: 1.491006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [419840/500434 (84%)] Loss: 1.538166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [430080/500434 (86%)] Loss: 1.530818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [440320/500434 (88%)] Loss: 1.523432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [450560/500434 (90%)] Loss: 1.500527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [460800/500434 (92%)] Loss: 1.525464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [471040/500434 (94%)] Loss: 1.456231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [481280/500434 (96%)] Loss: 1.572070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [491520/500434 (98%)] Loss: 1.486988\u001b[0m\n",
      "\u001b[34mcurrent epoch: 69\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [0/500434 (0%)] Loss: 1.586894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [10240/500434 (2%)] Loss: 1.434353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [20480/500434 (4%)] Loss: 1.474600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [30720/500434 (6%)] Loss: 1.521525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [40960/500434 (8%)] Loss: 1.537145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [51200/500434 (10%)] Loss: 1.631511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [61440/500434 (12%)] Loss: 1.619708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [71680/500434 (14%)] Loss: 1.636654\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [81920/500434 (16%)] Loss: 1.513363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [92160/500434 (18%)] Loss: 1.655145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [102400/500434 (20%)] Loss: 1.671392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [112640/500434 (22%)] Loss: 1.566946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [122880/500434 (25%)] Loss: 1.548917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [133120/500434 (27%)] Loss: 1.559399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [143360/500434 (29%)] Loss: 1.591609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [153600/500434 (31%)] Loss: 1.563561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [163840/500434 (33%)] Loss: 1.659030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [174080/500434 (35%)] Loss: 1.522857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [184320/500434 (37%)] Loss: 1.526000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [194560/500434 (39%)] Loss: 1.503657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [204800/500434 (41%)] Loss: 1.588547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [215040/500434 (43%)] Loss: 1.641190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [225280/500434 (45%)] Loss: 1.609702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [235520/500434 (47%)] Loss: 1.573197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [245760/500434 (49%)] Loss: 1.469972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [256000/500434 (51%)] Loss: 1.488219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [266240/500434 (53%)] Loss: 1.504639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [276480/500434 (55%)] Loss: 1.534181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [286720/500434 (57%)] Loss: 1.526190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [296960/500434 (59%)] Loss: 1.861972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [307200/500434 (61%)] Loss: 1.560926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [317440/500434 (63%)] Loss: 1.563586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [327680/500434 (65%)] Loss: 1.554123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [337920/500434 (67%)] Loss: 1.472314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [348160/500434 (70%)] Loss: 1.514894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [358400/500434 (72%)] Loss: 1.513454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [368640/500434 (74%)] Loss: 1.443363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [378880/500434 (76%)] Loss: 1.575574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [389120/500434 (78%)] Loss: 1.486925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [399360/500434 (80%)] Loss: 1.704940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [409600/500434 (82%)] Loss: 1.545139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [419840/500434 (84%)] Loss: 1.625955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [430080/500434 (86%)] Loss: 1.473215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [440320/500434 (88%)] Loss: 1.578339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [450560/500434 (90%)] Loss: 1.390473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [460800/500434 (92%)] Loss: 1.558439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [471040/500434 (94%)] Loss: 1.627991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [481280/500434 (96%)] Loss: 1.625808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [491520/500434 (98%)] Loss: 1.530137\u001b[0m\n",
      "\u001b[34mcurrent epoch: 70\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [0/500434 (0%)] Loss: 1.535291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [10240/500434 (2%)] Loss: 1.557756\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [20480/500434 (4%)] Loss: 1.453499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [30720/500434 (6%)] Loss: 1.545961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [40960/500434 (8%)] Loss: 1.485856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [51200/500434 (10%)] Loss: 1.597517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [61440/500434 (12%)] Loss: 1.533656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [71680/500434 (14%)] Loss: 1.539668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [81920/500434 (16%)] Loss: 1.457212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [92160/500434 (18%)] Loss: 1.587481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [102400/500434 (20%)] Loss: 1.511602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [112640/500434 (22%)] Loss: 1.669929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [122880/500434 (25%)] Loss: 1.558227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [133120/500434 (27%)] Loss: 1.553309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [143360/500434 (29%)] Loss: 1.507372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [153600/500434 (31%)] Loss: 1.572989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [163840/500434 (33%)] Loss: 1.473025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [174080/500434 (35%)] Loss: 1.492882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [184320/500434 (37%)] Loss: 1.569917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [194560/500434 (39%)] Loss: 1.578456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [204800/500434 (41%)] Loss: 1.587182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [215040/500434 (43%)] Loss: 1.530681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [225280/500434 (45%)] Loss: 1.509094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [235520/500434 (47%)] Loss: 1.488410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [245760/500434 (49%)] Loss: 1.538097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [256000/500434 (51%)] Loss: 1.525170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [266240/500434 (53%)] Loss: 1.576999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [276480/500434 (55%)] Loss: 1.587488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [286720/500434 (57%)] Loss: 1.493297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [296960/500434 (59%)] Loss: 1.544304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [307200/500434 (61%)] Loss: 1.618968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [317440/500434 (63%)] Loss: 1.542706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [327680/500434 (65%)] Loss: 1.571931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [337920/500434 (67%)] Loss: 1.382010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [348160/500434 (70%)] Loss: 1.557332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [358400/500434 (72%)] Loss: 1.504085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [368640/500434 (74%)] Loss: 1.446451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [378880/500434 (76%)] Loss: 1.452290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [389120/500434 (78%)] Loss: 1.511160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [399360/500434 (80%)] Loss: 1.604219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [409600/500434 (82%)] Loss: 1.603407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [419840/500434 (84%)] Loss: 1.576444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [430080/500434 (86%)] Loss: 1.497131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [440320/500434 (88%)] Loss: 1.566987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [450560/500434 (90%)] Loss: 1.454280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [460800/500434 (92%)] Loss: 1.648622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [471040/500434 (94%)] Loss: 1.588422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [481280/500434 (96%)] Loss: 1.584303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [491520/500434 (98%)] Loss: 1.510344\u001b[0m\n",
      "\u001b[34mcurrent epoch: 71\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [0/500434 (0%)] Loss: 1.502532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [10240/500434 (2%)] Loss: 1.479080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [20480/500434 (4%)] Loss: 1.488845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [30720/500434 (6%)] Loss: 1.467018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [40960/500434 (8%)] Loss: 1.511444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [51200/500434 (10%)] Loss: 1.558774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [61440/500434 (12%)] Loss: 1.409506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [71680/500434 (14%)] Loss: 1.601155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [81920/500434 (16%)] Loss: 1.491079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [92160/500434 (18%)] Loss: 1.483805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [102400/500434 (20%)] Loss: 1.443539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [112640/500434 (22%)] Loss: 1.507133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [122880/500434 (25%)] Loss: 1.638577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [133120/500434 (27%)] Loss: 1.477210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [143360/500434 (29%)] Loss: 1.465912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [153600/500434 (31%)] Loss: 1.506760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [163840/500434 (33%)] Loss: 1.468125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [174080/500434 (35%)] Loss: 1.423306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [184320/500434 (37%)] Loss: 1.537933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [194560/500434 (39%)] Loss: 1.476391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [204800/500434 (41%)] Loss: 2.297640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [215040/500434 (43%)] Loss: 17.911734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [225280/500434 (45%)] Loss: 10.360760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [235520/500434 (47%)] Loss: 3.482843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [245760/500434 (49%)] Loss: 2.602085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [256000/500434 (51%)] Loss: 2.356589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [266240/500434 (53%)] Loss: 2.202784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [276480/500434 (55%)] Loss: 2.047500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [286720/500434 (57%)] Loss: 2.164493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [296960/500434 (59%)] Loss: 1.935357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [307200/500434 (61%)] Loss: 1.799334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [317440/500434 (63%)] Loss: 1.755277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [327680/500434 (65%)] Loss: 1.781769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [337920/500434 (67%)] Loss: 1.766978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [348160/500434 (70%)] Loss: 1.751825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [358400/500434 (72%)] Loss: 1.643183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [368640/500434 (74%)] Loss: 1.770088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [378880/500434 (76%)] Loss: 1.708598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [389120/500434 (78%)] Loss: 1.615128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [399360/500434 (80%)] Loss: 1.600911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [409600/500434 (82%)] Loss: 1.597906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [419840/500434 (84%)] Loss: 1.590263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [430080/500434 (86%)] Loss: 1.588650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [440320/500434 (88%)] Loss: 1.486976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [450560/500434 (90%)] Loss: 1.662787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [460800/500434 (92%)] Loss: 1.562031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [471040/500434 (94%)] Loss: 1.662396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [481280/500434 (96%)] Loss: 1.724743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [491520/500434 (98%)] Loss: 1.594943\u001b[0m\n",
      "\u001b[34mcurrent epoch: 72\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [0/500434 (0%)] Loss: 1.562532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [10240/500434 (2%)] Loss: 1.647499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [20480/500434 (4%)] Loss: 1.569594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [30720/500434 (6%)] Loss: 1.680656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [40960/500434 (8%)] Loss: 1.531359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [51200/500434 (10%)] Loss: 1.575289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [61440/500434 (12%)] Loss: 1.548950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [71680/500434 (14%)] Loss: 1.558072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [81920/500434 (16%)] Loss: 1.571060\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [92160/500434 (18%)] Loss: 1.587320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [102400/500434 (20%)] Loss: 1.559096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [112640/500434 (22%)] Loss: 1.496933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [122880/500434 (25%)] Loss: 1.492112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [133120/500434 (27%)] Loss: 1.442960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [143360/500434 (29%)] Loss: 1.476015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [153600/500434 (31%)] Loss: 1.471695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [163840/500434 (33%)] Loss: 1.506629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [174080/500434 (35%)] Loss: 1.480281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [184320/500434 (37%)] Loss: 1.685835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [194560/500434 (39%)] Loss: 1.525426\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [204800/500434 (41%)] Loss: 1.534091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [215040/500434 (43%)] Loss: 1.679969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [225280/500434 (45%)] Loss: 1.584123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [235520/500434 (47%)] Loss: 1.620201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [245760/500434 (49%)] Loss: 1.503353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [256000/500434 (51%)] Loss: 1.526134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [266240/500434 (53%)] Loss: 1.475446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [276480/500434 (55%)] Loss: 1.421133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [286720/500434 (57%)] Loss: 1.539623\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [296960/500434 (59%)] Loss: 1.591612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [307200/500434 (61%)] Loss: 1.588615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [317440/500434 (63%)] Loss: 1.491421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [327680/500434 (65%)] Loss: 1.647491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [337920/500434 (67%)] Loss: 1.510077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [348160/500434 (70%)] Loss: 1.589881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [358400/500434 (72%)] Loss: 1.514581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [368640/500434 (74%)] Loss: 1.510192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [378880/500434 (76%)] Loss: 1.519743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [389120/500434 (78%)] Loss: 1.575779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [399360/500434 (80%)] Loss: 1.533352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [409600/500434 (82%)] Loss: 1.639255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [419840/500434 (84%)] Loss: 1.442036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [430080/500434 (86%)] Loss: 1.517980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [440320/500434 (88%)] Loss: 1.529898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [450560/500434 (90%)] Loss: 1.533959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [460800/500434 (92%)] Loss: 1.523385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [471040/500434 (94%)] Loss: 1.575481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [481280/500434 (96%)] Loss: 1.568018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [491520/500434 (98%)] Loss: 1.765532\u001b[0m\n",
      "\u001b[34mcurrent epoch: 73\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [0/500434 (0%)] Loss: 1.550890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [10240/500434 (2%)] Loss: 1.674003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [20480/500434 (4%)] Loss: 1.458025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [30720/500434 (6%)] Loss: 1.491235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [40960/500434 (8%)] Loss: 1.525335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [51200/500434 (10%)] Loss: 1.524576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [61440/500434 (12%)] Loss: 1.451057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [71680/500434 (14%)] Loss: 1.593995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [81920/500434 (16%)] Loss: 1.501083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [92160/500434 (18%)] Loss: 1.543560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [102400/500434 (20%)] Loss: 1.447572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [112640/500434 (22%)] Loss: 1.517555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [122880/500434 (25%)] Loss: 1.420729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [133120/500434 (27%)] Loss: 1.513273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [143360/500434 (29%)] Loss: 1.371306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [153600/500434 (31%)] Loss: 1.520182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [163840/500434 (33%)] Loss: 1.450231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [174080/500434 (35%)] Loss: 1.541912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [184320/500434 (37%)] Loss: 1.507896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [194560/500434 (39%)] Loss: 1.472702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [204800/500434 (41%)] Loss: 1.430019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [215040/500434 (43%)] Loss: 1.495602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [225280/500434 (45%)] Loss: 1.567618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [235520/500434 (47%)] Loss: 1.539587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [245760/500434 (49%)] Loss: 1.474146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [256000/500434 (51%)] Loss: 1.590349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [266240/500434 (53%)] Loss: 1.522578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [276480/500434 (55%)] Loss: 1.463932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [286720/500434 (57%)] Loss: 1.473739\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [296960/500434 (59%)] Loss: 1.417730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [307200/500434 (61%)] Loss: 1.519686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [317440/500434 (63%)] Loss: 1.540098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [327680/500434 (65%)] Loss: 1.623705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [337920/500434 (67%)] Loss: 1.502094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [348160/500434 (70%)] Loss: 1.513760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [358400/500434 (72%)] Loss: 1.504132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [368640/500434 (74%)] Loss: 1.505304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [378880/500434 (76%)] Loss: 1.618722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [389120/500434 (78%)] Loss: 1.680783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [399360/500434 (80%)] Loss: 1.525657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [409600/500434 (82%)] Loss: 1.441986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [419840/500434 (84%)] Loss: 1.530733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [430080/500434 (86%)] Loss: 1.441234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [440320/500434 (88%)] Loss: 1.484045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [450560/500434 (90%)] Loss: 1.443418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [460800/500434 (92%)] Loss: 1.488052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [471040/500434 (94%)] Loss: 1.510651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [481280/500434 (96%)] Loss: 1.514877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [491520/500434 (98%)] Loss: 1.463861\u001b[0m\n",
      "\u001b[34mcurrent epoch: 74\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [0/500434 (0%)] Loss: 1.694932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [10240/500434 (2%)] Loss: 1.513831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [20480/500434 (4%)] Loss: 1.426207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [30720/500434 (6%)] Loss: 1.449381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [40960/500434 (8%)] Loss: 1.462564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [51200/500434 (10%)] Loss: 1.464587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [61440/500434 (12%)] Loss: 1.477987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [71680/500434 (14%)] Loss: 1.487018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [81920/500434 (16%)] Loss: 1.459881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [92160/500434 (18%)] Loss: 1.502681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [102400/500434 (20%)] Loss: 1.432932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [112640/500434 (22%)] Loss: 1.463010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [122880/500434 (25%)] Loss: 1.479585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [133120/500434 (27%)] Loss: 1.539073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [143360/500434 (29%)] Loss: 1.529201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [153600/500434 (31%)] Loss: 1.488818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [163840/500434 (33%)] Loss: 1.434226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [174080/500434 (35%)] Loss: 1.530237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [184320/500434 (37%)] Loss: 1.406240\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [194560/500434 (39%)] Loss: 1.527228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [204800/500434 (41%)] Loss: 1.556062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [215040/500434 (43%)] Loss: 1.484401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [225280/500434 (45%)] Loss: 1.541601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [235520/500434 (47%)] Loss: 1.564665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [245760/500434 (49%)] Loss: 1.422900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [256000/500434 (51%)] Loss: 1.437183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [266240/500434 (53%)] Loss: 1.451170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [276480/500434 (55%)] Loss: 1.580472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [286720/500434 (57%)] Loss: 1.503118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [296960/500434 (59%)] Loss: 1.497573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [307200/500434 (61%)] Loss: 1.466478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [317440/500434 (63%)] Loss: 1.425665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [327680/500434 (65%)] Loss: 1.405345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [337920/500434 (67%)] Loss: 1.493899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [348160/500434 (70%)] Loss: 1.478976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [358400/500434 (72%)] Loss: 1.546834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [368640/500434 (74%)] Loss: 1.450039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [378880/500434 (76%)] Loss: 1.539892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [389120/500434 (78%)] Loss: 1.545477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [399360/500434 (80%)] Loss: 1.493657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [409600/500434 (82%)] Loss: 1.526291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [419840/500434 (84%)] Loss: 1.619167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [430080/500434 (86%)] Loss: 1.471262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [440320/500434 (88%)] Loss: 1.508840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [450560/500434 (90%)] Loss: 1.496285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [460800/500434 (92%)] Loss: 1.433736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [471040/500434 (94%)] Loss: 1.461825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [481280/500434 (96%)] Loss: 1.479084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [491520/500434 (98%)] Loss: 1.461806\u001b[0m\n",
      "\u001b[34mcurrent epoch: 75\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [0/500434 (0%)] Loss: 1.376710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [10240/500434 (2%)] Loss: 1.507768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [20480/500434 (4%)] Loss: 1.479579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [30720/500434 (6%)] Loss: 1.516489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [40960/500434 (8%)] Loss: 1.542322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [51200/500434 (10%)] Loss: 1.514281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [61440/500434 (12%)] Loss: 1.416755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [71680/500434 (14%)] Loss: 1.401948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [81920/500434 (16%)] Loss: 1.414984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [92160/500434 (18%)] Loss: 1.516233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [102400/500434 (20%)] Loss: 1.437029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [112640/500434 (22%)] Loss: 1.529759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [122880/500434 (25%)] Loss: 1.516655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [133120/500434 (27%)] Loss: 1.543043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [143360/500434 (29%)] Loss: 1.610528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [153600/500434 (31%)] Loss: 1.438296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [163840/500434 (33%)] Loss: 1.526842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [174080/500434 (35%)] Loss: 1.586006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [184320/500434 (37%)] Loss: 1.443916\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [194560/500434 (39%)] Loss: 1.485989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [204800/500434 (41%)] Loss: 1.355411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [215040/500434 (43%)] Loss: 1.472213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [225280/500434 (45%)] Loss: 1.453838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [235520/500434 (47%)] Loss: 1.549932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [245760/500434 (49%)] Loss: 1.387889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [256000/500434 (51%)] Loss: 1.533769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [266240/500434 (53%)] Loss: 1.477071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [276480/500434 (55%)] Loss: 1.583984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [286720/500434 (57%)] Loss: 1.554520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [296960/500434 (59%)] Loss: 1.885247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [307200/500434 (61%)] Loss: 1.416543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [317440/500434 (63%)] Loss: 1.472975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [327680/500434 (65%)] Loss: 1.484684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [337920/500434 (67%)] Loss: 1.569573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [348160/500434 (70%)] Loss: 1.530723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [358400/500434 (72%)] Loss: 1.519551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [368640/500434 (74%)] Loss: 1.462719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [378880/500434 (76%)] Loss: 1.550231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [389120/500434 (78%)] Loss: 1.517919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [399360/500434 (80%)] Loss: 1.412722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [409600/500434 (82%)] Loss: 1.537956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [419840/500434 (84%)] Loss: 1.461228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [430080/500434 (86%)] Loss: 1.478693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [440320/500434 (88%)] Loss: 1.447292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [450560/500434 (90%)] Loss: 1.486100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [460800/500434 (92%)] Loss: 1.480817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [471040/500434 (94%)] Loss: 1.488312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [481280/500434 (96%)] Loss: 1.436549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [491520/500434 (98%)] Loss: 1.520379\u001b[0m\n",
      "\u001b[34mcurrent epoch: 76\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [0/500434 (0%)] Loss: 1.658457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [10240/500434 (2%)] Loss: 1.587792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [20480/500434 (4%)] Loss: 1.519549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [30720/500434 (6%)] Loss: 1.510977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [40960/500434 (8%)] Loss: 1.496708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [51200/500434 (10%)] Loss: 1.433244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [61440/500434 (12%)] Loss: 1.619880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [71680/500434 (14%)] Loss: 1.450841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [81920/500434 (16%)] Loss: 1.541803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [92160/500434 (18%)] Loss: 1.543009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [102400/500434 (20%)] Loss: 1.440525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [112640/500434 (22%)] Loss: 1.558450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [122880/500434 (25%)] Loss: 1.445255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [133120/500434 (27%)] Loss: 1.460970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [143360/500434 (29%)] Loss: 1.510417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [153600/500434 (31%)] Loss: 1.527005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [163840/500434 (33%)] Loss: 1.551401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [174080/500434 (35%)] Loss: 1.522607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [184320/500434 (37%)] Loss: 1.446733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [194560/500434 (39%)] Loss: 1.453362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [204800/500434 (41%)] Loss: 1.461894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [215040/500434 (43%)] Loss: 1.559049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [225280/500434 (45%)] Loss: 1.442044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [235520/500434 (47%)] Loss: 1.502795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [245760/500434 (49%)] Loss: 1.499124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [256000/500434 (51%)] Loss: 1.412748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [266240/500434 (53%)] Loss: 1.547074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [276480/500434 (55%)] Loss: 1.613296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [286720/500434 (57%)] Loss: 1.497028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [296960/500434 (59%)] Loss: 1.571326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [307200/500434 (61%)] Loss: 1.457665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [317440/500434 (63%)] Loss: 1.391508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [327680/500434 (65%)] Loss: 1.454102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [337920/500434 (67%)] Loss: 1.533311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [348160/500434 (70%)] Loss: 1.428925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [358400/500434 (72%)] Loss: 1.507720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [368640/500434 (74%)] Loss: 1.418504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [378880/500434 (76%)] Loss: 1.473514\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [389120/500434 (78%)] Loss: 1.546578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [399360/500434 (80%)] Loss: 1.605734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [409600/500434 (82%)] Loss: 1.517228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [419840/500434 (84%)] Loss: 1.580043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [430080/500434 (86%)] Loss: 1.405598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [440320/500434 (88%)] Loss: 1.462530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [450560/500434 (90%)] Loss: 1.413280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [460800/500434 (92%)] Loss: 1.451551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [471040/500434 (94%)] Loss: 1.416334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [481280/500434 (96%)] Loss: 1.671324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [491520/500434 (98%)] Loss: 1.479721\u001b[0m\n",
      "\u001b[34mcurrent epoch: 77\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [0/500434 (0%)] Loss: 1.539710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [10240/500434 (2%)] Loss: 1.546110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [20480/500434 (4%)] Loss: 1.511680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [30720/500434 (6%)] Loss: 1.426061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [40960/500434 (8%)] Loss: 1.396309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [51200/500434 (10%)] Loss: 1.649076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [61440/500434 (12%)] Loss: 1.520432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [71680/500434 (14%)] Loss: 1.428250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [81920/500434 (16%)] Loss: 1.468887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [92160/500434 (18%)] Loss: 1.384766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [102400/500434 (20%)] Loss: 1.421037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [112640/500434 (22%)] Loss: 1.479765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [122880/500434 (25%)] Loss: 1.502402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [133120/500434 (27%)] Loss: 1.533202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [143360/500434 (29%)] Loss: 1.532170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [153600/500434 (31%)] Loss: 1.481116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [163840/500434 (33%)] Loss: 1.468303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [174080/500434 (35%)] Loss: 1.528508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [184320/500434 (37%)] Loss: 1.530302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [194560/500434 (39%)] Loss: 1.471590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [204800/500434 (41%)] Loss: 1.451530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [215040/500434 (43%)] Loss: 1.582767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [225280/500434 (45%)] Loss: 1.544538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [235520/500434 (47%)] Loss: 1.577090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [245760/500434 (49%)] Loss: 1.478556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [256000/500434 (51%)] Loss: 1.543145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [266240/500434 (53%)] Loss: 1.454519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [276480/500434 (55%)] Loss: 1.506610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [286720/500434 (57%)] Loss: 1.428956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [296960/500434 (59%)] Loss: 1.455302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [307200/500434 (61%)] Loss: 1.461229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [317440/500434 (63%)] Loss: 1.494927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [327680/500434 (65%)] Loss: 1.487383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [337920/500434 (67%)] Loss: 1.355113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [348160/500434 (70%)] Loss: 1.509237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [358400/500434 (72%)] Loss: 1.563077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [368640/500434 (74%)] Loss: 1.574799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [378880/500434 (76%)] Loss: 1.504752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [389120/500434 (78%)] Loss: 1.611474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [399360/500434 (80%)] Loss: 1.483487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [409600/500434 (82%)] Loss: 1.538089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [419840/500434 (84%)] Loss: 1.454183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [430080/500434 (86%)] Loss: 1.465842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [440320/500434 (88%)] Loss: 1.471928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [450560/500434 (90%)] Loss: 1.424314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [460800/500434 (92%)] Loss: 1.566418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [471040/500434 (94%)] Loss: 1.419857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [481280/500434 (96%)] Loss: 1.501615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [491520/500434 (98%)] Loss: 1.530002\u001b[0m\n",
      "\u001b[34mcurrent epoch: 78\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [0/500434 (0%)] Loss: 1.482147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [10240/500434 (2%)] Loss: 1.445211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [20480/500434 (4%)] Loss: 1.541959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [30720/500434 (6%)] Loss: 1.422145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [40960/500434 (8%)] Loss: 1.508097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [51200/500434 (10%)] Loss: 1.486307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [61440/500434 (12%)] Loss: 1.516725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [71680/500434 (14%)] Loss: 1.552373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [81920/500434 (16%)] Loss: 1.473681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [92160/500434 (18%)] Loss: 1.483828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [102400/500434 (20%)] Loss: 1.407486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [112640/500434 (22%)] Loss: 1.569449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [122880/500434 (25%)] Loss: 1.515991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [133120/500434 (27%)] Loss: 1.386347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [143360/500434 (29%)] Loss: 1.381737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [153600/500434 (31%)] Loss: 1.424416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [163840/500434 (33%)] Loss: 1.490593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [174080/500434 (35%)] Loss: 1.515632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [184320/500434 (37%)] Loss: 1.494629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [194560/500434 (39%)] Loss: 1.448962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [204800/500434 (41%)] Loss: 1.445414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [215040/500434 (43%)] Loss: 1.527049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [225280/500434 (45%)] Loss: 1.544386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [235520/500434 (47%)] Loss: 1.403910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [245760/500434 (49%)] Loss: 1.530077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [256000/500434 (51%)] Loss: 1.569803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [266240/500434 (53%)] Loss: 1.475699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [276480/500434 (55%)] Loss: 1.602827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [286720/500434 (57%)] Loss: 1.457306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [296960/500434 (59%)] Loss: 1.494890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [307200/500434 (61%)] Loss: 1.475120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [317440/500434 (63%)] Loss: 1.486007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [327680/500434 (65%)] Loss: 1.525660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [337920/500434 (67%)] Loss: 1.524903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [348160/500434 (70%)] Loss: 1.441669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [358400/500434 (72%)] Loss: 1.484632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [368640/500434 (74%)] Loss: 1.471300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [378880/500434 (76%)] Loss: 1.420014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [389120/500434 (78%)] Loss: 1.594319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [399360/500434 (80%)] Loss: 1.536791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [409600/500434 (82%)] Loss: 1.440391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [419840/500434 (84%)] Loss: 1.505751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [430080/500434 (86%)] Loss: 1.470823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [440320/500434 (88%)] Loss: 1.401748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [450560/500434 (90%)] Loss: 1.613481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [460800/500434 (92%)] Loss: 1.517973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [471040/500434 (94%)] Loss: 1.442221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [481280/500434 (96%)] Loss: 1.465013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [491520/500434 (98%)] Loss: 1.498081\u001b[0m\n",
      "\u001b[34mcurrent epoch: 79\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [0/500434 (0%)] Loss: 1.467076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [10240/500434 (2%)] Loss: 1.509340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [20480/500434 (4%)] Loss: 1.596899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [30720/500434 (6%)] Loss: 1.471420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [40960/500434 (8%)] Loss: 1.371048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [51200/500434 (10%)] Loss: 1.524014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [61440/500434 (12%)] Loss: 1.464755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [71680/500434 (14%)] Loss: 1.454979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [81920/500434 (16%)] Loss: 1.527650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [92160/500434 (18%)] Loss: 1.423311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [102400/500434 (20%)] Loss: 1.494345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [112640/500434 (22%)] Loss: 1.572295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [122880/500434 (25%)] Loss: 1.530449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [133120/500434 (27%)] Loss: 1.536145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [143360/500434 (29%)] Loss: 1.617777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [153600/500434 (31%)] Loss: 1.615370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [163840/500434 (33%)] Loss: 1.531809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [174080/500434 (35%)] Loss: 1.564277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [184320/500434 (37%)] Loss: 1.448331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [194560/500434 (39%)] Loss: 1.566713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [204800/500434 (41%)] Loss: 1.403575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [215040/500434 (43%)] Loss: 1.472731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [225280/500434 (45%)] Loss: 1.407928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [235520/500434 (47%)] Loss: 1.480742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [245760/500434 (49%)] Loss: 1.506427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [256000/500434 (51%)] Loss: 1.409452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [266240/500434 (53%)] Loss: 1.378647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [276480/500434 (55%)] Loss: 1.381008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [286720/500434 (57%)] Loss: 1.471787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [296960/500434 (59%)] Loss: 1.492785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [307200/500434 (61%)] Loss: 1.364757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [317440/500434 (63%)] Loss: 1.386283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [327680/500434 (65%)] Loss: 1.430420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [337920/500434 (67%)] Loss: 1.554697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [348160/500434 (70%)] Loss: 1.563283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [358400/500434 (72%)] Loss: 1.506258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [368640/500434 (74%)] Loss: 1.511102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [378880/500434 (76%)] Loss: 1.560211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [389120/500434 (78%)] Loss: 1.524271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [399360/500434 (80%)] Loss: 1.442520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [409600/500434 (82%)] Loss: 1.426324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [419840/500434 (84%)] Loss: 1.567521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [430080/500434 (86%)] Loss: 1.346536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [440320/500434 (88%)] Loss: 1.508035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [450560/500434 (90%)] Loss: 1.453490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [460800/500434 (92%)] Loss: 1.398826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [471040/500434 (94%)] Loss: 1.472113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [481280/500434 (96%)] Loss: 1.427478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [491520/500434 (98%)] Loss: 1.498568\u001b[0m\n",
      "\u001b[34mcurrent epoch: 80\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [0/500434 (0%)] Loss: 1.503900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [10240/500434 (2%)] Loss: 1.579474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [20480/500434 (4%)] Loss: 1.564189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [30720/500434 (6%)] Loss: 1.487799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [40960/500434 (8%)] Loss: 1.441829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [51200/500434 (10%)] Loss: 1.506484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [61440/500434 (12%)] Loss: 1.526450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [71680/500434 (14%)] Loss: 1.417794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [81920/500434 (16%)] Loss: 1.510691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [92160/500434 (18%)] Loss: 1.632031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [102400/500434 (20%)] Loss: 1.378957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [112640/500434 (22%)] Loss: 1.618389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [122880/500434 (25%)] Loss: 1.550124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [133120/500434 (27%)] Loss: 1.442340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [143360/500434 (29%)] Loss: 1.393068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [153600/500434 (31%)] Loss: 1.737723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [163840/500434 (33%)] Loss: 1.431592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [174080/500434 (35%)] Loss: 1.553268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [184320/500434 (37%)] Loss: 1.462746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [194560/500434 (39%)] Loss: 1.436281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [204800/500434 (41%)] Loss: 1.422328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [215040/500434 (43%)] Loss: 1.437997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [225280/500434 (45%)] Loss: 1.407956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [235520/500434 (47%)] Loss: 1.447767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [245760/500434 (49%)] Loss: 1.488060\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [256000/500434 (51%)] Loss: 1.569584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [266240/500434 (53%)] Loss: 1.432446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [276480/500434 (55%)] Loss: 1.472687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [286720/500434 (57%)] Loss: 1.437050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [296960/500434 (59%)] Loss: 1.559247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [307200/500434 (61%)] Loss: 1.518015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [317440/500434 (63%)] Loss: 1.535230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [327680/500434 (65%)] Loss: 1.482054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [337920/500434 (67%)] Loss: 1.504453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [348160/500434 (70%)] Loss: 1.437799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [358400/500434 (72%)] Loss: 1.480237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [368640/500434 (74%)] Loss: 1.388155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [378880/500434 (76%)] Loss: 1.513241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [389120/500434 (78%)] Loss: 1.549591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [399360/500434 (80%)] Loss: 1.705508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [409600/500434 (82%)] Loss: 1.575505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [419840/500434 (84%)] Loss: 1.542332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [430080/500434 (86%)] Loss: 1.534190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [440320/500434 (88%)] Loss: 1.401437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [450560/500434 (90%)] Loss: 1.477557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [460800/500434 (92%)] Loss: 1.545606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [471040/500434 (94%)] Loss: 1.536654\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [481280/500434 (96%)] Loss: 1.537932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [491520/500434 (98%)] Loss: 1.408115\u001b[0m\n",
      "\u001b[34mcurrent epoch: 81\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [0/500434 (0%)] Loss: 1.509248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [10240/500434 (2%)] Loss: 1.464022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [20480/500434 (4%)] Loss: 1.522758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [30720/500434 (6%)] Loss: 1.546924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [40960/500434 (8%)] Loss: 1.530063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [51200/500434 (10%)] Loss: 1.448451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [61440/500434 (12%)] Loss: 1.360668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [71680/500434 (14%)] Loss: 1.553981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [81920/500434 (16%)] Loss: 1.570831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [92160/500434 (18%)] Loss: 1.501350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [102400/500434 (20%)] Loss: 1.475814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [112640/500434 (22%)] Loss: 1.457305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [122880/500434 (25%)] Loss: 1.486514\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [133120/500434 (27%)] Loss: 1.509758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [143360/500434 (29%)] Loss: 1.534208\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [153600/500434 (31%)] Loss: 1.473331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [163840/500434 (33%)] Loss: 1.567833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [174080/500434 (35%)] Loss: 1.466808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [184320/500434 (37%)] Loss: 1.477605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [194560/500434 (39%)] Loss: 1.464650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [204800/500434 (41%)] Loss: 1.386782\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [215040/500434 (43%)] Loss: 1.497584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [225280/500434 (45%)] Loss: 1.430956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [235520/500434 (47%)] Loss: 1.395484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [245760/500434 (49%)] Loss: 1.452443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [256000/500434 (51%)] Loss: 1.559194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [266240/500434 (53%)] Loss: 1.462422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [276480/500434 (55%)] Loss: 1.595266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [286720/500434 (57%)] Loss: 1.471138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [296960/500434 (59%)] Loss: 1.416975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [307200/500434 (61%)] Loss: 1.522493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [317440/500434 (63%)] Loss: 1.447181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [327680/500434 (65%)] Loss: 1.436221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [337920/500434 (67%)] Loss: 1.416198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [348160/500434 (70%)] Loss: 1.500345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [358400/500434 (72%)] Loss: 1.418322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [368640/500434 (74%)] Loss: 1.392157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [378880/500434 (76%)] Loss: 1.534644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [389120/500434 (78%)] Loss: 1.491914\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [399360/500434 (80%)] Loss: 1.509261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [409600/500434 (82%)] Loss: 1.442557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [419840/500434 (84%)] Loss: 1.394178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [430080/500434 (86%)] Loss: 1.473899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [440320/500434 (88%)] Loss: 1.359769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [450560/500434 (90%)] Loss: 1.623366\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [460800/500434 (92%)] Loss: 1.388675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [471040/500434 (94%)] Loss: 1.419138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [481280/500434 (96%)] Loss: 1.535670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [491520/500434 (98%)] Loss: 1.466572\u001b[0m\n",
      "\u001b[34mcurrent epoch: 82\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [0/500434 (0%)] Loss: 1.490242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [10240/500434 (2%)] Loss: 1.413529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [20480/500434 (4%)] Loss: 1.426952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [30720/500434 (6%)] Loss: 1.582347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [40960/500434 (8%)] Loss: 1.353718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [51200/500434 (10%)] Loss: 1.512287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [61440/500434 (12%)] Loss: 1.424932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [71680/500434 (14%)] Loss: 1.482021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [81920/500434 (16%)] Loss: 1.429936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [92160/500434 (18%)] Loss: 1.495665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [102400/500434 (20%)] Loss: 1.400443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [112640/500434 (22%)] Loss: 1.445148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [122880/500434 (25%)] Loss: 1.477216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [133120/500434 (27%)] Loss: 1.454361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [143360/500434 (29%)] Loss: 1.427051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [153600/500434 (31%)] Loss: 1.465793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [163840/500434 (33%)] Loss: 2.239667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [174080/500434 (35%)] Loss: 6.328921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [184320/500434 (37%)] Loss: 3.609713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [194560/500434 (39%)] Loss: 3.409668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [204800/500434 (41%)] Loss: 3.420359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [215040/500434 (43%)] Loss: 3.366212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [225280/500434 (45%)] Loss: 3.319979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [235520/500434 (47%)] Loss: 3.312710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [245760/500434 (49%)] Loss: 3.165098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [256000/500434 (51%)] Loss: 3.121554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [266240/500434 (53%)] Loss: 2.974646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [276480/500434 (55%)] Loss: 2.850354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [286720/500434 (57%)] Loss: 2.580962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [296960/500434 (59%)] Loss: 2.429132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [307200/500434 (61%)] Loss: 2.313135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [317440/500434 (63%)] Loss: 2.274556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [327680/500434 (65%)] Loss: 2.396544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [337920/500434 (67%)] Loss: 2.175481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [348160/500434 (70%)] Loss: 2.050572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [358400/500434 (72%)] Loss: 2.033638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [368640/500434 (74%)] Loss: 2.045551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [378880/500434 (76%)] Loss: 2.057849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [389120/500434 (78%)] Loss: 1.867412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [399360/500434 (80%)] Loss: 1.900403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [409600/500434 (82%)] Loss: 1.935706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [419840/500434 (84%)] Loss: 1.880518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [430080/500434 (86%)] Loss: 2.037425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [440320/500434 (88%)] Loss: 1.971411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [450560/500434 (90%)] Loss: 1.970773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [460800/500434 (92%)] Loss: 2.014030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [471040/500434 (94%)] Loss: 1.801133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [481280/500434 (96%)] Loss: 1.942690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [491520/500434 (98%)] Loss: 1.895494\u001b[0m\n",
      "\u001b[34mcurrent epoch: 83\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [0/500434 (0%)] Loss: 1.957732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [10240/500434 (2%)] Loss: 1.898878\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [20480/500434 (4%)] Loss: 1.766866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [30720/500434 (6%)] Loss: 1.943379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [40960/500434 (8%)] Loss: 1.835305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [51200/500434 (10%)] Loss: 1.816555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [61440/500434 (12%)] Loss: 1.869887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [71680/500434 (14%)] Loss: 1.660656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [81920/500434 (16%)] Loss: 1.804658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [92160/500434 (18%)] Loss: 1.650853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [102400/500434 (20%)] Loss: 1.772478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [112640/500434 (22%)] Loss: 1.729071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [122880/500434 (25%)] Loss: 1.703938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [133120/500434 (27%)] Loss: 1.814325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [143360/500434 (29%)] Loss: 1.745281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [153600/500434 (31%)] Loss: 1.744882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [163840/500434 (33%)] Loss: 1.645753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [174080/500434 (35%)] Loss: 1.767190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [184320/500434 (37%)] Loss: 1.785796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [194560/500434 (39%)] Loss: 1.748178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [204800/500434 (41%)] Loss: 1.655566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [215040/500434 (43%)] Loss: 1.641268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [225280/500434 (45%)] Loss: 1.823356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [235520/500434 (47%)] Loss: 1.727340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [245760/500434 (49%)] Loss: 1.752904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [256000/500434 (51%)] Loss: 1.782435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [266240/500434 (53%)] Loss: 1.816853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [276480/500434 (55%)] Loss: 1.688467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [286720/500434 (57%)] Loss: 1.759542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [296960/500434 (59%)] Loss: 1.662774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [307200/500434 (61%)] Loss: 1.609548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [317440/500434 (63%)] Loss: 1.540262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [327680/500434 (65%)] Loss: 1.646785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [337920/500434 (67%)] Loss: 1.629397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [348160/500434 (70%)] Loss: 1.656680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [358400/500434 (72%)] Loss: 1.683260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [368640/500434 (74%)] Loss: 1.676813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [378880/500434 (76%)] Loss: 1.647826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [389120/500434 (78%)] Loss: 1.598104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [399360/500434 (80%)] Loss: 1.693805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [409600/500434 (82%)] Loss: 1.782535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [419840/500434 (84%)] Loss: 1.728902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [430080/500434 (86%)] Loss: 1.609300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [440320/500434 (88%)] Loss: 1.637283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [450560/500434 (90%)] Loss: 1.677004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [460800/500434 (92%)] Loss: 1.586740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [471040/500434 (94%)] Loss: 1.602224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [481280/500434 (96%)] Loss: 1.636939\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [491520/500434 (98%)] Loss: 1.610862\u001b[0m\n",
      "\u001b[34mcurrent epoch: 84\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [0/500434 (0%)] Loss: 1.534926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [10240/500434 (2%)] Loss: 1.617433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [20480/500434 (4%)] Loss: 1.636014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [30720/500434 (6%)] Loss: 1.541628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [40960/500434 (8%)] Loss: 1.701236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [51200/500434 (10%)] Loss: 1.521910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [61440/500434 (12%)] Loss: 1.547639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [71680/500434 (14%)] Loss: 1.555541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [81920/500434 (16%)] Loss: 1.697150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [92160/500434 (18%)] Loss: 1.512722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [102400/500434 (20%)] Loss: 1.563368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [112640/500434 (22%)] Loss: 1.629349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [122880/500434 (25%)] Loss: 1.513922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [133120/500434 (27%)] Loss: 1.520510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [143360/500434 (29%)] Loss: 1.582431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [153600/500434 (31%)] Loss: 1.605679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [163840/500434 (33%)] Loss: 1.581119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [174080/500434 (35%)] Loss: 1.573687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [184320/500434 (37%)] Loss: 1.590577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [194560/500434 (39%)] Loss: 1.547131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [204800/500434 (41%)] Loss: 2.329660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [215040/500434 (43%)] Loss: 1.916266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [225280/500434 (45%)] Loss: 1.766925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [235520/500434 (47%)] Loss: 1.589598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [245760/500434 (49%)] Loss: 1.519607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [256000/500434 (51%)] Loss: 1.647478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [266240/500434 (53%)] Loss: 1.488854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [276480/500434 (55%)] Loss: 1.612190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [286720/500434 (57%)] Loss: 1.625619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [296960/500434 (59%)] Loss: 1.480471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [307200/500434 (61%)] Loss: 1.489802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [317440/500434 (63%)] Loss: 1.499992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [327680/500434 (65%)] Loss: 1.582709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [337920/500434 (67%)] Loss: 1.460529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [348160/500434 (70%)] Loss: 1.553898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [358400/500434 (72%)] Loss: 1.605069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [368640/500434 (74%)] Loss: 1.471812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [378880/500434 (76%)] Loss: 1.610011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [389120/500434 (78%)] Loss: 1.543279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [399360/500434 (80%)] Loss: 1.494969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [409600/500434 (82%)] Loss: 1.533328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [419840/500434 (84%)] Loss: 1.503515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [430080/500434 (86%)] Loss: 1.565466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [440320/500434 (88%)] Loss: 1.708429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [450560/500434 (90%)] Loss: 1.543306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [460800/500434 (92%)] Loss: 1.552542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [471040/500434 (94%)] Loss: 1.708308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [481280/500434 (96%)] Loss: 1.642842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [491520/500434 (98%)] Loss: 1.573419\u001b[0m\n",
      "\u001b[34mcurrent epoch: 85\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [0/500434 (0%)] Loss: 1.530836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [10240/500434 (2%)] Loss: 1.596814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [20480/500434 (4%)] Loss: 1.603610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [30720/500434 (6%)] Loss: 1.627744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [40960/500434 (8%)] Loss: 1.656345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [51200/500434 (10%)] Loss: 1.542959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [61440/500434 (12%)] Loss: 1.479972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [71680/500434 (14%)] Loss: 1.471431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [81920/500434 (16%)] Loss: 1.453680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [92160/500434 (18%)] Loss: 1.562035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [102400/500434 (20%)] Loss: 1.450109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [112640/500434 (22%)] Loss: 1.553254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [122880/500434 (25%)] Loss: 1.548410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [133120/500434 (27%)] Loss: 1.479471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [143360/500434 (29%)] Loss: 1.460563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [153600/500434 (31%)] Loss: 1.527066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [163840/500434 (33%)] Loss: 1.502492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [174080/500434 (35%)] Loss: 1.528554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [184320/500434 (37%)] Loss: 1.541436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [194560/500434 (39%)] Loss: 1.505658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [204800/500434 (41%)] Loss: 1.557407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [215040/500434 (43%)] Loss: 1.493467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [225280/500434 (45%)] Loss: 1.537280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [235520/500434 (47%)] Loss: 1.539367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [245760/500434 (49%)] Loss: 1.520410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [256000/500434 (51%)] Loss: 1.492951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [266240/500434 (53%)] Loss: 1.508588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [276480/500434 (55%)] Loss: 1.541003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [286720/500434 (57%)] Loss: 1.458056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [296960/500434 (59%)] Loss: 1.502534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [307200/500434 (61%)] Loss: 1.486542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [317440/500434 (63%)] Loss: 1.568928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [327680/500434 (65%)] Loss: 1.448930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [337920/500434 (67%)] Loss: 1.549889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [348160/500434 (70%)] Loss: 1.441800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [358400/500434 (72%)] Loss: 1.507705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [368640/500434 (74%)] Loss: 1.528547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [378880/500434 (76%)] Loss: 1.458120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [389120/500434 (78%)] Loss: 1.519062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [399360/500434 (80%)] Loss: 1.402424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [409600/500434 (82%)] Loss: 1.484474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [419840/500434 (84%)] Loss: 1.449849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [430080/500434 (86%)] Loss: 1.558033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [440320/500434 (88%)] Loss: 1.514638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [450560/500434 (90%)] Loss: 1.532438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [460800/500434 (92%)] Loss: 1.505911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [471040/500434 (94%)] Loss: 1.446508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [481280/500434 (96%)] Loss: 1.508334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [491520/500434 (98%)] Loss: 1.588050\u001b[0m\n",
      "\u001b[34mcurrent epoch: 86\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [0/500434 (0%)] Loss: 1.421574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [10240/500434 (2%)] Loss: 1.529172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [20480/500434 (4%)] Loss: 1.579733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [30720/500434 (6%)] Loss: 1.474581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [40960/500434 (8%)] Loss: 1.435667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [51200/500434 (10%)] Loss: 1.541556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [61440/500434 (12%)] Loss: 1.546180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [71680/500434 (14%)] Loss: 1.555685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [81920/500434 (16%)] Loss: 1.486198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [92160/500434 (18%)] Loss: 1.538951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [102400/500434 (20%)] Loss: 1.490602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [112640/500434 (22%)] Loss: 1.459225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [122880/500434 (25%)] Loss: 1.510540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [133120/500434 (27%)] Loss: 1.525644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [143360/500434 (29%)] Loss: 1.425024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [153600/500434 (31%)] Loss: 1.487680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [163840/500434 (33%)] Loss: 1.423101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [174080/500434 (35%)] Loss: 1.551245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [184320/500434 (37%)] Loss: 1.457783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [194560/500434 (39%)] Loss: 1.461195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [204800/500434 (41%)] Loss: 1.314042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [215040/500434 (43%)] Loss: 1.457689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [225280/500434 (45%)] Loss: 1.480679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [235520/500434 (47%)] Loss: 1.537447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [245760/500434 (49%)] Loss: 1.491687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [256000/500434 (51%)] Loss: 1.438507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [266240/500434 (53%)] Loss: 1.512374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [276480/500434 (55%)] Loss: 1.561709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [286720/500434 (57%)] Loss: 1.478614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [296960/500434 (59%)] Loss: 1.679579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [307200/500434 (61%)] Loss: 1.456279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [317440/500434 (63%)] Loss: 1.515630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [327680/500434 (65%)] Loss: 1.555124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [337920/500434 (67%)] Loss: 1.524560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [348160/500434 (70%)] Loss: 1.631055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [358400/500434 (72%)] Loss: 1.639431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [368640/500434 (74%)] Loss: 1.515683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [378880/500434 (76%)] Loss: 1.523154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [389120/500434 (78%)] Loss: 1.504229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [399360/500434 (80%)] Loss: 1.618301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [409600/500434 (82%)] Loss: 1.496382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [419840/500434 (84%)] Loss: 1.531980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [430080/500434 (86%)] Loss: 1.623774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [440320/500434 (88%)] Loss: 1.470134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [450560/500434 (90%)] Loss: 1.456384\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [460800/500434 (92%)] Loss: 1.488686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [471040/500434 (94%)] Loss: 1.474230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [481280/500434 (96%)] Loss: 1.629486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [491520/500434 (98%)] Loss: 1.446182\u001b[0m\n",
      "\u001b[34mcurrent epoch: 87\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [0/500434 (0%)] Loss: 1.506715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [10240/500434 (2%)] Loss: 1.487432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [20480/500434 (4%)] Loss: 1.496688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [30720/500434 (6%)] Loss: 1.621412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [40960/500434 (8%)] Loss: 1.441330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [51200/500434 (10%)] Loss: 1.518709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [61440/500434 (12%)] Loss: 1.492713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [71680/500434 (14%)] Loss: 1.456184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [81920/500434 (16%)] Loss: 1.564693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [92160/500434 (18%)] Loss: 1.521736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [102400/500434 (20%)] Loss: 1.492747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [112640/500434 (22%)] Loss: 1.509674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [122880/500434 (25%)] Loss: 1.425514\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [133120/500434 (27%)] Loss: 1.600065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [143360/500434 (29%)] Loss: 1.600673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [153600/500434 (31%)] Loss: 1.483617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [163840/500434 (33%)] Loss: 1.610100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [174080/500434 (35%)] Loss: 1.475949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [184320/500434 (37%)] Loss: 1.553704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [194560/500434 (39%)] Loss: 1.498185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [204800/500434 (41%)] Loss: 1.527480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [215040/500434 (43%)] Loss: 1.542409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [225280/500434 (45%)] Loss: 1.400092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [235520/500434 (47%)] Loss: 1.427758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [245760/500434 (49%)] Loss: 1.460244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [256000/500434 (51%)] Loss: 1.369696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [266240/500434 (53%)] Loss: 1.418985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [276480/500434 (55%)] Loss: 1.592165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [286720/500434 (57%)] Loss: 1.487537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [296960/500434 (59%)] Loss: 1.537540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [307200/500434 (61%)] Loss: 1.429852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [317440/500434 (63%)] Loss: 1.437222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [327680/500434 (65%)] Loss: 1.510057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [337920/500434 (67%)] Loss: 1.487837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [348160/500434 (70%)] Loss: 1.458806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [358400/500434 (72%)] Loss: 1.514726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [368640/500434 (74%)] Loss: 1.409868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [378880/500434 (76%)] Loss: 1.490438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [389120/500434 (78%)] Loss: 1.488292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [399360/500434 (80%)] Loss: 1.586933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [409600/500434 (82%)] Loss: 1.496145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [419840/500434 (84%)] Loss: 1.484687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [430080/500434 (86%)] Loss: 1.554307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [440320/500434 (88%)] Loss: 1.508326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [450560/500434 (90%)] Loss: 1.525545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [460800/500434 (92%)] Loss: 1.411057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [471040/500434 (94%)] Loss: 1.487131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [481280/500434 (96%)] Loss: 1.518838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [491520/500434 (98%)] Loss: 1.578177\u001b[0m\n",
      "\u001b[34mcurrent epoch: 88\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [0/500434 (0%)] Loss: 1.402028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [10240/500434 (2%)] Loss: 1.488761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [20480/500434 (4%)] Loss: 1.522111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [30720/500434 (6%)] Loss: 1.383182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [40960/500434 (8%)] Loss: 1.506728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [51200/500434 (10%)] Loss: 1.310056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [61440/500434 (12%)] Loss: 1.433632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [71680/500434 (14%)] Loss: 1.423580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [81920/500434 (16%)] Loss: 1.471721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [92160/500434 (18%)] Loss: 1.409096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [102400/500434 (20%)] Loss: 1.422527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [112640/500434 (22%)] Loss: 1.537356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [122880/500434 (25%)] Loss: 1.484846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [133120/500434 (27%)] Loss: 1.457494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [143360/500434 (29%)] Loss: 1.356044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [153600/500434 (31%)] Loss: 1.487240\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [163840/500434 (33%)] Loss: 1.466749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [174080/500434 (35%)] Loss: 1.398940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [184320/500434 (37%)] Loss: 1.412297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [194560/500434 (39%)] Loss: 1.586535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [204800/500434 (41%)] Loss: 1.546797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [215040/500434 (43%)] Loss: 1.486805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [225280/500434 (45%)] Loss: 1.409025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [235520/500434 (47%)] Loss: 1.547058\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [245760/500434 (49%)] Loss: 1.575096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [256000/500434 (51%)] Loss: 1.492211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [266240/500434 (53%)] Loss: 1.506397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [276480/500434 (55%)] Loss: 1.371028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [286720/500434 (57%)] Loss: 1.447296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [296960/500434 (59%)] Loss: 1.378711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [307200/500434 (61%)] Loss: 1.435108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [317440/500434 (63%)] Loss: 1.393249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [327680/500434 (65%)] Loss: 1.466224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [337920/500434 (67%)] Loss: 1.456890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [348160/500434 (70%)] Loss: 1.503849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [358400/500434 (72%)] Loss: 1.448238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [368640/500434 (74%)] Loss: 1.489734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [378880/500434 (76%)] Loss: 1.620137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [389120/500434 (78%)] Loss: 1.434930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [399360/500434 (80%)] Loss: 1.398496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [409600/500434 (82%)] Loss: 1.534972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [419840/500434 (84%)] Loss: 1.421177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [430080/500434 (86%)] Loss: 1.527030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [440320/500434 (88%)] Loss: 1.485031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [450560/500434 (90%)] Loss: 1.416646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [460800/500434 (92%)] Loss: 1.506808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [471040/500434 (94%)] Loss: 1.605369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [481280/500434 (96%)] Loss: 1.540498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [491520/500434 (98%)] Loss: 1.350807\u001b[0m\n",
      "\u001b[34mcurrent epoch: 89\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [0/500434 (0%)] Loss: 1.435794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [10240/500434 (2%)] Loss: 1.482547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [20480/500434 (4%)] Loss: 1.357611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [30720/500434 (6%)] Loss: 1.407540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [40960/500434 (8%)] Loss: 1.569074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [51200/500434 (10%)] Loss: 1.544178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [61440/500434 (12%)] Loss: 1.533591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [71680/500434 (14%)] Loss: 1.446416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [81920/500434 (16%)] Loss: 1.470519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [92160/500434 (18%)] Loss: 1.512964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [102400/500434 (20%)] Loss: 1.406256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [112640/500434 (22%)] Loss: 1.679403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [122880/500434 (25%)] Loss: 1.569177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [133120/500434 (27%)] Loss: 1.579827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [143360/500434 (29%)] Loss: 1.604790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [153600/500434 (31%)] Loss: 1.627018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [163840/500434 (33%)] Loss: 1.517563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [174080/500434 (35%)] Loss: 1.487830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [184320/500434 (37%)] Loss: 1.479818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [194560/500434 (39%)] Loss: 1.433570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [204800/500434 (41%)] Loss: 1.580249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [215040/500434 (43%)] Loss: 1.507323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [225280/500434 (45%)] Loss: 1.506701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [235520/500434 (47%)] Loss: 1.525152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [245760/500434 (49%)] Loss: 1.508188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [256000/500434 (51%)] Loss: 1.470527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [266240/500434 (53%)] Loss: 1.460102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [276480/500434 (55%)] Loss: 1.603589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [286720/500434 (57%)] Loss: 1.613170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [296960/500434 (59%)] Loss: 1.515227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [307200/500434 (61%)] Loss: 1.369593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [317440/500434 (63%)] Loss: 1.484645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [327680/500434 (65%)] Loss: 1.507771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [337920/500434 (67%)] Loss: 1.407745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [348160/500434 (70%)] Loss: 1.492198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [358400/500434 (72%)] Loss: 1.478044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [368640/500434 (74%)] Loss: 1.499438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [378880/500434 (76%)] Loss: 1.504712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [389120/500434 (78%)] Loss: 1.498330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [399360/500434 (80%)] Loss: 1.549395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [409600/500434 (82%)] Loss: 1.589172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [419840/500434 (84%)] Loss: 1.488176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [430080/500434 (86%)] Loss: 1.396278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [440320/500434 (88%)] Loss: 1.576029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [450560/500434 (90%)] Loss: 1.515565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [460800/500434 (92%)] Loss: 1.612500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [471040/500434 (94%)] Loss: 1.482350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [481280/500434 (96%)] Loss: 1.398830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [491520/500434 (98%)] Loss: 1.405993\u001b[0m\n",
      "\u001b[34mcurrent epoch: 90\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [0/500434 (0%)] Loss: 1.419961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [10240/500434 (2%)] Loss: 1.510882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [20480/500434 (4%)] Loss: 1.389230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [30720/500434 (6%)] Loss: 1.499153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [40960/500434 (8%)] Loss: 1.448650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [51200/500434 (10%)] Loss: 1.340128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [61440/500434 (12%)] Loss: 1.413023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [71680/500434 (14%)] Loss: 1.399420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [81920/500434 (16%)] Loss: 1.458711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [92160/500434 (18%)] Loss: 1.420687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [102400/500434 (20%)] Loss: 1.441852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [112640/500434 (22%)] Loss: 1.410746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [122880/500434 (25%)] Loss: 1.423418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [133120/500434 (27%)] Loss: 1.468637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [143360/500434 (29%)] Loss: 1.454885\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [153600/500434 (31%)] Loss: 1.471268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [163840/500434 (33%)] Loss: 1.490152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [174080/500434 (35%)] Loss: 1.384406\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [184320/500434 (37%)] Loss: 1.438033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [194560/500434 (39%)] Loss: 1.478415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [204800/500434 (41%)] Loss: 1.517742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [215040/500434 (43%)] Loss: 1.474461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [225280/500434 (45%)] Loss: 1.374020\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [235520/500434 (47%)] Loss: 1.588591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [245760/500434 (49%)] Loss: 1.524967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [256000/500434 (51%)] Loss: 1.429740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [266240/500434 (53%)] Loss: 1.446588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [276480/500434 (55%)] Loss: 1.513771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [286720/500434 (57%)] Loss: 1.489880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [296960/500434 (59%)] Loss: 1.446395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [307200/500434 (61%)] Loss: 1.545180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [317440/500434 (63%)] Loss: 1.470588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [327680/500434 (65%)] Loss: 1.429583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [337920/500434 (67%)] Loss: 1.490743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [348160/500434 (70%)] Loss: 1.410038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [358400/500434 (72%)] Loss: 1.520720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [368640/500434 (74%)] Loss: 1.511653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [378880/500434 (76%)] Loss: 1.436419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [389120/500434 (78%)] Loss: 1.516175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [399360/500434 (80%)] Loss: 1.379499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [409600/500434 (82%)] Loss: 1.398163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [419840/500434 (84%)] Loss: 1.551218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [430080/500434 (86%)] Loss: 1.552657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [440320/500434 (88%)] Loss: 1.414898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [450560/500434 (90%)] Loss: 1.506111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [460800/500434 (92%)] Loss: 1.563479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [471040/500434 (94%)] Loss: 1.503753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [481280/500434 (96%)] Loss: 1.430943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [491520/500434 (98%)] Loss: 1.697728\u001b[0m\n",
      "\u001b[34mcurrent epoch: 91\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [0/500434 (0%)] Loss: 1.486934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [10240/500434 (2%)] Loss: 1.448500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [20480/500434 (4%)] Loss: 1.575336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [30720/500434 (6%)] Loss: 1.373982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [40960/500434 (8%)] Loss: 1.439934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [51200/500434 (10%)] Loss: 1.478996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [61440/500434 (12%)] Loss: 1.433413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [71680/500434 (14%)] Loss: 1.354528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [81920/500434 (16%)] Loss: 1.508815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [92160/500434 (18%)] Loss: 1.458261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [102400/500434 (20%)] Loss: 1.458078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [112640/500434 (22%)] Loss: 1.386851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [122880/500434 (25%)] Loss: 1.670990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [133120/500434 (27%)] Loss: 1.422747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [143360/500434 (29%)] Loss: 1.421162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [153600/500434 (31%)] Loss: 1.443062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [163840/500434 (33%)] Loss: 1.525256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [174080/500434 (35%)] Loss: 1.514103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [184320/500434 (37%)] Loss: 1.547109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [194560/500434 (39%)] Loss: 1.493881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [204800/500434 (41%)] Loss: 1.535039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [215040/500434 (43%)] Loss: 1.657759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [225280/500434 (45%)] Loss: 1.685733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [235520/500434 (47%)] Loss: 1.685539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [245760/500434 (49%)] Loss: 1.684864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [256000/500434 (51%)] Loss: 1.542434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [266240/500434 (53%)] Loss: 1.541826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [276480/500434 (55%)] Loss: 1.597485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [286720/500434 (57%)] Loss: 1.670538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [296960/500434 (59%)] Loss: 1.572395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [307200/500434 (61%)] Loss: 1.497278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [317440/500434 (63%)] Loss: 1.539486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [327680/500434 (65%)] Loss: 1.617490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [337920/500434 (67%)] Loss: 1.536023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [348160/500434 (70%)] Loss: 1.495028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [358400/500434 (72%)] Loss: 1.557499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [368640/500434 (74%)] Loss: 1.521606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [378880/500434 (76%)] Loss: 1.527423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [389120/500434 (78%)] Loss: 1.557678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [399360/500434 (80%)] Loss: 1.514927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [409600/500434 (82%)] Loss: 1.519166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [419840/500434 (84%)] Loss: 1.480502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [430080/500434 (86%)] Loss: 1.481255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [440320/500434 (88%)] Loss: 1.592977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [450560/500434 (90%)] Loss: 1.533082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [460800/500434 (92%)] Loss: 1.567257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [471040/500434 (94%)] Loss: 1.498162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [481280/500434 (96%)] Loss: 1.488932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [491520/500434 (98%)] Loss: 1.489275\u001b[0m\n",
      "\u001b[34mcurrent epoch: 92\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [0/500434 (0%)] Loss: 1.560311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [10240/500434 (2%)] Loss: 1.496554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [20480/500434 (4%)] Loss: 1.480120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [30720/500434 (6%)] Loss: 1.578517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [40960/500434 (8%)] Loss: 1.582322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [51200/500434 (10%)] Loss: 1.562328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [61440/500434 (12%)] Loss: 1.561972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [71680/500434 (14%)] Loss: 1.443552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [81920/500434 (16%)] Loss: 1.423515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [92160/500434 (18%)] Loss: 1.503021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [102400/500434 (20%)] Loss: 1.510656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [112640/500434 (22%)] Loss: 1.566321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [122880/500434 (25%)] Loss: 1.456840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [133120/500434 (27%)] Loss: 1.513714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [143360/500434 (29%)] Loss: 1.468307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [153600/500434 (31%)] Loss: 1.411378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [163840/500434 (33%)] Loss: 1.459576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [174080/500434 (35%)] Loss: 1.496337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [184320/500434 (37%)] Loss: 1.475567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [194560/500434 (39%)] Loss: 1.466616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [204800/500434 (41%)] Loss: 1.474781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [215040/500434 (43%)] Loss: 1.471908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [225280/500434 (45%)] Loss: 1.442869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [235520/500434 (47%)] Loss: 1.489293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [245760/500434 (49%)] Loss: 1.465099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [256000/500434 (51%)] Loss: 1.524132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [266240/500434 (53%)] Loss: 1.519964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [276480/500434 (55%)] Loss: 1.422485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [286720/500434 (57%)] Loss: 1.514680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [296960/500434 (59%)] Loss: 1.448596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [307200/500434 (61%)] Loss: 1.607306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [317440/500434 (63%)] Loss: 1.345017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [327680/500434 (65%)] Loss: 1.480132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [337920/500434 (67%)] Loss: 1.602250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [348160/500434 (70%)] Loss: 1.449206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [358400/500434 (72%)] Loss: 1.368555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [368640/500434 (74%)] Loss: 1.458547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [378880/500434 (76%)] Loss: 1.444535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [389120/500434 (78%)] Loss: 1.463792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [399360/500434 (80%)] Loss: 1.532341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [409600/500434 (82%)] Loss: 1.445678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [419840/500434 (84%)] Loss: 1.595721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [430080/500434 (86%)] Loss: 1.503141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [440320/500434 (88%)] Loss: 1.512677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [450560/500434 (90%)] Loss: 1.431360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [460800/500434 (92%)] Loss: 1.530498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [471040/500434 (94%)] Loss: 1.491602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [481280/500434 (96%)] Loss: 1.447163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [491520/500434 (98%)] Loss: 1.458970\u001b[0m\n",
      "\u001b[34mcurrent epoch: 93\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [0/500434 (0%)] Loss: 1.421365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [10240/500434 (2%)] Loss: 1.483076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [20480/500434 (4%)] Loss: 1.546744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [30720/500434 (6%)] Loss: 1.575799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [40960/500434 (8%)] Loss: 1.457508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [51200/500434 (10%)] Loss: 1.397462\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [61440/500434 (12%)] Loss: 1.459281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [71680/500434 (14%)] Loss: 1.486662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [81920/500434 (16%)] Loss: 1.491464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [92160/500434 (18%)] Loss: 1.521479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [102400/500434 (20%)] Loss: 1.460011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [112640/500434 (22%)] Loss: 1.463061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [122880/500434 (25%)] Loss: 1.467044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [133120/500434 (27%)] Loss: 1.485160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [143360/500434 (29%)] Loss: 1.417604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [153600/500434 (31%)] Loss: 1.504360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [163840/500434 (33%)] Loss: 1.461076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [174080/500434 (35%)] Loss: 1.487965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [184320/500434 (37%)] Loss: 1.512355\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [194560/500434 (39%)] Loss: 1.488073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [204800/500434 (41%)] Loss: 1.719617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [215040/500434 (43%)] Loss: 1.461928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [225280/500434 (45%)] Loss: 1.409603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [235520/500434 (47%)] Loss: 1.547861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [245760/500434 (49%)] Loss: 1.476680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [256000/500434 (51%)] Loss: 1.394394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [266240/500434 (53%)] Loss: 1.476032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [276480/500434 (55%)] Loss: 1.391928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [286720/500434 (57%)] Loss: 1.492598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [296960/500434 (59%)] Loss: 1.611231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [307200/500434 (61%)] Loss: 1.482985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [317440/500434 (63%)] Loss: 1.638059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [327680/500434 (65%)] Loss: 1.494898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [337920/500434 (67%)] Loss: 1.488781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [348160/500434 (70%)] Loss: 1.411612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [358400/500434 (72%)] Loss: 1.479522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [368640/500434 (74%)] Loss: 1.522288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [378880/500434 (76%)] Loss: 1.467820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [389120/500434 (78%)] Loss: 1.435673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [399360/500434 (80%)] Loss: 1.397510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [409600/500434 (82%)] Loss: 1.490446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [419840/500434 (84%)] Loss: 1.453272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [430080/500434 (86%)] Loss: 1.388919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [440320/500434 (88%)] Loss: 1.447403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [450560/500434 (90%)] Loss: 1.445702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [460800/500434 (92%)] Loss: 1.455458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [471040/500434 (94%)] Loss: 1.475698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [481280/500434 (96%)] Loss: 1.493953\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [491520/500434 (98%)] Loss: 1.513078\u001b[0m\n",
      "\u001b[34mcurrent epoch: 94\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [0/500434 (0%)] Loss: 1.424291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [10240/500434 (2%)] Loss: 1.462145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [20480/500434 (4%)] Loss: 1.484162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [30720/500434 (6%)] Loss: 1.561182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [40960/500434 (8%)] Loss: 1.437369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [51200/500434 (10%)] Loss: 1.466223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [61440/500434 (12%)] Loss: 1.531775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [71680/500434 (14%)] Loss: 1.482191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [81920/500434 (16%)] Loss: 1.413056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [92160/500434 (18%)] Loss: 1.482498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [102400/500434 (20%)] Loss: 1.390772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [112640/500434 (22%)] Loss: 1.413417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [122880/500434 (25%)] Loss: 1.447582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [133120/500434 (27%)] Loss: 1.490292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [143360/500434 (29%)] Loss: 1.438489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [153600/500434 (31%)] Loss: 1.471407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [163840/500434 (33%)] Loss: 1.347216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [174080/500434 (35%)] Loss: 1.385567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [184320/500434 (37%)] Loss: 1.477562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [194560/500434 (39%)] Loss: 1.442647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [204800/500434 (41%)] Loss: 1.403984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [215040/500434 (43%)] Loss: 1.423315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [225280/500434 (45%)] Loss: 1.498016\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [235520/500434 (47%)] Loss: 1.373090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [245760/500434 (49%)] Loss: 1.550646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [256000/500434 (51%)] Loss: 1.468670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [266240/500434 (53%)] Loss: 1.612221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [276480/500434 (55%)] Loss: 1.508500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [286720/500434 (57%)] Loss: 1.535774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [296960/500434 (59%)] Loss: 1.476696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [307200/500434 (61%)] Loss: 1.432060\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [317440/500434 (63%)] Loss: 1.377125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [327680/500434 (65%)] Loss: 1.511454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [337920/500434 (67%)] Loss: 1.380874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [348160/500434 (70%)] Loss: 1.456360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [358400/500434 (72%)] Loss: 1.369995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [368640/500434 (74%)] Loss: 1.463742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [378880/500434 (76%)] Loss: 1.449597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [389120/500434 (78%)] Loss: 1.431005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [399360/500434 (80%)] Loss: 1.440006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [409600/500434 (82%)] Loss: 1.493711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [419840/500434 (84%)] Loss: 1.576607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [430080/500434 (86%)] Loss: 1.559305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [440320/500434 (88%)] Loss: 1.408707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [450560/500434 (90%)] Loss: 1.548300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [460800/500434 (92%)] Loss: 1.686432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [471040/500434 (94%)] Loss: 1.596615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [481280/500434 (96%)] Loss: 1.442515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [491520/500434 (98%)] Loss: 1.453626\u001b[0m\n",
      "\u001b[34mcurrent epoch: 95\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [0/500434 (0%)] Loss: 1.726808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [10240/500434 (2%)] Loss: 1.453794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [20480/500434 (4%)] Loss: 1.384189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [30720/500434 (6%)] Loss: 1.506649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [40960/500434 (8%)] Loss: 1.495525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [51200/500434 (10%)] Loss: 1.439810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [61440/500434 (12%)] Loss: 1.432608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [71680/500434 (14%)] Loss: 1.679295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [81920/500434 (16%)] Loss: 1.476276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [92160/500434 (18%)] Loss: 1.343213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [102400/500434 (20%)] Loss: 1.466030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [112640/500434 (22%)] Loss: 1.499419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [122880/500434 (25%)] Loss: 1.386770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [133120/500434 (27%)] Loss: 1.428639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [143360/500434 (29%)] Loss: 1.505558\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [153600/500434 (31%)] Loss: 1.513254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [163840/500434 (33%)] Loss: 1.552142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [174080/500434 (35%)] Loss: 1.487275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [184320/500434 (37%)] Loss: 1.396444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [194560/500434 (39%)] Loss: 1.511231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [204800/500434 (41%)] Loss: 1.551610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [215040/500434 (43%)] Loss: 1.451222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [225280/500434 (45%)] Loss: 1.342234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [235520/500434 (47%)] Loss: 1.501570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [245760/500434 (49%)] Loss: 1.360821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [256000/500434 (51%)] Loss: 1.486097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [266240/500434 (53%)] Loss: 1.582910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [276480/500434 (55%)] Loss: 1.513955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [286720/500434 (57%)] Loss: 1.424134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [296960/500434 (59%)] Loss: 1.434880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [307200/500434 (61%)] Loss: 1.433008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [317440/500434 (63%)] Loss: 1.544930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [327680/500434 (65%)] Loss: 1.397695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [337920/500434 (67%)] Loss: 1.464800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [348160/500434 (70%)] Loss: 1.524681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [358400/500434 (72%)] Loss: 1.462032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [368640/500434 (74%)] Loss: 1.475033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [378880/500434 (76%)] Loss: 1.393508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [389120/500434 (78%)] Loss: 1.475947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [399360/500434 (80%)] Loss: 1.351533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [409600/500434 (82%)] Loss: 1.475860\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [419840/500434 (84%)] Loss: 1.519737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [430080/500434 (86%)] Loss: 1.516166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [440320/500434 (88%)] Loss: 1.440683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [450560/500434 (90%)] Loss: 1.560435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [460800/500434 (92%)] Loss: 1.320170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [471040/500434 (94%)] Loss: 1.497856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [481280/500434 (96%)] Loss: 1.384651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [491520/500434 (98%)] Loss: 1.535326\u001b[0m\n",
      "\u001b[34mcurrent epoch: 96\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [0/500434 (0%)] Loss: 1.383225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [10240/500434 (2%)] Loss: 1.389378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [20480/500434 (4%)] Loss: 1.442201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [30720/500434 (6%)] Loss: 1.467677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [40960/500434 (8%)] Loss: 1.465231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [51200/500434 (10%)] Loss: 1.326556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [61440/500434 (12%)] Loss: 1.390671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [71680/500434 (14%)] Loss: 1.540793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [81920/500434 (16%)] Loss: 1.402215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [92160/500434 (18%)] Loss: 1.490492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [102400/500434 (20%)] Loss: 1.463509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [112640/500434 (22%)] Loss: 1.435999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [122880/500434 (25%)] Loss: 1.474707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [133120/500434 (27%)] Loss: 1.418910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [143360/500434 (29%)] Loss: 1.432912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [153600/500434 (31%)] Loss: 1.502985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [163840/500434 (33%)] Loss: 1.471204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [174080/500434 (35%)] Loss: 1.389663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [184320/500434 (37%)] Loss: 1.548172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [194560/500434 (39%)] Loss: 1.389757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [204800/500434 (41%)] Loss: 1.421592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [215040/500434 (43%)] Loss: 1.655956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [225280/500434 (45%)] Loss: 1.438886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [235520/500434 (47%)] Loss: 1.370836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [245760/500434 (49%)] Loss: 1.569016\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [256000/500434 (51%)] Loss: 1.415715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [266240/500434 (53%)] Loss: 1.472587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [276480/500434 (55%)] Loss: 1.350422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [286720/500434 (57%)] Loss: 1.454921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [296960/500434 (59%)] Loss: 1.399937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [307200/500434 (61%)] Loss: 1.383994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [317440/500434 (63%)] Loss: 1.447137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [327680/500434 (65%)] Loss: 1.399118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [337920/500434 (67%)] Loss: 1.397696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [348160/500434 (70%)] Loss: 1.480483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [358400/500434 (72%)] Loss: 1.508995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [368640/500434 (74%)] Loss: 1.558916\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [378880/500434 (76%)] Loss: 1.480045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [389120/500434 (78%)] Loss: 1.486712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [399360/500434 (80%)] Loss: 1.540752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [409600/500434 (82%)] Loss: 1.546031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [419840/500434 (84%)] Loss: 1.504063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [430080/500434 (86%)] Loss: 1.481463\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [440320/500434 (88%)] Loss: 1.466125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [450560/500434 (90%)] Loss: 1.396355\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [460800/500434 (92%)] Loss: 1.653593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [471040/500434 (94%)] Loss: 1.440443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [481280/500434 (96%)] Loss: 1.513894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [491520/500434 (98%)] Loss: 1.408004\u001b[0m\n",
      "\u001b[34mcurrent epoch: 97\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [0/500434 (0%)] Loss: 1.542151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [10240/500434 (2%)] Loss: 1.548550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [20480/500434 (4%)] Loss: 1.612271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [30720/500434 (6%)] Loss: 1.466432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [40960/500434 (8%)] Loss: 1.446585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [51200/500434 (10%)] Loss: 1.435343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [61440/500434 (12%)] Loss: 1.400332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [71680/500434 (14%)] Loss: 1.532206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [81920/500434 (16%)] Loss: 1.497996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [92160/500434 (18%)] Loss: 1.414139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [102400/500434 (20%)] Loss: 1.417362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [112640/500434 (22%)] Loss: 1.342117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [122880/500434 (25%)] Loss: 1.368941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [133120/500434 (27%)] Loss: 1.418170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [143360/500434 (29%)] Loss: 1.531050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [153600/500434 (31%)] Loss: 1.348086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [163840/500434 (33%)] Loss: 1.408993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [174080/500434 (35%)] Loss: 1.426627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [184320/500434 (37%)] Loss: 1.409688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [194560/500434 (39%)] Loss: 1.361995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [204800/500434 (41%)] Loss: 1.371448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [215040/500434 (43%)] Loss: 1.409924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [225280/500434 (45%)] Loss: 1.419587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [235520/500434 (47%)] Loss: 1.532883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [245760/500434 (49%)] Loss: 1.500359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [256000/500434 (51%)] Loss: 1.514887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [266240/500434 (53%)] Loss: 1.395277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [276480/500434 (55%)] Loss: 1.498879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [286720/500434 (57%)] Loss: 1.454450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [296960/500434 (59%)] Loss: 1.524199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [307200/500434 (61%)] Loss: 1.496465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [317440/500434 (63%)] Loss: 1.615387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [327680/500434 (65%)] Loss: 1.450694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [337920/500434 (67%)] Loss: 1.466643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [348160/500434 (70%)] Loss: 1.369094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [358400/500434 (72%)] Loss: 1.542468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [368640/500434 (74%)] Loss: 1.465819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [378880/500434 (76%)] Loss: 1.401495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [389120/500434 (78%)] Loss: 1.551282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [399360/500434 (80%)] Loss: 1.443790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [409600/500434 (82%)] Loss: 1.436592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [419840/500434 (84%)] Loss: 1.465361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [430080/500434 (86%)] Loss: 1.389450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [440320/500434 (88%)] Loss: 1.425173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [450560/500434 (90%)] Loss: 1.374130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [460800/500434 (92%)] Loss: 1.339264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [471040/500434 (94%)] Loss: 1.419452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [481280/500434 (96%)] Loss: 1.398376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [491520/500434 (98%)] Loss: 1.453257\u001b[0m\n",
      "\u001b[34mcurrent epoch: 98\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [0/500434 (0%)] Loss: 1.416155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [10240/500434 (2%)] Loss: 1.629307\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [20480/500434 (4%)] Loss: 1.448066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [30720/500434 (6%)] Loss: 1.387415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [40960/500434 (8%)] Loss: 1.351037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [51200/500434 (10%)] Loss: 1.440966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [61440/500434 (12%)] Loss: 1.398311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [71680/500434 (14%)] Loss: 1.387570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [81920/500434 (16%)] Loss: 1.423146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [92160/500434 (18%)] Loss: 1.539977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [102400/500434 (20%)] Loss: 1.323403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [112640/500434 (22%)] Loss: 1.349367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [122880/500434 (25%)] Loss: 1.453808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [133120/500434 (27%)] Loss: 1.391014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [143360/500434 (29%)] Loss: 1.287138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [153600/500434 (31%)] Loss: 1.577840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [163840/500434 (33%)] Loss: 1.474086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [174080/500434 (35%)] Loss: 1.429898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [184320/500434 (37%)] Loss: 1.608696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [194560/500434 (39%)] Loss: 1.544346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [204800/500434 (41%)] Loss: 1.425195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [215040/500434 (43%)] Loss: 1.700113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [225280/500434 (45%)] Loss: 1.441471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [235520/500434 (47%)] Loss: 1.481665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [245760/500434 (49%)] Loss: 1.524125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [256000/500434 (51%)] Loss: 1.433561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [266240/500434 (53%)] Loss: 1.520578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [276480/500434 (55%)] Loss: 1.482207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [286720/500434 (57%)] Loss: 1.424078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [296960/500434 (59%)] Loss: 1.380988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [307200/500434 (61%)] Loss: 1.437657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [317440/500434 (63%)] Loss: 1.377733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [327680/500434 (65%)] Loss: 1.351720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [337920/500434 (67%)] Loss: 1.321248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [348160/500434 (70%)] Loss: 1.460551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [358400/500434 (72%)] Loss: 1.485291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [368640/500434 (74%)] Loss: 1.388591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [378880/500434 (76%)] Loss: 1.488480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [389120/500434 (78%)] Loss: 1.473044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [399360/500434 (80%)] Loss: 1.425305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [409600/500434 (82%)] Loss: 1.448336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [419840/500434 (84%)] Loss: 1.386203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [430080/500434 (86%)] Loss: 1.328078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [440320/500434 (88%)] Loss: 1.416689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [450560/500434 (90%)] Loss: 1.380039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [460800/500434 (92%)] Loss: 1.483669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [471040/500434 (94%)] Loss: 1.523847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [481280/500434 (96%)] Loss: 1.416142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [491520/500434 (98%)] Loss: 1.509413\u001b[0m\n",
      "\u001b[34mcurrent epoch: 99\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [0/500434 (0%)] Loss: 1.386538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [10240/500434 (2%)] Loss: 1.384072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [20480/500434 (4%)] Loss: 1.326988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [30720/500434 (6%)] Loss: 1.335841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [40960/500434 (8%)] Loss: 1.425117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [51200/500434 (10%)] Loss: 1.327528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [61440/500434 (12%)] Loss: 1.519090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [71680/500434 (14%)] Loss: 1.391238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [81920/500434 (16%)] Loss: 1.366966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [92160/500434 (18%)] Loss: 1.499048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [102400/500434 (20%)] Loss: 1.432033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [112640/500434 (22%)] Loss: 1.409047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [122880/500434 (25%)] Loss: 1.397294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [133120/500434 (27%)] Loss: 1.437513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [143360/500434 (29%)] Loss: 1.499793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [153600/500434 (31%)] Loss: 1.512985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [163840/500434 (33%)] Loss: 1.561599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [174080/500434 (35%)] Loss: 1.448363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [184320/500434 (37%)] Loss: 1.498501\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [194560/500434 (39%)] Loss: 1.447750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [204800/500434 (41%)] Loss: 1.512693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [215040/500434 (43%)] Loss: 1.418737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [225280/500434 (45%)] Loss: 1.482039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [235520/500434 (47%)] Loss: 1.347341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [245760/500434 (49%)] Loss: 1.351404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [256000/500434 (51%)] Loss: 1.559822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [266240/500434 (53%)] Loss: 1.372853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [276480/500434 (55%)] Loss: 1.491449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [286720/500434 (57%)] Loss: 1.565877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [296960/500434 (59%)] Loss: 1.404243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [307200/500434 (61%)] Loss: 1.422122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [317440/500434 (63%)] Loss: 1.339354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [327680/500434 (65%)] Loss: 1.408633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [337920/500434 (67%)] Loss: 1.564028\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [348160/500434 (70%)] Loss: 1.383080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [358400/500434 (72%)] Loss: 1.399484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [368640/500434 (74%)] Loss: 1.448378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [378880/500434 (76%)] Loss: 1.445237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [389120/500434 (78%)] Loss: 1.407897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [399360/500434 (80%)] Loss: 1.358246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [409600/500434 (82%)] Loss: 1.372015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [419840/500434 (84%)] Loss: 1.450018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [430080/500434 (86%)] Loss: 1.413004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [440320/500434 (88%)] Loss: 1.441062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [450560/500434 (90%)] Loss: 1.438636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [460800/500434 (92%)] Loss: 1.423471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [471040/500434 (94%)] Loss: 1.478784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [481280/500434 (96%)] Loss: 1.498201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [491520/500434 (98%)] Loss: 1.453047\u001b[0m\n",
      "\u001b[34mcurrent epoch: 100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [0/500434 (0%)] Loss: 1.377580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [10240/500434 (2%)] Loss: 1.416032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [20480/500434 (4%)] Loss: 1.329865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [30720/500434 (6%)] Loss: 1.454120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [40960/500434 (8%)] Loss: 1.428819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [51200/500434 (10%)] Loss: 1.419146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [61440/500434 (12%)] Loss: 1.332036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [71680/500434 (14%)] Loss: 1.460108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [81920/500434 (16%)] Loss: 1.338199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [92160/500434 (18%)] Loss: 1.444618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [102400/500434 (20%)] Loss: 1.454805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [112640/500434 (22%)] Loss: 1.482657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [122880/500434 (25%)] Loss: 1.415818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [133120/500434 (27%)] Loss: 1.365324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [143360/500434 (29%)] Loss: 1.397473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [153600/500434 (31%)] Loss: 1.390918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [163840/500434 (33%)] Loss: 1.497943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [174080/500434 (35%)] Loss: 1.444219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [184320/500434 (37%)] Loss: 1.448669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [194560/500434 (39%)] Loss: 1.416479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [204800/500434 (41%)] Loss: 1.336543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [215040/500434 (43%)] Loss: 1.289352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [225280/500434 (45%)] Loss: 1.371385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [235520/500434 (47%)] Loss: 1.528273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [245760/500434 (49%)] Loss: 1.507083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [256000/500434 (51%)] Loss: 1.496388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [266240/500434 (53%)] Loss: 1.463328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [276480/500434 (55%)] Loss: 1.381729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [286720/500434 (57%)] Loss: 1.459280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [296960/500434 (59%)] Loss: 1.557616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [307200/500434 (61%)] Loss: 1.330048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [317440/500434 (63%)] Loss: 1.552165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [327680/500434 (65%)] Loss: 1.495092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [337920/500434 (67%)] Loss: 1.314853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [348160/500434 (70%)] Loss: 1.440838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [358400/500434 (72%)] Loss: 1.552207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [368640/500434 (74%)] Loss: 1.508798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [378880/500434 (76%)] Loss: 1.452922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [389120/500434 (78%)] Loss: 1.370326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [399360/500434 (80%)] Loss: 1.582666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [409600/500434 (82%)] Loss: 1.262244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [419840/500434 (84%)] Loss: 1.501011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [430080/500434 (86%)] Loss: 1.506481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [440320/500434 (88%)] Loss: 1.427602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [450560/500434 (90%)] Loss: 1.474627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [460800/500434 (92%)] Loss: 1.553880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [471040/500434 (94%)] Loss: 1.684570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [481280/500434 (96%)] Loss: 1.482283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [491520/500434 (98%)] Loss: 1.398753\u001b[0m\n",
      "\u001b[34mcurrent epoch: 101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [0/500434 (0%)] Loss: 1.344489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [10240/500434 (2%)] Loss: 1.428060\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [20480/500434 (4%)] Loss: 1.446958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [30720/500434 (6%)] Loss: 1.359335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [40960/500434 (8%)] Loss: 1.428891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [51200/500434 (10%)] Loss: 1.432231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [61440/500434 (12%)] Loss: 1.400210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [71680/500434 (14%)] Loss: 1.524405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [81920/500434 (16%)] Loss: 1.403471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [92160/500434 (18%)] Loss: 1.368159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [102400/500434 (20%)] Loss: 1.469969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [112640/500434 (22%)] Loss: 1.429473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [122880/500434 (25%)] Loss: 1.404085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [133120/500434 (27%)] Loss: 1.416564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [143360/500434 (29%)] Loss: 1.409423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [153600/500434 (31%)] Loss: 1.420250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [163840/500434 (33%)] Loss: 1.504504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [174080/500434 (35%)] Loss: 1.398421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [184320/500434 (37%)] Loss: 1.475678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [194560/500434 (39%)] Loss: 1.465050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [204800/500434 (41%)] Loss: 1.444044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [215040/500434 (43%)] Loss: 1.555298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [225280/500434 (45%)] Loss: 1.409067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [235520/500434 (47%)] Loss: 1.477671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [245760/500434 (49%)] Loss: 1.410491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [256000/500434 (51%)] Loss: 1.361214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [266240/500434 (53%)] Loss: 1.561482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [276480/500434 (55%)] Loss: 1.483366\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [286720/500434 (57%)] Loss: 1.490168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [296960/500434 (59%)] Loss: 1.492393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [307200/500434 (61%)] Loss: 1.337047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [317440/500434 (63%)] Loss: 1.311235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [327680/500434 (65%)] Loss: 1.393605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [337920/500434 (67%)] Loss: 1.463808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [348160/500434 (70%)] Loss: 1.465041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [358400/500434 (72%)] Loss: 1.545866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [368640/500434 (74%)] Loss: 1.409636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [378880/500434 (76%)] Loss: 1.508372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [389120/500434 (78%)] Loss: 1.387180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [399360/500434 (80%)] Loss: 1.418451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [409600/500434 (82%)] Loss: 1.397588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [419840/500434 (84%)] Loss: 1.489166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [430080/500434 (86%)] Loss: 1.427475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [440320/500434 (88%)] Loss: 1.493372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [450560/500434 (90%)] Loss: 1.393962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [460800/500434 (92%)] Loss: 1.338392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [471040/500434 (94%)] Loss: 1.426345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [481280/500434 (96%)] Loss: 1.426294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 101 [491520/500434 (98%)] Loss: 1.445838\u001b[0m\n",
      "\u001b[34mcurrent epoch: 102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [0/500434 (0%)] Loss: 1.408125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [10240/500434 (2%)] Loss: 1.491068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [20480/500434 (4%)] Loss: 1.404309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [30720/500434 (6%)] Loss: 1.492337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [40960/500434 (8%)] Loss: 1.356708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [51200/500434 (10%)] Loss: 1.474332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [61440/500434 (12%)] Loss: 1.390688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [71680/500434 (14%)] Loss: 1.374172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [81920/500434 (16%)] Loss: 1.561770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [92160/500434 (18%)] Loss: 1.425760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [102400/500434 (20%)] Loss: 1.279320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [112640/500434 (22%)] Loss: 1.404333\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [122880/500434 (25%)] Loss: 1.374430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [133120/500434 (27%)] Loss: 1.407062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [143360/500434 (29%)] Loss: 1.415334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [153600/500434 (31%)] Loss: 1.402834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [163840/500434 (33%)] Loss: 1.499735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [174080/500434 (35%)] Loss: 1.422024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [184320/500434 (37%)] Loss: 1.388945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [194560/500434 (39%)] Loss: 1.504207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [204800/500434 (41%)] Loss: 1.263539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [215040/500434 (43%)] Loss: 1.325324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [225280/500434 (45%)] Loss: 1.459120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [235520/500434 (47%)] Loss: 1.463847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [245760/500434 (49%)] Loss: 1.386750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [256000/500434 (51%)] Loss: 1.405666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [266240/500434 (53%)] Loss: 1.414239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [276480/500434 (55%)] Loss: 1.479309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [286720/500434 (57%)] Loss: 1.499048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [296960/500434 (59%)] Loss: 1.382709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [307200/500434 (61%)] Loss: 1.412857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [317440/500434 (63%)] Loss: 1.496632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [327680/500434 (65%)] Loss: 1.338544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [337920/500434 (67%)] Loss: 1.473666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [348160/500434 (70%)] Loss: 1.406421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [358400/500434 (72%)] Loss: 1.471019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [368640/500434 (74%)] Loss: 1.366482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [378880/500434 (76%)] Loss: 1.412836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [389120/500434 (78%)] Loss: 1.303090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [399360/500434 (80%)] Loss: 1.474321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [409600/500434 (82%)] Loss: 1.483018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [419840/500434 (84%)] Loss: 1.439052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [430080/500434 (86%)] Loss: 1.403898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [440320/500434 (88%)] Loss: 1.555952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [450560/500434 (90%)] Loss: 1.441978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [460800/500434 (92%)] Loss: 1.370071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [471040/500434 (94%)] Loss: 1.374553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [481280/500434 (96%)] Loss: 1.343382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 102 [491520/500434 (98%)] Loss: 1.373678\u001b[0m\n",
      "\u001b[34mcurrent epoch: 103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [0/500434 (0%)] Loss: 1.365241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [10240/500434 (2%)] Loss: 1.371978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [20480/500434 (4%)] Loss: 1.442376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [30720/500434 (6%)] Loss: 1.338531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [40960/500434 (8%)] Loss: 1.402731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [51200/500434 (10%)] Loss: 1.341341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [61440/500434 (12%)] Loss: 1.359523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [71680/500434 (14%)] Loss: 1.367008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [81920/500434 (16%)] Loss: 1.408765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [92160/500434 (18%)] Loss: 1.450882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [102400/500434 (20%)] Loss: 1.438543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [112640/500434 (22%)] Loss: 1.368887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [122880/500434 (25%)] Loss: 1.400851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [133120/500434 (27%)] Loss: 1.400808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [143360/500434 (29%)] Loss: 1.381133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [153600/500434 (31%)] Loss: 1.454192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [163840/500434 (33%)] Loss: 1.355625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [174080/500434 (35%)] Loss: 1.418437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [184320/500434 (37%)] Loss: 1.432602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [194560/500434 (39%)] Loss: 1.287376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [204800/500434 (41%)] Loss: 1.395030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [215040/500434 (43%)] Loss: 1.403255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [225280/500434 (45%)] Loss: 1.406011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [235520/500434 (47%)] Loss: 1.368711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [245760/500434 (49%)] Loss: 1.409950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [256000/500434 (51%)] Loss: 1.431823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [266240/500434 (53%)] Loss: 1.366432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [276480/500434 (55%)] Loss: 1.414603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [286720/500434 (57%)] Loss: 1.291423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [296960/500434 (59%)] Loss: 1.335764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [307200/500434 (61%)] Loss: 1.495297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [317440/500434 (63%)] Loss: 1.411280\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [327680/500434 (65%)] Loss: 1.408065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [337920/500434 (67%)] Loss: 1.421486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [348160/500434 (70%)] Loss: 1.347783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [358400/500434 (72%)] Loss: 1.380167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [368640/500434 (74%)] Loss: 1.352098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [378880/500434 (76%)] Loss: 1.713910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [389120/500434 (78%)] Loss: 1.763023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [399360/500434 (80%)] Loss: 2.510673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [409600/500434 (82%)] Loss: 1.747336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [419840/500434 (84%)] Loss: 1.565449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [430080/500434 (86%)] Loss: 1.508019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [440320/500434 (88%)] Loss: 1.295416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [450560/500434 (90%)] Loss: 1.318498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [460800/500434 (92%)] Loss: 1.488910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [471040/500434 (94%)] Loss: 1.568697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [481280/500434 (96%)] Loss: 1.451019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 103 [491520/500434 (98%)] Loss: 1.377428\u001b[0m\n",
      "\u001b[34mcurrent epoch: 104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [0/500434 (0%)] Loss: 1.489290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [10240/500434 (2%)] Loss: 1.318698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [20480/500434 (4%)] Loss: 1.459935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [30720/500434 (6%)] Loss: 1.375477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [40960/500434 (8%)] Loss: 1.381196\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [51200/500434 (10%)] Loss: 1.456262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [61440/500434 (12%)] Loss: 1.328098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [71680/500434 (14%)] Loss: 1.382855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [81920/500434 (16%)] Loss: 1.346876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [92160/500434 (18%)] Loss: 1.354245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [102400/500434 (20%)] Loss: 1.422046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [112640/500434 (22%)] Loss: 1.353510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [122880/500434 (25%)] Loss: 1.375284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [133120/500434 (27%)] Loss: 1.420326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [143360/500434 (29%)] Loss: 1.376275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [153600/500434 (31%)] Loss: 1.342480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [163840/500434 (33%)] Loss: 1.387788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [174080/500434 (35%)] Loss: 1.381716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [184320/500434 (37%)] Loss: 1.359482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [194560/500434 (39%)] Loss: 1.358026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [204800/500434 (41%)] Loss: 1.447202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [215040/500434 (43%)] Loss: 1.458259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [225280/500434 (45%)] Loss: 1.468445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [235520/500434 (47%)] Loss: 1.445002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [245760/500434 (49%)] Loss: 1.436903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [256000/500434 (51%)] Loss: 1.353394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [266240/500434 (53%)] Loss: 1.434935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [276480/500434 (55%)] Loss: 1.399681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [286720/500434 (57%)] Loss: 1.424600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [296960/500434 (59%)] Loss: 1.317580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [307200/500434 (61%)] Loss: 1.435162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [317440/500434 (63%)] Loss: 1.356001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [327680/500434 (65%)] Loss: 1.422014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [337920/500434 (67%)] Loss: 1.392545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [348160/500434 (70%)] Loss: 1.379279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [358400/500434 (72%)] Loss: 1.411527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [368640/500434 (74%)] Loss: 1.406521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [378880/500434 (76%)] Loss: 1.241972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [389120/500434 (78%)] Loss: 1.392165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [399360/500434 (80%)] Loss: 1.348475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [409600/500434 (82%)] Loss: 1.451431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [419840/500434 (84%)] Loss: 1.342911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [430080/500434 (86%)] Loss: 1.462894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [440320/500434 (88%)] Loss: 1.372733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [450560/500434 (90%)] Loss: 1.420248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [460800/500434 (92%)] Loss: 1.351124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [471040/500434 (94%)] Loss: 1.351706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [481280/500434 (96%)] Loss: 1.409534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 104 [491520/500434 (98%)] Loss: 1.457904\u001b[0m\n",
      "\u001b[34mcurrent epoch: 105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [0/500434 (0%)] Loss: 1.369485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [10240/500434 (2%)] Loss: 1.224053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [20480/500434 (4%)] Loss: 1.352780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [30720/500434 (6%)] Loss: 1.369571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [40960/500434 (8%)] Loss: 1.390209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [51200/500434 (10%)] Loss: 1.448046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [61440/500434 (12%)] Loss: 1.319056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [71680/500434 (14%)] Loss: 1.447005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [81920/500434 (16%)] Loss: 1.455123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [92160/500434 (18%)] Loss: 1.372274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [102400/500434 (20%)] Loss: 1.381027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [112640/500434 (22%)] Loss: 1.433503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [122880/500434 (25%)] Loss: 1.325572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [133120/500434 (27%)] Loss: 1.342407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [143360/500434 (29%)] Loss: 1.447062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [153600/500434 (31%)] Loss: 1.423924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [163840/500434 (33%)] Loss: 1.326768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [174080/500434 (35%)] Loss: 1.543570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [184320/500434 (37%)] Loss: 1.309385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [194560/500434 (39%)] Loss: 1.403783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [204800/500434 (41%)] Loss: 1.400076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [215040/500434 (43%)] Loss: 1.380824\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [225280/500434 (45%)] Loss: 1.351063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [235520/500434 (47%)] Loss: 1.498770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [245760/500434 (49%)] Loss: 1.310897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [256000/500434 (51%)] Loss: 1.453444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [266240/500434 (53%)] Loss: 1.564295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [276480/500434 (55%)] Loss: 1.385285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [286720/500434 (57%)] Loss: 1.661470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [296960/500434 (59%)] Loss: 1.537986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [307200/500434 (61%)] Loss: 1.330198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [317440/500434 (63%)] Loss: 1.336278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [327680/500434 (65%)] Loss: 1.384509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [337920/500434 (67%)] Loss: 1.251466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [348160/500434 (70%)] Loss: 1.349112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [358400/500434 (72%)] Loss: 1.349160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [368640/500434 (74%)] Loss: 1.334453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [378880/500434 (76%)] Loss: 1.401565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [389120/500434 (78%)] Loss: 1.474414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [399360/500434 (80%)] Loss: 1.344365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [409600/500434 (82%)] Loss: 1.453729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [419840/500434 (84%)] Loss: 1.387795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [430080/500434 (86%)] Loss: 1.357303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [440320/500434 (88%)] Loss: 1.443176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [450560/500434 (90%)] Loss: 1.401395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [460800/500434 (92%)] Loss: 1.441759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [471040/500434 (94%)] Loss: 1.359644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [481280/500434 (96%)] Loss: 1.336565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 105 [491520/500434 (98%)] Loss: 1.336537\u001b[0m\n",
      "\u001b[34mcurrent epoch: 106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [0/500434 (0%)] Loss: 1.287755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [10240/500434 (2%)] Loss: 1.406011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [20480/500434 (4%)] Loss: 1.287094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [30720/500434 (6%)] Loss: 1.379180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [40960/500434 (8%)] Loss: 1.236097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [51200/500434 (10%)] Loss: 1.387891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [61440/500434 (12%)] Loss: 1.295209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [71680/500434 (14%)] Loss: 1.364013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [81920/500434 (16%)] Loss: 1.275069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [92160/500434 (18%)] Loss: 1.439255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [102400/500434 (20%)] Loss: 1.396861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [112640/500434 (22%)] Loss: 1.386270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [122880/500434 (25%)] Loss: 1.365861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [133120/500434 (27%)] Loss: 1.385231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [143360/500434 (29%)] Loss: 1.381628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [153600/500434 (31%)] Loss: 1.397353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [163840/500434 (33%)] Loss: 1.342807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [174080/500434 (35%)] Loss: 1.386082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [184320/500434 (37%)] Loss: 1.340628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [194560/500434 (39%)] Loss: 1.402902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [204800/500434 (41%)] Loss: 1.447300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [215040/500434 (43%)] Loss: 1.292595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [225280/500434 (45%)] Loss: 1.246014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [235520/500434 (47%)] Loss: 1.401139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [245760/500434 (49%)] Loss: 1.527435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [256000/500434 (51%)] Loss: 1.429900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [266240/500434 (53%)] Loss: 1.355746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [276480/500434 (55%)] Loss: 1.330809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [286720/500434 (57%)] Loss: 1.331746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [296960/500434 (59%)] Loss: 1.418059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [307200/500434 (61%)] Loss: 1.382996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [317440/500434 (63%)] Loss: 1.434389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [327680/500434 (65%)] Loss: 1.360022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [337920/500434 (67%)] Loss: 1.343632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [348160/500434 (70%)] Loss: 1.316157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [358400/500434 (72%)] Loss: 1.296547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [368640/500434 (74%)] Loss: 1.286361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [378880/500434 (76%)] Loss: 1.350677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [389120/500434 (78%)] Loss: 1.452124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [399360/500434 (80%)] Loss: 1.416441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [409600/500434 (82%)] Loss: 1.348089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [419840/500434 (84%)] Loss: 1.334801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [430080/500434 (86%)] Loss: 1.442464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [440320/500434 (88%)] Loss: 1.417668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [450560/500434 (90%)] Loss: 1.397575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [460800/500434 (92%)] Loss: 1.349831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [471040/500434 (94%)] Loss: 1.389308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [481280/500434 (96%)] Loss: 1.346850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 106 [491520/500434 (98%)] Loss: 1.394601\u001b[0m\n",
      "\u001b[34mcurrent epoch: 107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [0/500434 (0%)] Loss: 1.334621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [10240/500434 (2%)] Loss: 1.322725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [20480/500434 (4%)] Loss: 1.386896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [30720/500434 (6%)] Loss: 1.277651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [40960/500434 (8%)] Loss: 1.374241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [51200/500434 (10%)] Loss: 1.314122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [61440/500434 (12%)] Loss: 1.374981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [71680/500434 (14%)] Loss: 1.392371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [81920/500434 (16%)] Loss: 1.403767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [92160/500434 (18%)] Loss: 1.311068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [102400/500434 (20%)] Loss: 1.362646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [112640/500434 (22%)] Loss: 1.380843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [122880/500434 (25%)] Loss: 1.374392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [133120/500434 (27%)] Loss: 1.377974\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [143360/500434 (29%)] Loss: 1.351271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [153600/500434 (31%)] Loss: 1.408944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [163840/500434 (33%)] Loss: 1.379335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [174080/500434 (35%)] Loss: 1.411591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [184320/500434 (37%)] Loss: 1.356047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [194560/500434 (39%)] Loss: 1.298325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [204800/500434 (41%)] Loss: 1.432137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [215040/500434 (43%)] Loss: 1.342216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [225280/500434 (45%)] Loss: 1.371502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [235520/500434 (47%)] Loss: 1.427806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [245760/500434 (49%)] Loss: 1.409111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [256000/500434 (51%)] Loss: 1.365710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [266240/500434 (53%)] Loss: 1.638476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [276480/500434 (55%)] Loss: 1.400110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [286720/500434 (57%)] Loss: 1.428341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [296960/500434 (59%)] Loss: 1.426422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [307200/500434 (61%)] Loss: 1.291305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [317440/500434 (63%)] Loss: 1.294415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [327680/500434 (65%)] Loss: 1.312291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [337920/500434 (67%)] Loss: 1.287405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [348160/500434 (70%)] Loss: 1.403502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [358400/500434 (72%)] Loss: 1.318564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [368640/500434 (74%)] Loss: 1.432302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [378880/500434 (76%)] Loss: 1.362479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [389120/500434 (78%)] Loss: 1.324162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [399360/500434 (80%)] Loss: 1.365897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [409600/500434 (82%)] Loss: 1.368215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [419840/500434 (84%)] Loss: 1.422201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [430080/500434 (86%)] Loss: 1.367495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [440320/500434 (88%)] Loss: 1.526474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [450560/500434 (90%)] Loss: 1.342248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [460800/500434 (92%)] Loss: 1.430328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [471040/500434 (94%)] Loss: 1.372297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [481280/500434 (96%)] Loss: 1.353266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 107 [491520/500434 (98%)] Loss: 1.384442\u001b[0m\n",
      "\u001b[34mcurrent epoch: 108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [0/500434 (0%)] Loss: 1.386804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [10240/500434 (2%)] Loss: 1.372005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [20480/500434 (4%)] Loss: 1.276952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [30720/500434 (6%)] Loss: 1.317015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [40960/500434 (8%)] Loss: 1.344765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [51200/500434 (10%)] Loss: 1.442900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [61440/500434 (12%)] Loss: 1.269191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [71680/500434 (14%)] Loss: 1.402949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [81920/500434 (16%)] Loss: 1.241795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [92160/500434 (18%)] Loss: 1.381571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [102400/500434 (20%)] Loss: 1.311165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [112640/500434 (22%)] Loss: 1.293479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [122880/500434 (25%)] Loss: 1.473414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [133120/500434 (27%)] Loss: 1.424310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [143360/500434 (29%)] Loss: 1.364224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [153600/500434 (31%)] Loss: 1.316149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [163840/500434 (33%)] Loss: 1.329217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [174080/500434 (35%)] Loss: 1.476001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [184320/500434 (37%)] Loss: 1.394486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [194560/500434 (39%)] Loss: 1.428334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [204800/500434 (41%)] Loss: 1.352856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [215040/500434 (43%)] Loss: 1.432203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [225280/500434 (45%)] Loss: 1.460168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [235520/500434 (47%)] Loss: 1.439312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [245760/500434 (49%)] Loss: 1.477730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [256000/500434 (51%)] Loss: 1.417962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [266240/500434 (53%)] Loss: 1.319708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [276480/500434 (55%)] Loss: 1.380711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [286720/500434 (57%)] Loss: 1.340036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [296960/500434 (59%)] Loss: 1.330869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [307200/500434 (61%)] Loss: 1.413510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [317440/500434 (63%)] Loss: 1.354326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [327680/500434 (65%)] Loss: 1.342647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [337920/500434 (67%)] Loss: 1.351250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [348160/500434 (70%)] Loss: 1.300682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [358400/500434 (72%)] Loss: 1.379168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [368640/500434 (74%)] Loss: 1.396066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [378880/500434 (76%)] Loss: 1.276662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [389120/500434 (78%)] Loss: 1.421762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [399360/500434 (80%)] Loss: 1.451684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [409600/500434 (82%)] Loss: 1.301380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [419840/500434 (84%)] Loss: 1.351799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [430080/500434 (86%)] Loss: 1.363730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [440320/500434 (88%)] Loss: 1.354259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [450560/500434 (90%)] Loss: 1.291855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [460800/500434 (92%)] Loss: 1.245825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [471040/500434 (94%)] Loss: 1.407643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [481280/500434 (96%)] Loss: 1.459847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 108 [491520/500434 (98%)] Loss: 1.367869\u001b[0m\n",
      "\u001b[34mcurrent epoch: 109\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [0/500434 (0%)] Loss: 1.330530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [10240/500434 (2%)] Loss: 1.358597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [20480/500434 (4%)] Loss: 1.345570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [30720/500434 (6%)] Loss: 1.405823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [40960/500434 (8%)] Loss: 1.371037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [51200/500434 (10%)] Loss: 1.207103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [61440/500434 (12%)] Loss: 1.376477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [71680/500434 (14%)] Loss: 1.341441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [81920/500434 (16%)] Loss: 1.368864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [92160/500434 (18%)] Loss: 1.437107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [102400/500434 (20%)] Loss: 1.374007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [112640/500434 (22%)] Loss: 1.288817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [122880/500434 (25%)] Loss: 1.363003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [133120/500434 (27%)] Loss: 1.401450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [143360/500434 (29%)] Loss: 1.352866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [153600/500434 (31%)] Loss: 1.355358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [163840/500434 (33%)] Loss: 1.301770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [174080/500434 (35%)] Loss: 1.471724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [184320/500434 (37%)] Loss: 1.311237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [194560/500434 (39%)] Loss: 1.442158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [204800/500434 (41%)] Loss: 1.514986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [215040/500434 (43%)] Loss: 1.379847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [225280/500434 (45%)] Loss: 1.362489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [235520/500434 (47%)] Loss: 1.431667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [245760/500434 (49%)] Loss: 1.310356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [256000/500434 (51%)] Loss: 1.329092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [266240/500434 (53%)] Loss: 1.377669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [276480/500434 (55%)] Loss: 1.378092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [286720/500434 (57%)] Loss: 1.306387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [296960/500434 (59%)] Loss: 1.359764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [307200/500434 (61%)] Loss: 1.350411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [317440/500434 (63%)] Loss: 1.277137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [327680/500434 (65%)] Loss: 1.394572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [337920/500434 (67%)] Loss: 1.441290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [348160/500434 (70%)] Loss: 1.352316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [358400/500434 (72%)] Loss: 1.367232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [368640/500434 (74%)] Loss: 1.312049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [378880/500434 (76%)] Loss: 1.458044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [389120/500434 (78%)] Loss: 1.317352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [399360/500434 (80%)] Loss: 1.284792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [409600/500434 (82%)] Loss: 1.284179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [419840/500434 (84%)] Loss: 1.354195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [430080/500434 (86%)] Loss: 1.412051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [440320/500434 (88%)] Loss: 1.328151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [450560/500434 (90%)] Loss: 1.381234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [460800/500434 (92%)] Loss: 1.441805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [471040/500434 (94%)] Loss: 1.245758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [481280/500434 (96%)] Loss: 1.431197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 109 [491520/500434 (98%)] Loss: 1.333043\u001b[0m\n",
      "\u001b[34mcurrent epoch: 110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [0/500434 (0%)] Loss: 1.331840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [10240/500434 (2%)] Loss: 1.452905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [20480/500434 (4%)] Loss: 1.291445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [30720/500434 (6%)] Loss: 1.318987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [40960/500434 (8%)] Loss: 1.281206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [51200/500434 (10%)] Loss: 1.249445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [61440/500434 (12%)] Loss: 1.323513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [71680/500434 (14%)] Loss: 1.355350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [81920/500434 (16%)] Loss: 1.384652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [92160/500434 (18%)] Loss: 1.365538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [102400/500434 (20%)] Loss: 1.287565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [112640/500434 (22%)] Loss: 1.276408\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [122880/500434 (25%)] Loss: 1.304046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [133120/500434 (27%)] Loss: 1.462220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [143360/500434 (29%)] Loss: 1.360499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [153600/500434 (31%)] Loss: 1.308441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [163840/500434 (33%)] Loss: 1.342153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [174080/500434 (35%)] Loss: 1.416185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [184320/500434 (37%)] Loss: 1.329948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [194560/500434 (39%)] Loss: 1.262124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [204800/500434 (41%)] Loss: 1.654783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [215040/500434 (43%)] Loss: 1.442859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [225280/500434 (45%)] Loss: 1.318488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [235520/500434 (47%)] Loss: 1.495538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [245760/500434 (49%)] Loss: 1.423699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [256000/500434 (51%)] Loss: 1.468451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [266240/500434 (53%)] Loss: 1.458188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [276480/500434 (55%)] Loss: 1.527268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [286720/500434 (57%)] Loss: 1.620326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [296960/500434 (59%)] Loss: 1.496115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [307200/500434 (61%)] Loss: 1.451747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [317440/500434 (63%)] Loss: 1.348128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [327680/500434 (65%)] Loss: 1.369817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [337920/500434 (67%)] Loss: 1.434531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [348160/500434 (70%)] Loss: 1.398061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [358400/500434 (72%)] Loss: 1.216228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [368640/500434 (74%)] Loss: 1.450550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [378880/500434 (76%)] Loss: 1.439703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [389120/500434 (78%)] Loss: 1.376981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [399360/500434 (80%)] Loss: 1.470774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [409600/500434 (82%)] Loss: 1.347963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [419840/500434 (84%)] Loss: 1.354717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [430080/500434 (86%)] Loss: 1.426795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [440320/500434 (88%)] Loss: 1.317001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [450560/500434 (90%)] Loss: 1.292742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [460800/500434 (92%)] Loss: 1.376609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [471040/500434 (94%)] Loss: 1.455882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [481280/500434 (96%)] Loss: 1.313862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 110 [491520/500434 (98%)] Loss: 1.349250\u001b[0m\n",
      "\u001b[34mcurrent epoch: 111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [0/500434 (0%)] Loss: 1.299561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [10240/500434 (2%)] Loss: 1.412400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [20480/500434 (4%)] Loss: 1.341151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [30720/500434 (6%)] Loss: 1.290489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [40960/500434 (8%)] Loss: 1.366293\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [51200/500434 (10%)] Loss: 1.371924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [61440/500434 (12%)] Loss: 1.443582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [71680/500434 (14%)] Loss: 1.425282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [81920/500434 (16%)] Loss: 1.340873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [92160/500434 (18%)] Loss: 1.309596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [102400/500434 (20%)] Loss: 1.474089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [112640/500434 (22%)] Loss: 1.468553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [122880/500434 (25%)] Loss: 1.517041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [133120/500434 (27%)] Loss: 1.633218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [143360/500434 (29%)] Loss: 1.476254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [153600/500434 (31%)] Loss: 1.577310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [163840/500434 (33%)] Loss: 1.509644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [174080/500434 (35%)] Loss: 1.471185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [184320/500434 (37%)] Loss: 1.511132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [194560/500434 (39%)] Loss: 1.478798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [204800/500434 (41%)] Loss: 1.373806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [215040/500434 (43%)] Loss: 1.501257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [225280/500434 (45%)] Loss: 1.470329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [235520/500434 (47%)] Loss: 1.404232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [245760/500434 (49%)] Loss: 1.411689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [256000/500434 (51%)] Loss: 1.391042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [266240/500434 (53%)] Loss: 1.395697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [276480/500434 (55%)] Loss: 1.279242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [286720/500434 (57%)] Loss: 1.310707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [296960/500434 (59%)] Loss: 1.468047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [307200/500434 (61%)] Loss: 1.342527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [317440/500434 (63%)] Loss: 1.244400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [327680/500434 (65%)] Loss: 1.329456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [337920/500434 (67%)] Loss: 1.573643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [348160/500434 (70%)] Loss: 1.397093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [358400/500434 (72%)] Loss: 1.549856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [368640/500434 (74%)] Loss: 1.329086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [378880/500434 (76%)] Loss: 1.248497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [389120/500434 (78%)] Loss: 1.398107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [399360/500434 (80%)] Loss: 1.267826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [409600/500434 (82%)] Loss: 1.430147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [419840/500434 (84%)] Loss: 1.284113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [430080/500434 (86%)] Loss: 1.408092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [440320/500434 (88%)] Loss: 1.463283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [450560/500434 (90%)] Loss: 1.390493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [460800/500434 (92%)] Loss: 1.380517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [471040/500434 (94%)] Loss: 1.420429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [481280/500434 (96%)] Loss: 1.402605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 111 [491520/500434 (98%)] Loss: 1.420441\u001b[0m\n",
      "\u001b[34mcurrent epoch: 112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [0/500434 (0%)] Loss: 1.317420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [10240/500434 (2%)] Loss: 1.388560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [20480/500434 (4%)] Loss: 1.499850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [30720/500434 (6%)] Loss: 1.275624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [40960/500434 (8%)] Loss: 1.306414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [51200/500434 (10%)] Loss: 1.313272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [61440/500434 (12%)] Loss: 1.511465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [71680/500434 (14%)] Loss: 1.326500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [81920/500434 (16%)] Loss: 1.296580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [92160/500434 (18%)] Loss: 1.281217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [102400/500434 (20%)] Loss: 1.329674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [112640/500434 (22%)] Loss: 1.450603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [122880/500434 (25%)] Loss: 1.299230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [133120/500434 (27%)] Loss: 1.384290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [143360/500434 (29%)] Loss: 1.285392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [153600/500434 (31%)] Loss: 1.415827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [163840/500434 (33%)] Loss: 1.437982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [174080/500434 (35%)] Loss: 1.326796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [184320/500434 (37%)] Loss: 1.312412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [194560/500434 (39%)] Loss: 1.325771\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [204800/500434 (41%)] Loss: 1.407403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [215040/500434 (43%)] Loss: 1.328358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [225280/500434 (45%)] Loss: 1.449973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [235520/500434 (47%)] Loss: 1.375564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [245760/500434 (49%)] Loss: 1.320543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [256000/500434 (51%)] Loss: 1.309314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [266240/500434 (53%)] Loss: 1.318149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [276480/500434 (55%)] Loss: 1.356198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [286720/500434 (57%)] Loss: 1.375817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [296960/500434 (59%)] Loss: 1.402540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [307200/500434 (61%)] Loss: 1.452939\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [317440/500434 (63%)] Loss: 1.328145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [327680/500434 (65%)] Loss: 1.343431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [337920/500434 (67%)] Loss: 1.347657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [348160/500434 (70%)] Loss: 1.421567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [358400/500434 (72%)] Loss: 1.354695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [368640/500434 (74%)] Loss: 1.344625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [378880/500434 (76%)] Loss: 1.298476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [389120/500434 (78%)] Loss: 1.291471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [399360/500434 (80%)] Loss: 1.315835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [409600/500434 (82%)] Loss: 1.398063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [419840/500434 (84%)] Loss: 1.420248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [430080/500434 (86%)] Loss: 1.393104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [440320/500434 (88%)] Loss: 1.419811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [450560/500434 (90%)] Loss: 1.358054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [460800/500434 (92%)] Loss: 1.301001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [471040/500434 (94%)] Loss: 1.377478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [481280/500434 (96%)] Loss: 1.438087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 112 [491520/500434 (98%)] Loss: 1.264745\u001b[0m\n",
      "\u001b[34mcurrent epoch: 113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [0/500434 (0%)] Loss: 1.452237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [10240/500434 (2%)] Loss: 1.444368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [20480/500434 (4%)] Loss: 1.361515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [30720/500434 (6%)] Loss: 1.417682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [40960/500434 (8%)] Loss: 1.466230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [51200/500434 (10%)] Loss: 1.391842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [61440/500434 (12%)] Loss: 1.415334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [71680/500434 (14%)] Loss: 1.335080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [81920/500434 (16%)] Loss: 1.214459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [92160/500434 (18%)] Loss: 1.394890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [102400/500434 (20%)] Loss: 1.507832\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [112640/500434 (22%)] Loss: 1.266299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [122880/500434 (25%)] Loss: 1.424633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [133120/500434 (27%)] Loss: 1.374567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [143360/500434 (29%)] Loss: 1.335408\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [153600/500434 (31%)] Loss: 1.339342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [163840/500434 (33%)] Loss: 1.309488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [174080/500434 (35%)] Loss: 1.391051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [184320/500434 (37%)] Loss: 1.310516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [194560/500434 (39%)] Loss: 1.374040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [204800/500434 (41%)] Loss: 1.423469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [215040/500434 (43%)] Loss: 1.394674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [225280/500434 (45%)] Loss: 1.282398\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [235520/500434 (47%)] Loss: 1.365391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [245760/500434 (49%)] Loss: 1.349498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [256000/500434 (51%)] Loss: 1.348913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [266240/500434 (53%)] Loss: 1.553428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [276480/500434 (55%)] Loss: 1.304452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [286720/500434 (57%)] Loss: 1.315812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [296960/500434 (59%)] Loss: 1.494531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [307200/500434 (61%)] Loss: 1.443466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [317440/500434 (63%)] Loss: 1.403317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [327680/500434 (65%)] Loss: 1.369635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [337920/500434 (67%)] Loss: 1.329224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [348160/500434 (70%)] Loss: 1.338226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [358400/500434 (72%)] Loss: 1.249561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [368640/500434 (74%)] Loss: 1.383762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [378880/500434 (76%)] Loss: 1.391236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [389120/500434 (78%)] Loss: 1.355666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [399360/500434 (80%)] Loss: 1.327759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [409600/500434 (82%)] Loss: 1.369997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [419840/500434 (84%)] Loss: 1.314427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [430080/500434 (86%)] Loss: 1.441409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [440320/500434 (88%)] Loss: 1.250596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [450560/500434 (90%)] Loss: 1.197244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [460800/500434 (92%)] Loss: 1.393725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [471040/500434 (94%)] Loss: 1.438085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [481280/500434 (96%)] Loss: 1.317284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 113 [491520/500434 (98%)] Loss: 1.375989\u001b[0m\n",
      "\u001b[34mcurrent epoch: 114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [0/500434 (0%)] Loss: 1.407429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [10240/500434 (2%)] Loss: 1.354059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [20480/500434 (4%)] Loss: 1.451202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [30720/500434 (6%)] Loss: 1.653945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [40960/500434 (8%)] Loss: 1.434922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [51200/500434 (10%)] Loss: 1.436232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [61440/500434 (12%)] Loss: 1.325817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [71680/500434 (14%)] Loss: 1.359303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [81920/500434 (16%)] Loss: 1.321009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [92160/500434 (18%)] Loss: 1.482032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [102400/500434 (20%)] Loss: 1.277243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [112640/500434 (22%)] Loss: 1.423432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [122880/500434 (25%)] Loss: 1.380264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [133120/500434 (27%)] Loss: 1.353853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [143360/500434 (29%)] Loss: 1.383174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [153600/500434 (31%)] Loss: 1.431662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [163840/500434 (33%)] Loss: 1.304934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [174080/500434 (35%)] Loss: 1.348448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [184320/500434 (37%)] Loss: 1.319763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [194560/500434 (39%)] Loss: 1.362959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [204800/500434 (41%)] Loss: 1.374502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [215040/500434 (43%)] Loss: 1.370486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [225280/500434 (45%)] Loss: 1.363418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [235520/500434 (47%)] Loss: 1.236656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [245760/500434 (49%)] Loss: 1.331844\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [256000/500434 (51%)] Loss: 1.364054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [266240/500434 (53%)] Loss: 1.412898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [276480/500434 (55%)] Loss: 1.367855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [286720/500434 (57%)] Loss: 1.407103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [296960/500434 (59%)] Loss: 1.315584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [307200/500434 (61%)] Loss: 1.235599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [317440/500434 (63%)] Loss: 1.287376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [327680/500434 (65%)] Loss: 1.253780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [337920/500434 (67%)] Loss: 1.405720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [348160/500434 (70%)] Loss: 1.403693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [358400/500434 (72%)] Loss: 1.297026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [368640/500434 (74%)] Loss: 1.295761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [378880/500434 (76%)] Loss: 1.351967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [389120/500434 (78%)] Loss: 1.291988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [399360/500434 (80%)] Loss: 1.331684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [409600/500434 (82%)] Loss: 1.529919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [419840/500434 (84%)] Loss: 1.299232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [430080/500434 (86%)] Loss: 1.407181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [440320/500434 (88%)] Loss: 1.412288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [450560/500434 (90%)] Loss: 1.327767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [460800/500434 (92%)] Loss: 1.311665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [471040/500434 (94%)] Loss: 1.339882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [481280/500434 (96%)] Loss: 1.219300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 114 [491520/500434 (98%)] Loss: 1.339540\u001b[0m\n",
      "\u001b[34mcurrent epoch: 115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [0/500434 (0%)] Loss: 1.337859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [10240/500434 (2%)] Loss: 1.521325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [20480/500434 (4%)] Loss: 1.326841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [30720/500434 (6%)] Loss: 1.333068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [40960/500434 (8%)] Loss: 1.355538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [51200/500434 (10%)] Loss: 1.334554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [61440/500434 (12%)] Loss: 1.350743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [71680/500434 (14%)] Loss: 1.345024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [81920/500434 (16%)] Loss: 1.314776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [92160/500434 (18%)] Loss: 1.301153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [102400/500434 (20%)] Loss: 1.347830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [112640/500434 (22%)] Loss: 1.337225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [122880/500434 (25%)] Loss: 1.384820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [133120/500434 (27%)] Loss: 1.383210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [143360/500434 (29%)] Loss: 1.407843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [153600/500434 (31%)] Loss: 1.309975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [163840/500434 (33%)] Loss: 1.341590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [174080/500434 (35%)] Loss: 1.401762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [184320/500434 (37%)] Loss: 1.267601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [194560/500434 (39%)] Loss: 1.319108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [204800/500434 (41%)] Loss: 1.382925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [215040/500434 (43%)] Loss: 1.390349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [225280/500434 (45%)] Loss: 1.530690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [235520/500434 (47%)] Loss: 1.562504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [245760/500434 (49%)] Loss: 1.609998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [256000/500434 (51%)] Loss: 1.499708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [266240/500434 (53%)] Loss: 1.367209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [276480/500434 (55%)] Loss: 1.376517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [286720/500434 (57%)] Loss: 1.472126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [296960/500434 (59%)] Loss: 1.438838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [307200/500434 (61%)] Loss: 1.423439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [317440/500434 (63%)] Loss: 1.381932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [327680/500434 (65%)] Loss: 1.330415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [337920/500434 (67%)] Loss: 1.202995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [348160/500434 (70%)] Loss: 1.373504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [358400/500434 (72%)] Loss: 1.323362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [368640/500434 (74%)] Loss: 1.261612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [378880/500434 (76%)] Loss: 1.487264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [389120/500434 (78%)] Loss: 1.374445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [399360/500434 (80%)] Loss: 1.332868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [409600/500434 (82%)] Loss: 1.258391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [419840/500434 (84%)] Loss: 1.358281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [430080/500434 (86%)] Loss: 1.440792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [440320/500434 (88%)] Loss: 1.177029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [450560/500434 (90%)] Loss: 1.193556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [460800/500434 (92%)] Loss: 1.332243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [471040/500434 (94%)] Loss: 1.609669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [481280/500434 (96%)] Loss: 1.450537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 115 [491520/500434 (98%)] Loss: 1.450631\u001b[0m\n",
      "\u001b[34mcurrent epoch: 116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [0/500434 (0%)] Loss: 1.312926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [10240/500434 (2%)] Loss: 1.355322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [20480/500434 (4%)] Loss: 1.331701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [30720/500434 (6%)] Loss: 1.293036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [40960/500434 (8%)] Loss: 1.225731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [51200/500434 (10%)] Loss: 1.310288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [61440/500434 (12%)] Loss: 1.370324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [71680/500434 (14%)] Loss: 1.307137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [81920/500434 (16%)] Loss: 1.465017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [92160/500434 (18%)] Loss: 1.509342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [102400/500434 (20%)] Loss: 1.433053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [112640/500434 (22%)] Loss: 1.344064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [122880/500434 (25%)] Loss: 1.315486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [133120/500434 (27%)] Loss: 1.539693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [143360/500434 (29%)] Loss: 1.308708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [153600/500434 (31%)] Loss: 1.474255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [163840/500434 (33%)] Loss: 1.311934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [174080/500434 (35%)] Loss: 1.440717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [184320/500434 (37%)] Loss: 1.457746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [194560/500434 (39%)] Loss: 1.301460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [204800/500434 (41%)] Loss: 1.465874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [215040/500434 (43%)] Loss: 1.371224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [225280/500434 (45%)] Loss: 1.321696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [235520/500434 (47%)] Loss: 1.393289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [245760/500434 (49%)] Loss: 1.418303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [256000/500434 (51%)] Loss: 1.282822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [266240/500434 (53%)] Loss: 1.396185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [276480/500434 (55%)] Loss: 1.359924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [286720/500434 (57%)] Loss: 1.339227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [296960/500434 (59%)] Loss: 1.468829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [307200/500434 (61%)] Loss: 1.313231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [317440/500434 (63%)] Loss: 1.312057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [327680/500434 (65%)] Loss: 1.436935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [337920/500434 (67%)] Loss: 1.234865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [348160/500434 (70%)] Loss: 1.339520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [358400/500434 (72%)] Loss: 1.335019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [368640/500434 (74%)] Loss: 1.326616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [378880/500434 (76%)] Loss: 1.372534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [389120/500434 (78%)] Loss: 1.598679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [399360/500434 (80%)] Loss: 1.498295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [409600/500434 (82%)] Loss: 1.372380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [419840/500434 (84%)] Loss: 1.453285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [430080/500434 (86%)] Loss: 1.337381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [440320/500434 (88%)] Loss: 1.266457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [450560/500434 (90%)] Loss: 1.408178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [460800/500434 (92%)] Loss: 1.339580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [471040/500434 (94%)] Loss: 1.341008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [481280/500434 (96%)] Loss: 1.376150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 116 [491520/500434 (98%)] Loss: 1.375700\u001b[0m\n",
      "\u001b[34mcurrent epoch: 117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [0/500434 (0%)] Loss: 1.335266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [10240/500434 (2%)] Loss: 1.306238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [20480/500434 (4%)] Loss: 1.316792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [30720/500434 (6%)] Loss: 1.320144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [40960/500434 (8%)] Loss: 1.357591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [51200/500434 (10%)] Loss: 1.368878\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [61440/500434 (12%)] Loss: 1.262044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [71680/500434 (14%)] Loss: 1.406478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [81920/500434 (16%)] Loss: 1.356511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [92160/500434 (18%)] Loss: 1.320166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [102400/500434 (20%)] Loss: 1.359258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [112640/500434 (22%)] Loss: 1.286621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [122880/500434 (25%)] Loss: 1.426974\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [133120/500434 (27%)] Loss: 1.405019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [143360/500434 (29%)] Loss: 1.323983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [153600/500434 (31%)] Loss: 1.306794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [163840/500434 (33%)] Loss: 1.362636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [174080/500434 (35%)] Loss: 1.294218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [184320/500434 (37%)] Loss: 1.372694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [194560/500434 (39%)] Loss: 1.345627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [204800/500434 (41%)] Loss: 1.354506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [215040/500434 (43%)] Loss: 1.409068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [225280/500434 (45%)] Loss: 1.459694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [235520/500434 (47%)] Loss: 1.441794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [245760/500434 (49%)] Loss: 1.400258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [256000/500434 (51%)] Loss: 1.341746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [266240/500434 (53%)] Loss: 1.388819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [276480/500434 (55%)] Loss: 1.423543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [286720/500434 (57%)] Loss: 1.452713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [296960/500434 (59%)] Loss: 1.303136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [307200/500434 (61%)] Loss: 1.374057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [317440/500434 (63%)] Loss: 1.335145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [327680/500434 (65%)] Loss: 1.321282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [337920/500434 (67%)] Loss: 1.320634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [348160/500434 (70%)] Loss: 1.356570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [358400/500434 (72%)] Loss: 1.402631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [368640/500434 (74%)] Loss: 1.351980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [378880/500434 (76%)] Loss: 1.382492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [389120/500434 (78%)] Loss: 1.501443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [399360/500434 (80%)] Loss: 1.422574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [409600/500434 (82%)] Loss: 1.282974\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [419840/500434 (84%)] Loss: 1.330950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [430080/500434 (86%)] Loss: 1.368754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [440320/500434 (88%)] Loss: 1.384782\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [450560/500434 (90%)] Loss: 1.299442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [460800/500434 (92%)] Loss: 1.374174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [471040/500434 (94%)] Loss: 1.245766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [481280/500434 (96%)] Loss: 1.245876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 117 [491520/500434 (98%)] Loss: 1.388947\u001b[0m\n",
      "\u001b[34mcurrent epoch: 118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [0/500434 (0%)] Loss: 1.284810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [10240/500434 (2%)] Loss: 1.377856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [20480/500434 (4%)] Loss: 1.366947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [30720/500434 (6%)] Loss: 1.320552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [40960/500434 (8%)] Loss: 1.430570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [51200/500434 (10%)] Loss: 1.393136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [61440/500434 (12%)] Loss: 1.413295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [71680/500434 (14%)] Loss: 1.382419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [81920/500434 (16%)] Loss: 1.360274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [92160/500434 (18%)] Loss: 1.274448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [102400/500434 (20%)] Loss: 1.398715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [112640/500434 (22%)] Loss: 1.273687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [122880/500434 (25%)] Loss: 1.327399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [133120/500434 (27%)] Loss: 1.285070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [143360/500434 (29%)] Loss: 1.417299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [153600/500434 (31%)] Loss: 1.314457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [163840/500434 (33%)] Loss: 1.420792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [174080/500434 (35%)] Loss: 1.335079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [184320/500434 (37%)] Loss: 1.299616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [194560/500434 (39%)] Loss: 1.241027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [204800/500434 (41%)] Loss: 1.272614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [215040/500434 (43%)] Loss: 1.325421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [225280/500434 (45%)] Loss: 1.302855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [235520/500434 (47%)] Loss: 1.248758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [245760/500434 (49%)] Loss: 1.272324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [256000/500434 (51%)] Loss: 1.300409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [266240/500434 (53%)] Loss: 1.181934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [276480/500434 (55%)] Loss: 1.311175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [286720/500434 (57%)] Loss: 1.445286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [296960/500434 (59%)] Loss: 1.254449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [307200/500434 (61%)] Loss: 1.357812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [317440/500434 (63%)] Loss: 1.354035\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [327680/500434 (65%)] Loss: 1.314430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [337920/500434 (67%)] Loss: 1.340091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [348160/500434 (70%)] Loss: 1.277243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [358400/500434 (72%)] Loss: 1.418803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [368640/500434 (74%)] Loss: 1.263843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [378880/500434 (76%)] Loss: 1.260719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [389120/500434 (78%)] Loss: 1.303039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [399360/500434 (80%)] Loss: 1.327708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [409600/500434 (82%)] Loss: 1.445711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [419840/500434 (84%)] Loss: 1.395770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [430080/500434 (86%)] Loss: 1.377023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [440320/500434 (88%)] Loss: 1.357371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [450560/500434 (90%)] Loss: 1.263766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [460800/500434 (92%)] Loss: 1.227701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [471040/500434 (94%)] Loss: 1.448573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [481280/500434 (96%)] Loss: 1.254967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 118 [491520/500434 (98%)] Loss: 1.392033\u001b[0m\n",
      "\u001b[34mcurrent epoch: 119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [0/500434 (0%)] Loss: 1.315382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [10240/500434 (2%)] Loss: 1.313694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [20480/500434 (4%)] Loss: 1.309899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [30720/500434 (6%)] Loss: 1.296347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [40960/500434 (8%)] Loss: 1.354806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [51200/500434 (10%)] Loss: 1.345742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [61440/500434 (12%)] Loss: 1.265488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [71680/500434 (14%)] Loss: 1.253953\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [81920/500434 (16%)] Loss: 1.288877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [92160/500434 (18%)] Loss: 1.302744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [102400/500434 (20%)] Loss: 1.345342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [112640/500434 (22%)] Loss: 1.328829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [122880/500434 (25%)] Loss: 1.326272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [133120/500434 (27%)] Loss: 1.210014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [143360/500434 (29%)] Loss: 1.249995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [153600/500434 (31%)] Loss: 1.295732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [163840/500434 (33%)] Loss: 1.324601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [174080/500434 (35%)] Loss: 1.230785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [184320/500434 (37%)] Loss: 1.383007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [194560/500434 (39%)] Loss: 1.316759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [204800/500434 (41%)] Loss: 1.508183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [215040/500434 (43%)] Loss: 1.290445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [225280/500434 (45%)] Loss: 1.343247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [235520/500434 (47%)] Loss: 1.300200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [245760/500434 (49%)] Loss: 1.353484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [256000/500434 (51%)] Loss: 1.366887\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [266240/500434 (53%)] Loss: 1.207732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [276480/500434 (55%)] Loss: 1.325049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [286720/500434 (57%)] Loss: 1.404722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [296960/500434 (59%)] Loss: 1.232911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [307200/500434 (61%)] Loss: 1.453762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [317440/500434 (63%)] Loss: 1.265698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [327680/500434 (65%)] Loss: 1.326019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [337920/500434 (67%)] Loss: 1.338102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [348160/500434 (70%)] Loss: 1.463210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [358400/500434 (72%)] Loss: 1.319225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [368640/500434 (74%)] Loss: 1.351511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [378880/500434 (76%)] Loss: 1.257566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [389120/500434 (78%)] Loss: 1.278778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [399360/500434 (80%)] Loss: 1.351009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [409600/500434 (82%)] Loss: 1.363403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [419840/500434 (84%)] Loss: 1.459864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [430080/500434 (86%)] Loss: 1.340599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [440320/500434 (88%)] Loss: 1.288843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [450560/500434 (90%)] Loss: 1.460544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [460800/500434 (92%)] Loss: 1.398914\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [471040/500434 (94%)] Loss: 1.347383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [481280/500434 (96%)] Loss: 1.357412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 119 [491520/500434 (98%)] Loss: 1.316999\u001b[0m\n",
      "\u001b[34mcurrent epoch: 120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [0/500434 (0%)] Loss: 1.277496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [10240/500434 (2%)] Loss: 1.236859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [20480/500434 (4%)] Loss: 1.343835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [30720/500434 (6%)] Loss: 1.313440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [40960/500434 (8%)] Loss: 1.429876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [51200/500434 (10%)] Loss: 1.366258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [61440/500434 (12%)] Loss: 1.388370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [71680/500434 (14%)] Loss: 1.362511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [81920/500434 (16%)] Loss: 1.387817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [92160/500434 (18%)] Loss: 1.341335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [102400/500434 (20%)] Loss: 1.343076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [112640/500434 (22%)] Loss: 1.199701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [122880/500434 (25%)] Loss: 1.390046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [133120/500434 (27%)] Loss: 1.272989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [143360/500434 (29%)] Loss: 1.275059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [153600/500434 (31%)] Loss: 1.278115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [163840/500434 (33%)] Loss: 1.348147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [174080/500434 (35%)] Loss: 1.509915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [184320/500434 (37%)] Loss: 1.326272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [194560/500434 (39%)] Loss: 1.375980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [204800/500434 (41%)] Loss: 1.371202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [215040/500434 (43%)] Loss: 1.291261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [225280/500434 (45%)] Loss: 1.269118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [235520/500434 (47%)] Loss: 1.377332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [245760/500434 (49%)] Loss: 1.420417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [256000/500434 (51%)] Loss: 1.288528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [266240/500434 (53%)] Loss: 1.337376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [276480/500434 (55%)] Loss: 1.393729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [286720/500434 (57%)] Loss: 1.429929\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [296960/500434 (59%)] Loss: 1.656707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [307200/500434 (61%)] Loss: 1.479321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [317440/500434 (63%)] Loss: 1.484774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [327680/500434 (65%)] Loss: 1.425516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [337920/500434 (67%)] Loss: 1.351159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [348160/500434 (70%)] Loss: 1.449849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [358400/500434 (72%)] Loss: 1.427637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [368640/500434 (74%)] Loss: 1.337274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [378880/500434 (76%)] Loss: 1.352066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [389120/500434 (78%)] Loss: 1.395778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [399360/500434 (80%)] Loss: 1.374272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [409600/500434 (82%)] Loss: 1.319897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [419840/500434 (84%)] Loss: 1.324100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [430080/500434 (86%)] Loss: 1.431250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [440320/500434 (88%)] Loss: 1.339559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [450560/500434 (90%)] Loss: 1.518206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [460800/500434 (92%)] Loss: 1.327785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [471040/500434 (94%)] Loss: 1.401798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [481280/500434 (96%)] Loss: 1.381661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 120 [491520/500434 (98%)] Loss: 1.278099\u001b[0m\n",
      "\u001b[34mcurrent epoch: 121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [0/500434 (0%)] Loss: 1.440655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [10240/500434 (2%)] Loss: 1.183318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [20480/500434 (4%)] Loss: 1.292986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [30720/500434 (6%)] Loss: 1.371159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [40960/500434 (8%)] Loss: 1.229096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [51200/500434 (10%)] Loss: 1.232167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [61440/500434 (12%)] Loss: 1.339691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [71680/500434 (14%)] Loss: 1.351117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [81920/500434 (16%)] Loss: 1.350433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [92160/500434 (18%)] Loss: 1.325449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [102400/500434 (20%)] Loss: 1.292613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [112640/500434 (22%)] Loss: 1.313610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [122880/500434 (25%)] Loss: 1.311040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [133120/500434 (27%)] Loss: 1.321472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [143360/500434 (29%)] Loss: 1.341637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [153600/500434 (31%)] Loss: 1.413001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [163840/500434 (33%)] Loss: 1.281954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [174080/500434 (35%)] Loss: 1.311790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [184320/500434 (37%)] Loss: 1.376473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [194560/500434 (39%)] Loss: 1.351907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [204800/500434 (41%)] Loss: 1.334927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [215040/500434 (43%)] Loss: 1.336303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [225280/500434 (45%)] Loss: 1.333722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [235520/500434 (47%)] Loss: 1.346024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [245760/500434 (49%)] Loss: 1.337500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [256000/500434 (51%)] Loss: 1.343286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [266240/500434 (53%)] Loss: 1.367396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [276480/500434 (55%)] Loss: 1.270157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [286720/500434 (57%)] Loss: 1.361022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [296960/500434 (59%)] Loss: 1.280807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [307200/500434 (61%)] Loss: 1.341707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [317440/500434 (63%)] Loss: 1.318385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [327680/500434 (65%)] Loss: 1.344085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [337920/500434 (67%)] Loss: 1.299234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [348160/500434 (70%)] Loss: 1.152593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [358400/500434 (72%)] Loss: 1.362240\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [368640/500434 (74%)] Loss: 1.244116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [378880/500434 (76%)] Loss: 1.336470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [389120/500434 (78%)] Loss: 1.396911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [399360/500434 (80%)] Loss: 1.400077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [409600/500434 (82%)] Loss: 1.292762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [419840/500434 (84%)] Loss: 1.300493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [430080/500434 (86%)] Loss: 1.318034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [440320/500434 (88%)] Loss: 1.335988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [450560/500434 (90%)] Loss: 1.332696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [460800/500434 (92%)] Loss: 1.317577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [471040/500434 (94%)] Loss: 1.348305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [481280/500434 (96%)] Loss: 1.300279\u001b[0m\n",
      "\u001b[34mTrain Epoch: 121 [491520/500434 (98%)] Loss: 1.388717\u001b[0m\n",
      "\u001b[34mcurrent epoch: 122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [0/500434 (0%)] Loss: 1.416241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [10240/500434 (2%)] Loss: 1.301947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [20480/500434 (4%)] Loss: 1.258031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [30720/500434 (6%)] Loss: 1.272333\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [40960/500434 (8%)] Loss: 1.284763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [51200/500434 (10%)] Loss: 1.275665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [61440/500434 (12%)] Loss: 1.312589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [71680/500434 (14%)] Loss: 1.386555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [81920/500434 (16%)] Loss: 1.347480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [92160/500434 (18%)] Loss: 1.320427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [102400/500434 (20%)] Loss: 1.401672\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [112640/500434 (22%)] Loss: 1.326289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [122880/500434 (25%)] Loss: 1.325830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [133120/500434 (27%)] Loss: 1.333423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [143360/500434 (29%)] Loss: 1.314700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [153600/500434 (31%)] Loss: 1.351531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [163840/500434 (33%)] Loss: 1.271536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [174080/500434 (35%)] Loss: 1.412031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [184320/500434 (37%)] Loss: 1.340224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [194560/500434 (39%)] Loss: 1.251695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [204800/500434 (41%)] Loss: 1.349498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [215040/500434 (43%)] Loss: 1.321825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [225280/500434 (45%)] Loss: 1.381704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [235520/500434 (47%)] Loss: 1.243369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [245760/500434 (49%)] Loss: 1.300441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [256000/500434 (51%)] Loss: 1.144313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [266240/500434 (53%)] Loss: 1.325214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [276480/500434 (55%)] Loss: 1.366593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [286720/500434 (57%)] Loss: 1.334217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [296960/500434 (59%)] Loss: 1.258031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [307200/500434 (61%)] Loss: 1.304565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [317440/500434 (63%)] Loss: 1.232231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [327680/500434 (65%)] Loss: 1.225850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [337920/500434 (67%)] Loss: 1.363230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [348160/500434 (70%)] Loss: 1.302449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [358400/500434 (72%)] Loss: 1.406314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [368640/500434 (74%)] Loss: 1.406152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [378880/500434 (76%)] Loss: 1.437225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [389120/500434 (78%)] Loss: 1.280997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [399360/500434 (80%)] Loss: 1.383927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [409600/500434 (82%)] Loss: 1.268209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [419840/500434 (84%)] Loss: 1.450694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [430080/500434 (86%)] Loss: 1.293900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [440320/500434 (88%)] Loss: 1.385790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [450560/500434 (90%)] Loss: 1.286366\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [460800/500434 (92%)] Loss: 1.331181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [471040/500434 (94%)] Loss: 1.248873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [481280/500434 (96%)] Loss: 1.373374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 122 [491520/500434 (98%)] Loss: 1.221545\u001b[0m\n",
      "\u001b[34mcurrent epoch: 123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [0/500434 (0%)] Loss: 1.266495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [10240/500434 (2%)] Loss: 1.337223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [20480/500434 (4%)] Loss: 1.424339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [30720/500434 (6%)] Loss: 1.273913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [40960/500434 (8%)] Loss: 1.321148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [51200/500434 (10%)] Loss: 1.285625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [61440/500434 (12%)] Loss: 1.380041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [71680/500434 (14%)] Loss: 1.285091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [81920/500434 (16%)] Loss: 1.330348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [92160/500434 (18%)] Loss: 1.407769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [102400/500434 (20%)] Loss: 1.323462\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [112640/500434 (22%)] Loss: 1.304793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [122880/500434 (25%)] Loss: 1.401683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [133120/500434 (27%)] Loss: 1.351589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [143360/500434 (29%)] Loss: 1.364141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [153600/500434 (31%)] Loss: 1.303170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [163840/500434 (33%)] Loss: 1.238657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [174080/500434 (35%)] Loss: 1.364298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [184320/500434 (37%)] Loss: 1.335242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [194560/500434 (39%)] Loss: 1.260998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [204800/500434 (41%)] Loss: 1.424705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [215040/500434 (43%)] Loss: 1.432360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [225280/500434 (45%)] Loss: 1.341248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [235520/500434 (47%)] Loss: 1.334626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [245760/500434 (49%)] Loss: 1.380209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [256000/500434 (51%)] Loss: 1.355269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [266240/500434 (53%)] Loss: 1.406829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [276480/500434 (55%)] Loss: 1.348451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [286720/500434 (57%)] Loss: 1.385494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [296960/500434 (59%)] Loss: 1.335214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [307200/500434 (61%)] Loss: 1.342202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [317440/500434 (63%)] Loss: 1.302119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [327680/500434 (65%)] Loss: 1.256703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [337920/500434 (67%)] Loss: 1.359628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [348160/500434 (70%)] Loss: 1.245613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [358400/500434 (72%)] Loss: 1.259051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [368640/500434 (74%)] Loss: 1.309334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [378880/500434 (76%)] Loss: 1.466215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [389120/500434 (78%)] Loss: 1.323589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [399360/500434 (80%)] Loss: 1.412619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [409600/500434 (82%)] Loss: 1.400834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [419840/500434 (84%)] Loss: 1.285021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [430080/500434 (86%)] Loss: 1.276918\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [440320/500434 (88%)] Loss: 1.231223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [450560/500434 (90%)] Loss: 1.473495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [460800/500434 (92%)] Loss: 1.304718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [471040/500434 (94%)] Loss: 1.328604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [481280/500434 (96%)] Loss: 1.173972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 123 [491520/500434 (98%)] Loss: 1.310993\u001b[0m\n",
      "\u001b[34mcurrent epoch: 124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [0/500434 (0%)] Loss: 1.246113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [10240/500434 (2%)] Loss: 1.193937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [20480/500434 (4%)] Loss: 1.287325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [30720/500434 (6%)] Loss: 1.113507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [40960/500434 (8%)] Loss: 1.264637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [51200/500434 (10%)] Loss: 1.234439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [61440/500434 (12%)] Loss: 1.700248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [71680/500434 (14%)] Loss: 1.334004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [81920/500434 (16%)] Loss: 1.295246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [92160/500434 (18%)] Loss: 1.224957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [102400/500434 (20%)] Loss: 1.276479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [112640/500434 (22%)] Loss: 1.379563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [122880/500434 (25%)] Loss: 1.234000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [133120/500434 (27%)] Loss: 1.154211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [143360/500434 (29%)] Loss: 1.322734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [153600/500434 (31%)] Loss: 1.185383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [163840/500434 (33%)] Loss: 1.299549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [174080/500434 (35%)] Loss: 1.332886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [184320/500434 (37%)] Loss: 1.250632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [194560/500434 (39%)] Loss: 1.407112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [204800/500434 (41%)] Loss: 1.382462\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [215040/500434 (43%)] Loss: 1.367646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [225280/500434 (45%)] Loss: 1.285469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [235520/500434 (47%)] Loss: 1.270125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [245760/500434 (49%)] Loss: 1.316342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [256000/500434 (51%)] Loss: 1.286770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [266240/500434 (53%)] Loss: 1.176800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [276480/500434 (55%)] Loss: 1.215126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [286720/500434 (57%)] Loss: 1.245836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [296960/500434 (59%)] Loss: 1.417573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [307200/500434 (61%)] Loss: 1.423390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [317440/500434 (63%)] Loss: 1.407997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [327680/500434 (65%)] Loss: 1.454398\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [337920/500434 (67%)] Loss: 1.354132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [348160/500434 (70%)] Loss: 1.407396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [358400/500434 (72%)] Loss: 1.243727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [368640/500434 (74%)] Loss: 1.333379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [378880/500434 (76%)] Loss: 1.311136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [389120/500434 (78%)] Loss: 1.305656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [399360/500434 (80%)] Loss: 1.289980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [409600/500434 (82%)] Loss: 1.172821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [419840/500434 (84%)] Loss: 1.430819\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [430080/500434 (86%)] Loss: 1.193358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [440320/500434 (88%)] Loss: 1.363082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [450560/500434 (90%)] Loss: 1.257005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [460800/500434 (92%)] Loss: 1.337325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [471040/500434 (94%)] Loss: 1.299228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [481280/500434 (96%)] Loss: 1.372040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 124 [491520/500434 (98%)] Loss: 1.339544\u001b[0m\n",
      "\u001b[34mcurrent epoch: 125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [0/500434 (0%)] Loss: 1.298039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [10240/500434 (2%)] Loss: 1.301226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [20480/500434 (4%)] Loss: 1.143172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [30720/500434 (6%)] Loss: 1.251304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [40960/500434 (8%)] Loss: 1.317255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [51200/500434 (10%)] Loss: 1.344709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [61440/500434 (12%)] Loss: 1.268532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [71680/500434 (14%)] Loss: 1.256966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [81920/500434 (16%)] Loss: 1.334640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [92160/500434 (18%)] Loss: 1.311103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [102400/500434 (20%)] Loss: 1.292522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [112640/500434 (22%)] Loss: 1.223186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [122880/500434 (25%)] Loss: 1.389813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [133120/500434 (27%)] Loss: 1.345757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [143360/500434 (29%)] Loss: 1.309957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [153600/500434 (31%)] Loss: 1.211611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [163840/500434 (33%)] Loss: 1.273085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [174080/500434 (35%)] Loss: 1.341308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [184320/500434 (37%)] Loss: 1.293416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [194560/500434 (39%)] Loss: 1.221633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [204800/500434 (41%)] Loss: 1.273017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [215040/500434 (43%)] Loss: 1.403379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [225280/500434 (45%)] Loss: 1.203437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [235520/500434 (47%)] Loss: 1.219075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [245760/500434 (49%)] Loss: 1.354092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [256000/500434 (51%)] Loss: 1.283118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [266240/500434 (53%)] Loss: 1.355935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [276480/500434 (55%)] Loss: 1.265975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [286720/500434 (57%)] Loss: 1.225827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [296960/500434 (59%)] Loss: 1.317116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [307200/500434 (61%)] Loss: 1.191456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [317440/500434 (63%)] Loss: 1.339808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [327680/500434 (65%)] Loss: 1.371549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [337920/500434 (67%)] Loss: 1.177555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [348160/500434 (70%)] Loss: 1.372687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [358400/500434 (72%)] Loss: 1.219007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [368640/500434 (74%)] Loss: 1.375221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [378880/500434 (76%)] Loss: 1.284179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [389120/500434 (78%)] Loss: 1.227502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [399360/500434 (80%)] Loss: 1.292576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [409600/500434 (82%)] Loss: 1.316965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [419840/500434 (84%)] Loss: 1.204111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [430080/500434 (86%)] Loss: 1.951513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [440320/500434 (88%)] Loss: 1.229957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [450560/500434 (90%)] Loss: 1.331642\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [460800/500434 (92%)] Loss: 1.291155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [471040/500434 (94%)] Loss: 1.339693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [481280/500434 (96%)] Loss: 1.363564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 125 [491520/500434 (98%)] Loss: 1.240795\u001b[0m\n",
      "\u001b[34mcurrent epoch: 126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [0/500434 (0%)] Loss: 1.262616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [10240/500434 (2%)] Loss: 1.334193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [20480/500434 (4%)] Loss: 1.292138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [30720/500434 (6%)] Loss: 1.213076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [40960/500434 (8%)] Loss: 1.307668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [51200/500434 (10%)] Loss: 1.226136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [61440/500434 (12%)] Loss: 1.313616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [71680/500434 (14%)] Loss: 1.244503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [81920/500434 (16%)] Loss: 1.264619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [92160/500434 (18%)] Loss: 1.263864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [102400/500434 (20%)] Loss: 1.197572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [112640/500434 (22%)] Loss: 1.319788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [122880/500434 (25%)] Loss: 1.302257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [133120/500434 (27%)] Loss: 1.365457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [143360/500434 (29%)] Loss: 1.244071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [153600/500434 (31%)] Loss: 1.291711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [163840/500434 (33%)] Loss: 1.269456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [174080/500434 (35%)] Loss: 1.217928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [184320/500434 (37%)] Loss: 1.424026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [194560/500434 (39%)] Loss: 1.472091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [204800/500434 (41%)] Loss: 1.691809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [215040/500434 (43%)] Loss: 1.446912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [225280/500434 (45%)] Loss: 1.232942\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [235520/500434 (47%)] Loss: 1.387560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [245760/500434 (49%)] Loss: 1.304173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [256000/500434 (51%)] Loss: 1.384872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [266240/500434 (53%)] Loss: 1.285706\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [276480/500434 (55%)] Loss: 1.313078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [286720/500434 (57%)] Loss: 1.292078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [296960/500434 (59%)] Loss: 1.433943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [307200/500434 (61%)] Loss: 1.243160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [317440/500434 (63%)] Loss: 1.289187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [327680/500434 (65%)] Loss: 1.419549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [337920/500434 (67%)] Loss: 1.246190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [348160/500434 (70%)] Loss: 1.391088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [358400/500434 (72%)] Loss: 1.227973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [368640/500434 (74%)] Loss: 1.400069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [378880/500434 (76%)] Loss: 1.263698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [389120/500434 (78%)] Loss: 1.326476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [399360/500434 (80%)] Loss: 1.300216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [409600/500434 (82%)] Loss: 1.285183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [419840/500434 (84%)] Loss: 1.313185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [430080/500434 (86%)] Loss: 1.272223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [440320/500434 (88%)] Loss: 1.395893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [450560/500434 (90%)] Loss: 1.299033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [460800/500434 (92%)] Loss: 1.248900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [471040/500434 (94%)] Loss: 1.316292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [481280/500434 (96%)] Loss: 1.245896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 126 [491520/500434 (98%)] Loss: 1.319149\u001b[0m\n",
      "\u001b[34mcurrent epoch: 127\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [0/500434 (0%)] Loss: 1.218047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [10240/500434 (2%)] Loss: 1.243702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [20480/500434 (4%)] Loss: 1.275711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [30720/500434 (6%)] Loss: 1.318390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [40960/500434 (8%)] Loss: 1.315382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [51200/500434 (10%)] Loss: 1.393541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [61440/500434 (12%)] Loss: 1.361794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [71680/500434 (14%)] Loss: 1.257445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [81920/500434 (16%)] Loss: 1.236647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [92160/500434 (18%)] Loss: 1.327073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [102400/500434 (20%)] Loss: 1.221598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [112640/500434 (22%)] Loss: 1.242854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [122880/500434 (25%)] Loss: 1.164556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [133120/500434 (27%)] Loss: 1.268063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [143360/500434 (29%)] Loss: 1.201378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [153600/500434 (31%)] Loss: 1.286891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [163840/500434 (33%)] Loss: 1.363553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [174080/500434 (35%)] Loss: 1.298394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [184320/500434 (37%)] Loss: 1.297118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [194560/500434 (39%)] Loss: 1.381906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [204800/500434 (41%)] Loss: 1.304294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [215040/500434 (43%)] Loss: 1.549260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [225280/500434 (45%)] Loss: 1.262800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [235520/500434 (47%)] Loss: 1.380560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [245760/500434 (49%)] Loss: 1.355151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [256000/500434 (51%)] Loss: 1.315151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [266240/500434 (53%)] Loss: 1.308526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [276480/500434 (55%)] Loss: 1.244678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [286720/500434 (57%)] Loss: 1.308150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [296960/500434 (59%)] Loss: 1.383793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [307200/500434 (61%)] Loss: 1.349767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [317440/500434 (63%)] Loss: 1.270809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [327680/500434 (65%)] Loss: 1.308640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [337920/500434 (67%)] Loss: 1.333409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [348160/500434 (70%)] Loss: 1.334177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [358400/500434 (72%)] Loss: 1.285079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [368640/500434 (74%)] Loss: 1.321585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [378880/500434 (76%)] Loss: 1.315624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [389120/500434 (78%)] Loss: 1.163413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [399360/500434 (80%)] Loss: 1.366193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [409600/500434 (82%)] Loss: 1.261738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [419840/500434 (84%)] Loss: 1.310402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [430080/500434 (86%)] Loss: 1.405714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [440320/500434 (88%)] Loss: 1.261603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [450560/500434 (90%)] Loss: 1.384281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [460800/500434 (92%)] Loss: 1.361418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [471040/500434 (94%)] Loss: 1.273697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [481280/500434 (96%)] Loss: 1.344540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 127 [491520/500434 (98%)] Loss: 1.369732\u001b[0m\n",
      "\u001b[34mcurrent epoch: 128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [0/500434 (0%)] Loss: 1.229535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [10240/500434 (2%)] Loss: 1.361761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [20480/500434 (4%)] Loss: 1.419019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [30720/500434 (6%)] Loss: 1.291922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [40960/500434 (8%)] Loss: 1.241141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [51200/500434 (10%)] Loss: 1.201264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [61440/500434 (12%)] Loss: 1.270565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [71680/500434 (14%)] Loss: 1.271345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [81920/500434 (16%)] Loss: 1.279078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [92160/500434 (18%)] Loss: 1.318814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [102400/500434 (20%)] Loss: 1.358938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [112640/500434 (22%)] Loss: 1.361115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [122880/500434 (25%)] Loss: 1.251820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [133120/500434 (27%)] Loss: 1.330086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [143360/500434 (29%)] Loss: 1.372543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [153600/500434 (31%)] Loss: 1.313395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [163840/500434 (33%)] Loss: 1.309474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [174080/500434 (35%)] Loss: 1.290520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [184320/500434 (37%)] Loss: 1.290356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [194560/500434 (39%)] Loss: 1.347751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [204800/500434 (41%)] Loss: 1.248758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [215040/500434 (43%)] Loss: 1.301777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [225280/500434 (45%)] Loss: 1.258981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [235520/500434 (47%)] Loss: 1.497593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [245760/500434 (49%)] Loss: 1.353605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [256000/500434 (51%)] Loss: 1.281064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [266240/500434 (53%)] Loss: 1.329781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [276480/500434 (55%)] Loss: 1.262664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [286720/500434 (57%)] Loss: 1.329683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [296960/500434 (59%)] Loss: 1.461730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [307200/500434 (61%)] Loss: 1.251839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [317440/500434 (63%)] Loss: 1.270080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [327680/500434 (65%)] Loss: 1.315516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [337920/500434 (67%)] Loss: 1.220435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [348160/500434 (70%)] Loss: 1.321260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [358400/500434 (72%)] Loss: 1.199988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [368640/500434 (74%)] Loss: 1.280793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [378880/500434 (76%)] Loss: 1.557536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [389120/500434 (78%)] Loss: 1.407791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [399360/500434 (80%)] Loss: 1.694155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [409600/500434 (82%)] Loss: 1.658662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [419840/500434 (84%)] Loss: 1.512822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [430080/500434 (86%)] Loss: 1.355857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [440320/500434 (88%)] Loss: 1.502445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [450560/500434 (90%)] Loss: 1.215616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [460800/500434 (92%)] Loss: 1.281660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [471040/500434 (94%)] Loss: 1.306889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [481280/500434 (96%)] Loss: 1.245753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 128 [491520/500434 (98%)] Loss: 1.335203\u001b[0m\n",
      "\u001b[34mcurrent epoch: 129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [0/500434 (0%)] Loss: 1.256147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [10240/500434 (2%)] Loss: 1.283068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [20480/500434 (4%)] Loss: 1.231129\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [30720/500434 (6%)] Loss: 1.261505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [40960/500434 (8%)] Loss: 1.332317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [51200/500434 (10%)] Loss: 1.306425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [61440/500434 (12%)] Loss: 1.431741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [71680/500434 (14%)] Loss: 1.382971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [81920/500434 (16%)] Loss: 1.261213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [92160/500434 (18%)] Loss: 1.273841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [102400/500434 (20%)] Loss: 1.312194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [112640/500434 (22%)] Loss: 1.239588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [122880/500434 (25%)] Loss: 1.312516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [133120/500434 (27%)] Loss: 1.359247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [143360/500434 (29%)] Loss: 1.318723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [153600/500434 (31%)] Loss: 1.256686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [163840/500434 (33%)] Loss: 1.382778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [174080/500434 (35%)] Loss: 1.402104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [184320/500434 (37%)] Loss: 1.442981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [194560/500434 (39%)] Loss: 1.294368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [204800/500434 (41%)] Loss: 1.279284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [215040/500434 (43%)] Loss: 1.402287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [225280/500434 (45%)] Loss: 1.296296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [235520/500434 (47%)] Loss: 1.330532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [245760/500434 (49%)] Loss: 1.265718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [256000/500434 (51%)] Loss: 1.406261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [266240/500434 (53%)] Loss: 1.242806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [276480/500434 (55%)] Loss: 1.391770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [286720/500434 (57%)] Loss: 1.303478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [296960/500434 (59%)] Loss: 1.496829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [307200/500434 (61%)] Loss: 1.219397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [317440/500434 (63%)] Loss: 1.316558\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [327680/500434 (65%)] Loss: 1.193917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [337920/500434 (67%)] Loss: 1.194541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [348160/500434 (70%)] Loss: 1.339564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [358400/500434 (72%)] Loss: 1.211873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [368640/500434 (74%)] Loss: 1.211906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [378880/500434 (76%)] Loss: 1.288674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [389120/500434 (78%)] Loss: 1.292812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [399360/500434 (80%)] Loss: 1.285243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [409600/500434 (82%)] Loss: 1.324735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [419840/500434 (84%)] Loss: 1.299297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [430080/500434 (86%)] Loss: 1.267105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [440320/500434 (88%)] Loss: 1.334993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [450560/500434 (90%)] Loss: 1.360641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [460800/500434 (92%)] Loss: 1.349562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [471040/500434 (94%)] Loss: 1.299365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [481280/500434 (96%)] Loss: 1.291777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 129 [491520/500434 (98%)] Loss: 1.223112\u001b[0m\n",
      "\u001b[34mcurrent epoch: 130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [0/500434 (0%)] Loss: 1.399183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [10240/500434 (2%)] Loss: 1.218802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [20480/500434 (4%)] Loss: 1.393075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [30720/500434 (6%)] Loss: 1.385391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [40960/500434 (8%)] Loss: 1.316908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [51200/500434 (10%)] Loss: 1.276987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [61440/500434 (12%)] Loss: 1.335629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [71680/500434 (14%)] Loss: 1.288984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [81920/500434 (16%)] Loss: 1.259773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [92160/500434 (18%)] Loss: 1.340436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [102400/500434 (20%)] Loss: 1.305294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [112640/500434 (22%)] Loss: 1.214537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [122880/500434 (25%)] Loss: 1.359842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [133120/500434 (27%)] Loss: 1.256310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [143360/500434 (29%)] Loss: 1.177617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [153600/500434 (31%)] Loss: 1.229299\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [163840/500434 (33%)] Loss: 1.223510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [174080/500434 (35%)] Loss: 1.392815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [184320/500434 (37%)] Loss: 1.310229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [194560/500434 (39%)] Loss: 1.301072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [204800/500434 (41%)] Loss: 1.271143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [215040/500434 (43%)] Loss: 1.354277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [225280/500434 (45%)] Loss: 1.366197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [235520/500434 (47%)] Loss: 1.306111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [245760/500434 (49%)] Loss: 1.257921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [256000/500434 (51%)] Loss: 1.272182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [266240/500434 (53%)] Loss: 1.325826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [276480/500434 (55%)] Loss: 1.213315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [286720/500434 (57%)] Loss: 1.293521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [296960/500434 (59%)] Loss: 1.267044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [307200/500434 (61%)] Loss: 1.338054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [317440/500434 (63%)] Loss: 1.327814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [327680/500434 (65%)] Loss: 1.198061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [337920/500434 (67%)] Loss: 1.248489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [348160/500434 (70%)] Loss: 1.302238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [358400/500434 (72%)] Loss: 1.259749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [368640/500434 (74%)] Loss: 1.297820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [378880/500434 (76%)] Loss: 1.299501\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [389120/500434 (78%)] Loss: 1.290454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [399360/500434 (80%)] Loss: 1.202445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [409600/500434 (82%)] Loss: 1.180683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [419840/500434 (84%)] Loss: 1.256250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [430080/500434 (86%)] Loss: 1.292957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [440320/500434 (88%)] Loss: 1.246933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [450560/500434 (90%)] Loss: 1.227677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [460800/500434 (92%)] Loss: 1.349725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [471040/500434 (94%)] Loss: 1.264656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [481280/500434 (96%)] Loss: 1.244828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 130 [491520/500434 (98%)] Loss: 1.218170\u001b[0m\n",
      "\u001b[34mcurrent epoch: 131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [0/500434 (0%)] Loss: 1.278499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [10240/500434 (2%)] Loss: 1.171323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [20480/500434 (4%)] Loss: 1.264653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [30720/500434 (6%)] Loss: 1.351011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [40960/500434 (8%)] Loss: 1.354798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [51200/500434 (10%)] Loss: 1.246941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [61440/500434 (12%)] Loss: 1.330450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [71680/500434 (14%)] Loss: 1.203606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [81920/500434 (16%)] Loss: 1.402637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [92160/500434 (18%)] Loss: 1.226418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [102400/500434 (20%)] Loss: 1.387907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [112640/500434 (22%)] Loss: 1.374041\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [122880/500434 (25%)] Loss: 1.229523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [133120/500434 (27%)] Loss: 1.282662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [143360/500434 (29%)] Loss: 1.269500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [153600/500434 (31%)] Loss: 1.296826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [163840/500434 (33%)] Loss: 1.140702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [174080/500434 (35%)] Loss: 1.224283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [184320/500434 (37%)] Loss: 1.424873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [194560/500434 (39%)] Loss: 1.293886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [204800/500434 (41%)] Loss: 1.323785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [215040/500434 (43%)] Loss: 1.256653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [225280/500434 (45%)] Loss: 1.309049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [235520/500434 (47%)] Loss: 1.272656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [245760/500434 (49%)] Loss: 1.313455\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [256000/500434 (51%)] Loss: 1.340585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [266240/500434 (53%)] Loss: 1.140886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [276480/500434 (55%)] Loss: 1.285327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [286720/500434 (57%)] Loss: 1.349986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [296960/500434 (59%)] Loss: 1.365733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [307200/500434 (61%)] Loss: 1.446377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [317440/500434 (63%)] Loss: 1.313221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [327680/500434 (65%)] Loss: 1.403088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [337920/500434 (67%)] Loss: 1.264506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [348160/500434 (70%)] Loss: 1.445933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [358400/500434 (72%)] Loss: 1.511392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [368640/500434 (74%)] Loss: 1.318257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [378880/500434 (76%)] Loss: 1.298753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [389120/500434 (78%)] Loss: 1.339402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [399360/500434 (80%)] Loss: 1.265874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [409600/500434 (82%)] Loss: 1.404686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [419840/500434 (84%)] Loss: 1.446005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [430080/500434 (86%)] Loss: 1.293006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [440320/500434 (88%)] Loss: 1.338106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [450560/500434 (90%)] Loss: 1.291572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [460800/500434 (92%)] Loss: 1.354144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [471040/500434 (94%)] Loss: 1.258742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [481280/500434 (96%)] Loss: 1.304223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 131 [491520/500434 (98%)] Loss: 1.198741\u001b[0m\n",
      "\u001b[34mcurrent epoch: 132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [0/500434 (0%)] Loss: 1.213590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [10240/500434 (2%)] Loss: 1.333821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [20480/500434 (4%)] Loss: 1.343817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [30720/500434 (6%)] Loss: 1.280711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [40960/500434 (8%)] Loss: 1.353747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [51200/500434 (10%)] Loss: 1.363700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [61440/500434 (12%)] Loss: 1.301057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [71680/500434 (14%)] Loss: 1.275126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [81920/500434 (16%)] Loss: 1.242413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [92160/500434 (18%)] Loss: 1.279545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [102400/500434 (20%)] Loss: 1.322017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [112640/500434 (22%)] Loss: 1.253745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [122880/500434 (25%)] Loss: 1.278652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [133120/500434 (27%)] Loss: 1.269429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [143360/500434 (29%)] Loss: 1.265133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [153600/500434 (31%)] Loss: 1.166970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [163840/500434 (33%)] Loss: 1.348658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [174080/500434 (35%)] Loss: 1.320021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [184320/500434 (37%)] Loss: 1.190613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [194560/500434 (39%)] Loss: 1.373259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [204800/500434 (41%)] Loss: 1.459686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [215040/500434 (43%)] Loss: 1.376227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [225280/500434 (45%)] Loss: 1.326826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [235520/500434 (47%)] Loss: 1.266826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [245760/500434 (49%)] Loss: 1.304404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [256000/500434 (51%)] Loss: 1.332242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [266240/500434 (53%)] Loss: 1.301777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [276480/500434 (55%)] Loss: 1.117075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [286720/500434 (57%)] Loss: 1.273779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [296960/500434 (59%)] Loss: 1.408772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [307200/500434 (61%)] Loss: 1.168663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [317440/500434 (63%)] Loss: 1.294416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [327680/500434 (65%)] Loss: 1.247859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [337920/500434 (67%)] Loss: 1.230138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [348160/500434 (70%)] Loss: 1.303547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [358400/500434 (72%)] Loss: 1.236484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [368640/500434 (74%)] Loss: 1.400311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [378880/500434 (76%)] Loss: 1.372621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [389120/500434 (78%)] Loss: 1.328935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [399360/500434 (80%)] Loss: 1.264187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [409600/500434 (82%)] Loss: 1.263566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [419840/500434 (84%)] Loss: 1.349410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [430080/500434 (86%)] Loss: 1.310667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [440320/500434 (88%)] Loss: 1.303773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [450560/500434 (90%)] Loss: 1.427685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [460800/500434 (92%)] Loss: 1.291869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [471040/500434 (94%)] Loss: 1.361610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [481280/500434 (96%)] Loss: 1.377836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 132 [491520/500434 (98%)] Loss: 1.248193\u001b[0m\n",
      "\u001b[34mcurrent epoch: 133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [0/500434 (0%)] Loss: 1.317049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [10240/500434 (2%)] Loss: 1.143087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [20480/500434 (4%)] Loss: 1.262192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [30720/500434 (6%)] Loss: 1.312984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [40960/500434 (8%)] Loss: 1.249172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [51200/500434 (10%)] Loss: 1.365175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [61440/500434 (12%)] Loss: 1.440034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [71680/500434 (14%)] Loss: 1.262135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [81920/500434 (16%)] Loss: 1.288239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [92160/500434 (18%)] Loss: 1.263188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [102400/500434 (20%)] Loss: 1.316808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [112640/500434 (22%)] Loss: 1.194568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [122880/500434 (25%)] Loss: 1.331853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [133120/500434 (27%)] Loss: 1.300260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [143360/500434 (29%)] Loss: 1.239694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [153600/500434 (31%)] Loss: 1.231778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [163840/500434 (33%)] Loss: 1.217358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [174080/500434 (35%)] Loss: 1.226049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [184320/500434 (37%)] Loss: 1.262904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [194560/500434 (39%)] Loss: 1.181566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [204800/500434 (41%)] Loss: 1.288111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [215040/500434 (43%)] Loss: 1.330516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [225280/500434 (45%)] Loss: 1.205317\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [235520/500434 (47%)] Loss: 1.198743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [245760/500434 (49%)] Loss: 1.322936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [256000/500434 (51%)] Loss: 1.312150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [266240/500434 (53%)] Loss: 1.370828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [276480/500434 (55%)] Loss: 1.300290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [286720/500434 (57%)] Loss: 1.263604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [296960/500434 (59%)] Loss: 1.438303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [307200/500434 (61%)] Loss: 1.309250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [317440/500434 (63%)] Loss: 1.259707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [327680/500434 (65%)] Loss: 1.317802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [337920/500434 (67%)] Loss: 1.223182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [348160/500434 (70%)] Loss: 1.276938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [358400/500434 (72%)] Loss: 1.267543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [368640/500434 (74%)] Loss: 1.282873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [378880/500434 (76%)] Loss: 1.339209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [389120/500434 (78%)] Loss: 1.348456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [399360/500434 (80%)] Loss: 1.285676\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [409600/500434 (82%)] Loss: 1.295103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [419840/500434 (84%)] Loss: 1.288569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [430080/500434 (86%)] Loss: 1.275160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [440320/500434 (88%)] Loss: 1.248806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [450560/500434 (90%)] Loss: 1.251112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [460800/500434 (92%)] Loss: 1.262187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [471040/500434 (94%)] Loss: 1.367362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [481280/500434 (96%)] Loss: 1.383712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 133 [491520/500434 (98%)] Loss: 1.282975\u001b[0m\n",
      "\u001b[34mcurrent epoch: 134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [0/500434 (0%)] Loss: 1.294271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [10240/500434 (2%)] Loss: 1.299742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [20480/500434 (4%)] Loss: 1.321172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [30720/500434 (6%)] Loss: 1.289055\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [40960/500434 (8%)] Loss: 1.279845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [51200/500434 (10%)] Loss: 1.307763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [61440/500434 (12%)] Loss: 1.307681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [71680/500434 (14%)] Loss: 1.260940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [81920/500434 (16%)] Loss: 1.249560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [92160/500434 (18%)] Loss: 1.310695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [102400/500434 (20%)] Loss: 1.378730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [112640/500434 (22%)] Loss: 1.236390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [122880/500434 (25%)] Loss: 1.193447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [133120/500434 (27%)] Loss: 1.204426\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [143360/500434 (29%)] Loss: 1.322922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [153600/500434 (31%)] Loss: 1.345705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [163840/500434 (33%)] Loss: 1.237882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [174080/500434 (35%)] Loss: 1.297945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [184320/500434 (37%)] Loss: 1.265344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [194560/500434 (39%)] Loss: 1.247626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [204800/500434 (41%)] Loss: 1.236634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [215040/500434 (43%)] Loss: 1.246937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [225280/500434 (45%)] Loss: 1.179361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [235520/500434 (47%)] Loss: 1.428583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [245760/500434 (49%)] Loss: 1.344399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [256000/500434 (51%)] Loss: 1.318741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [266240/500434 (53%)] Loss: 1.303132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [276480/500434 (55%)] Loss: 1.144080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [286720/500434 (57%)] Loss: 1.308001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [296960/500434 (59%)] Loss: 1.402731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [307200/500434 (61%)] Loss: 1.323475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [317440/500434 (63%)] Loss: 1.286155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [327680/500434 (65%)] Loss: 1.370476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [337920/500434 (67%)] Loss: 1.245331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [348160/500434 (70%)] Loss: 1.285908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [358400/500434 (72%)] Loss: 1.289344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [368640/500434 (74%)] Loss: 1.243831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [378880/500434 (76%)] Loss: 1.288006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [389120/500434 (78%)] Loss: 1.381151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [399360/500434 (80%)] Loss: 1.293987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [409600/500434 (82%)] Loss: 1.338780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [419840/500434 (84%)] Loss: 1.231653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [430080/500434 (86%)] Loss: 1.245143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [440320/500434 (88%)] Loss: 1.290178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [450560/500434 (90%)] Loss: 1.313902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [460800/500434 (92%)] Loss: 1.347557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [471040/500434 (94%)] Loss: 1.351351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [481280/500434 (96%)] Loss: 1.254988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 134 [491520/500434 (98%)] Loss: 1.289010\u001b[0m\n",
      "\u001b[34mcurrent epoch: 135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [0/500434 (0%)] Loss: 1.302721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [10240/500434 (2%)] Loss: 1.322703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [20480/500434 (4%)] Loss: 1.477391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [30720/500434 (6%)] Loss: 1.293726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [40960/500434 (8%)] Loss: 1.201411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [51200/500434 (10%)] Loss: 1.403664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [61440/500434 (12%)] Loss: 1.272571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [71680/500434 (14%)] Loss: 1.324652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [81920/500434 (16%)] Loss: 1.247686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [92160/500434 (18%)] Loss: 1.215379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [102400/500434 (20%)] Loss: 1.234093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [112640/500434 (22%)] Loss: 1.352478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [122880/500434 (25%)] Loss: 1.242177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [133120/500434 (27%)] Loss: 1.232198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [143360/500434 (29%)] Loss: 1.264428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [153600/500434 (31%)] Loss: 1.367141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [163840/500434 (33%)] Loss: 1.475496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [174080/500434 (35%)] Loss: 1.318045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [184320/500434 (37%)] Loss: 1.347799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [194560/500434 (39%)] Loss: 1.274286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [204800/500434 (41%)] Loss: 1.341716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [215040/500434 (43%)] Loss: 1.321803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [225280/500434 (45%)] Loss: 1.159905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [235520/500434 (47%)] Loss: 1.287761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [245760/500434 (49%)] Loss: 1.320284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [256000/500434 (51%)] Loss: 1.311713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [266240/500434 (53%)] Loss: 1.278563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [276480/500434 (55%)] Loss: 1.260480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [286720/500434 (57%)] Loss: 1.253158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [296960/500434 (59%)] Loss: 1.295729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [307200/500434 (61%)] Loss: 1.273263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [317440/500434 (63%)] Loss: 1.316110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [327680/500434 (65%)] Loss: 1.273548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [337920/500434 (67%)] Loss: 1.209796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [348160/500434 (70%)] Loss: 1.243436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [358400/500434 (72%)] Loss: 1.251532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [368640/500434 (74%)] Loss: 1.120075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [378880/500434 (76%)] Loss: 1.259590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [389120/500434 (78%)] Loss: 1.147759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [399360/500434 (80%)] Loss: 1.276368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [409600/500434 (82%)] Loss: 1.243644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [419840/500434 (84%)] Loss: 1.388480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [430080/500434 (86%)] Loss: 1.308071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [440320/500434 (88%)] Loss: 1.450920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [450560/500434 (90%)] Loss: 1.263053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [460800/500434 (92%)] Loss: 1.693270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [471040/500434 (94%)] Loss: 1.354493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [481280/500434 (96%)] Loss: 1.273862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 135 [491520/500434 (98%)] Loss: 1.137486\u001b[0m\n",
      "\u001b[34mcurrent epoch: 136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [0/500434 (0%)] Loss: 1.452450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [10240/500434 (2%)] Loss: 1.203134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [20480/500434 (4%)] Loss: 1.302396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [30720/500434 (6%)] Loss: 1.284723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [40960/500434 (8%)] Loss: 1.245737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [51200/500434 (10%)] Loss: 1.360620\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [61440/500434 (12%)] Loss: 1.314052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [71680/500434 (14%)] Loss: 1.335584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [81920/500434 (16%)] Loss: 1.339441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [92160/500434 (18%)] Loss: 1.343846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [102400/500434 (20%)] Loss: 1.304171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [112640/500434 (22%)] Loss: 1.261481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [122880/500434 (25%)] Loss: 1.192193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [133120/500434 (27%)] Loss: 1.250087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [143360/500434 (29%)] Loss: 1.221461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [153600/500434 (31%)] Loss: 1.308692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [163840/500434 (33%)] Loss: 1.206573\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [174080/500434 (35%)] Loss: 1.294667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [184320/500434 (37%)] Loss: 1.223648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [194560/500434 (39%)] Loss: 1.205120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [204800/500434 (41%)] Loss: 1.398808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [215040/500434 (43%)] Loss: 1.264206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [225280/500434 (45%)] Loss: 1.297081\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [235520/500434 (47%)] Loss: 1.393579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [245760/500434 (49%)] Loss: 1.171695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [256000/500434 (51%)] Loss: 1.229527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [266240/500434 (53%)] Loss: 1.255534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [276480/500434 (55%)] Loss: 1.346608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [286720/500434 (57%)] Loss: 1.321376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [296960/500434 (59%)] Loss: 1.245363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [307200/500434 (61%)] Loss: 1.284672\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [317440/500434 (63%)] Loss: 1.258878\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [327680/500434 (65%)] Loss: 1.317626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [337920/500434 (67%)] Loss: 1.273385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [348160/500434 (70%)] Loss: 1.244365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [358400/500434 (72%)] Loss: 1.335551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [368640/500434 (74%)] Loss: 1.252215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [378880/500434 (76%)] Loss: 1.241802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [389120/500434 (78%)] Loss: 1.263985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [399360/500434 (80%)] Loss: 1.239877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [409600/500434 (82%)] Loss: 1.292045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [419840/500434 (84%)] Loss: 1.268183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [430080/500434 (86%)] Loss: 1.307418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [440320/500434 (88%)] Loss: 1.294630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [450560/500434 (90%)] Loss: 1.419519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [460800/500434 (92%)] Loss: 1.353108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [471040/500434 (94%)] Loss: 1.293044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [481280/500434 (96%)] Loss: 1.267570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 136 [491520/500434 (98%)] Loss: 1.361640\u001b[0m\n",
      "\u001b[34mcurrent epoch: 137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [0/500434 (0%)] Loss: 1.376720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [10240/500434 (2%)] Loss: 1.290566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [20480/500434 (4%)] Loss: 1.324194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [30720/500434 (6%)] Loss: 1.390843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [40960/500434 (8%)] Loss: 1.283356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [51200/500434 (10%)] Loss: 1.337627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [61440/500434 (12%)] Loss: 1.313430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [71680/500434 (14%)] Loss: 1.211643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [81920/500434 (16%)] Loss: 1.310852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [92160/500434 (18%)] Loss: 1.384869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [102400/500434 (20%)] Loss: 1.328802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [112640/500434 (22%)] Loss: 1.350029\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [122880/500434 (25%)] Loss: 1.289663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [133120/500434 (27%)] Loss: 1.291981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [143360/500434 (29%)] Loss: 1.227843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [153600/500434 (31%)] Loss: 1.378159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [163840/500434 (33%)] Loss: 1.303515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [174080/500434 (35%)] Loss: 1.265890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [184320/500434 (37%)] Loss: 1.212393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [194560/500434 (39%)] Loss: 1.225489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [204800/500434 (41%)] Loss: 1.200889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [215040/500434 (43%)] Loss: 1.405043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [225280/500434 (45%)] Loss: 1.293728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [235520/500434 (47%)] Loss: 1.177326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [245760/500434 (49%)] Loss: 1.370921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [256000/500434 (51%)] Loss: 1.274309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [266240/500434 (53%)] Loss: 1.243126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [276480/500434 (55%)] Loss: 1.367110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [286720/500434 (57%)] Loss: 1.332356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [296960/500434 (59%)] Loss: 1.234323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [307200/500434 (61%)] Loss: 1.451373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [317440/500434 (63%)] Loss: 1.270322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [327680/500434 (65%)] Loss: 1.206563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [337920/500434 (67%)] Loss: 1.207023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [348160/500434 (70%)] Loss: 1.370326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [358400/500434 (72%)] Loss: 1.376133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [368640/500434 (74%)] Loss: 1.262474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [378880/500434 (76%)] Loss: 1.279249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [389120/500434 (78%)] Loss: 1.285121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [399360/500434 (80%)] Loss: 1.336582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [409600/500434 (82%)] Loss: 1.167534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [419840/500434 (84%)] Loss: 1.279822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [430080/500434 (86%)] Loss: 1.287925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [440320/500434 (88%)] Loss: 1.356545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [450560/500434 (90%)] Loss: 1.245468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [460800/500434 (92%)] Loss: 1.348959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [471040/500434 (94%)] Loss: 1.345943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [481280/500434 (96%)] Loss: 1.303291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 137 [491520/500434 (98%)] Loss: 1.284728\u001b[0m\n",
      "\u001b[34mcurrent epoch: 138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [0/500434 (0%)] Loss: 1.425482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [10240/500434 (2%)] Loss: 1.389522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [20480/500434 (4%)] Loss: 1.299128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [30720/500434 (6%)] Loss: 1.212590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [40960/500434 (8%)] Loss: 1.188680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [51200/500434 (10%)] Loss: 1.263113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [61440/500434 (12%)] Loss: 1.176166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [71680/500434 (14%)] Loss: 1.402377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [81920/500434 (16%)] Loss: 1.252458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [92160/500434 (18%)] Loss: 1.293505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [102400/500434 (20%)] Loss: 1.222581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [112640/500434 (22%)] Loss: 1.262949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [122880/500434 (25%)] Loss: 1.303458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [133120/500434 (27%)] Loss: 1.240325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [143360/500434 (29%)] Loss: 1.340163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [153600/500434 (31%)] Loss: 1.298182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [163840/500434 (33%)] Loss: 1.321797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [174080/500434 (35%)] Loss: 1.222145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [184320/500434 (37%)] Loss: 1.235569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [194560/500434 (39%)] Loss: 1.199551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [204800/500434 (41%)] Loss: 1.232845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [215040/500434 (43%)] Loss: 1.315679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [225280/500434 (45%)] Loss: 1.225336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [235520/500434 (47%)] Loss: 1.258750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [245760/500434 (49%)] Loss: 1.294220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [256000/500434 (51%)] Loss: 1.249106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [266240/500434 (53%)] Loss: 1.286680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [276480/500434 (55%)] Loss: 1.236216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [286720/500434 (57%)] Loss: 1.246651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [296960/500434 (59%)] Loss: 1.385998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [307200/500434 (61%)] Loss: 1.201839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [317440/500434 (63%)] Loss: 1.346499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [327680/500434 (65%)] Loss: 1.299161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [337920/500434 (67%)] Loss: 1.234471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [348160/500434 (70%)] Loss: 1.301195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [358400/500434 (72%)] Loss: 1.319315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [368640/500434 (74%)] Loss: 1.236329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [378880/500434 (76%)] Loss: 1.250821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [389120/500434 (78%)] Loss: 1.218792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [399360/500434 (80%)] Loss: 1.252044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [409600/500434 (82%)] Loss: 1.202043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [419840/500434 (84%)] Loss: 1.276779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [430080/500434 (86%)] Loss: 1.283684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [440320/500434 (88%)] Loss: 1.289493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [450560/500434 (90%)] Loss: 1.202219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [460800/500434 (92%)] Loss: 1.314690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [471040/500434 (94%)] Loss: 1.299868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [481280/500434 (96%)] Loss: 1.336495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 138 [491520/500434 (98%)] Loss: 1.347115\u001b[0m\n",
      "\u001b[34mcurrent epoch: 139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [0/500434 (0%)] Loss: 1.433358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [10240/500434 (2%)] Loss: 1.251373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [20480/500434 (4%)] Loss: 1.215132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [30720/500434 (6%)] Loss: 1.373033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [40960/500434 (8%)] Loss: 1.304658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [51200/500434 (10%)] Loss: 1.268658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [61440/500434 (12%)] Loss: 1.278388\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [71680/500434 (14%)] Loss: 1.320265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [81920/500434 (16%)] Loss: 1.381348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [92160/500434 (18%)] Loss: 1.304815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [102400/500434 (20%)] Loss: 2.400838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [112640/500434 (22%)] Loss: 1.641407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [122880/500434 (25%)] Loss: 1.666114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [133120/500434 (27%)] Loss: 1.560695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [143360/500434 (29%)] Loss: 1.608323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [153600/500434 (31%)] Loss: 1.627444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [163840/500434 (33%)] Loss: 1.587468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [174080/500434 (35%)] Loss: 1.472687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [184320/500434 (37%)] Loss: 1.545444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [194560/500434 (39%)] Loss: 1.617703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [204800/500434 (41%)] Loss: 1.490412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [215040/500434 (43%)] Loss: 1.332425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [225280/500434 (45%)] Loss: 1.512986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [235520/500434 (47%)] Loss: 1.504661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [245760/500434 (49%)] Loss: 1.418128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [256000/500434 (51%)] Loss: 1.435149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [266240/500434 (53%)] Loss: 1.426208\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [276480/500434 (55%)] Loss: 1.362175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [286720/500434 (57%)] Loss: 1.386241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [296960/500434 (59%)] Loss: 1.525338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [307200/500434 (61%)] Loss: 1.474969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [317440/500434 (63%)] Loss: 1.393002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [327680/500434 (65%)] Loss: 1.578984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [337920/500434 (67%)] Loss: 1.694816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [348160/500434 (70%)] Loss: 1.634158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [358400/500434 (72%)] Loss: 1.578719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [368640/500434 (74%)] Loss: 1.617392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [378880/500434 (76%)] Loss: 1.658726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [389120/500434 (78%)] Loss: 1.540413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [399360/500434 (80%)] Loss: 1.529383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [409600/500434 (82%)] Loss: 1.576302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [419840/500434 (84%)] Loss: 1.567851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [430080/500434 (86%)] Loss: 1.572737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [440320/500434 (88%)] Loss: 1.601089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [450560/500434 (90%)] Loss: 1.539783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [460800/500434 (92%)] Loss: 1.459511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [471040/500434 (94%)] Loss: 1.479588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [481280/500434 (96%)] Loss: 1.436315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 139 [491520/500434 (98%)] Loss: 1.520209\u001b[0m\n",
      "\u001b[34mcurrent epoch: 140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [0/500434 (0%)] Loss: 1.545400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [10240/500434 (2%)] Loss: 1.562775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [20480/500434 (4%)] Loss: 1.476879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [30720/500434 (6%)] Loss: 1.534574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [40960/500434 (8%)] Loss: 1.445500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [51200/500434 (10%)] Loss: 1.403101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [61440/500434 (12%)] Loss: 1.421338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [71680/500434 (14%)] Loss: 1.464652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [81920/500434 (16%)] Loss: 1.462868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [92160/500434 (18%)] Loss: 1.513472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [102400/500434 (20%)] Loss: 1.486904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [112640/500434 (22%)] Loss: 1.427713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [122880/500434 (25%)] Loss: 1.492478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [133120/500434 (27%)] Loss: 1.465313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [143360/500434 (29%)] Loss: 1.408410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [153600/500434 (31%)] Loss: 1.451371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [163840/500434 (33%)] Loss: 1.436378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [174080/500434 (35%)] Loss: 1.479468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [184320/500434 (37%)] Loss: 1.436337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [194560/500434 (39%)] Loss: 1.548991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [204800/500434 (41%)] Loss: 1.446564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [215040/500434 (43%)] Loss: 1.478616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [225280/500434 (45%)] Loss: 1.508968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [235520/500434 (47%)] Loss: 1.410289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [245760/500434 (49%)] Loss: 1.455787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [256000/500434 (51%)] Loss: 1.403757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [266240/500434 (53%)] Loss: 1.460956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [276480/500434 (55%)] Loss: 1.542526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [286720/500434 (57%)] Loss: 1.553713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [296960/500434 (59%)] Loss: 1.472657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [307200/500434 (61%)] Loss: 1.433848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [317440/500434 (63%)] Loss: 1.470574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [327680/500434 (65%)] Loss: 1.407984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [337920/500434 (67%)] Loss: 1.436345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [348160/500434 (70%)] Loss: 1.420286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [358400/500434 (72%)] Loss: 1.449440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [368640/500434 (74%)] Loss: 1.360161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [378880/500434 (76%)] Loss: 1.418017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [389120/500434 (78%)] Loss: 1.627199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [399360/500434 (80%)] Loss: 1.409526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [409600/500434 (82%)] Loss: 1.459882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [419840/500434 (84%)] Loss: 1.497822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [430080/500434 (86%)] Loss: 1.402952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [440320/500434 (88%)] Loss: 1.555834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [450560/500434 (90%)] Loss: 1.364394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [460800/500434 (92%)] Loss: 1.346534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [471040/500434 (94%)] Loss: 1.414172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [481280/500434 (96%)] Loss: 1.453933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 140 [491520/500434 (98%)] Loss: 1.488162\u001b[0m\n",
      "\u001b[34mcurrent epoch: 141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [0/500434 (0%)] Loss: 1.351554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [10240/500434 (2%)] Loss: 1.584601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [20480/500434 (4%)] Loss: 1.490334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [30720/500434 (6%)] Loss: 1.340150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [40960/500434 (8%)] Loss: 1.446847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [51200/500434 (10%)] Loss: 1.454214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [61440/500434 (12%)] Loss: 1.440291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [71680/500434 (14%)] Loss: 1.498431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [81920/500434 (16%)] Loss: 1.402347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [92160/500434 (18%)] Loss: 1.497521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [102400/500434 (20%)] Loss: 1.382228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [112640/500434 (22%)] Loss: 1.401255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [122880/500434 (25%)] Loss: 1.343848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [133120/500434 (27%)] Loss: 1.560902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [143360/500434 (29%)] Loss: 1.438988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [153600/500434 (31%)] Loss: 1.378112\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [163840/500434 (33%)] Loss: 1.551067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [174080/500434 (35%)] Loss: 1.546592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [184320/500434 (37%)] Loss: 1.396872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [194560/500434 (39%)] Loss: 1.526772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [204800/500434 (41%)] Loss: 1.432508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [215040/500434 (43%)] Loss: 1.448128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [225280/500434 (45%)] Loss: 1.360140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [235520/500434 (47%)] Loss: 1.468625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [245760/500434 (49%)] Loss: 1.441004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [256000/500434 (51%)] Loss: 1.335087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [266240/500434 (53%)] Loss: 1.466597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [276480/500434 (55%)] Loss: 1.268687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [286720/500434 (57%)] Loss: 1.429780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [296960/500434 (59%)] Loss: 1.222807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [307200/500434 (61%)] Loss: 1.482122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [317440/500434 (63%)] Loss: 1.389239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [327680/500434 (65%)] Loss: 1.446851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [337920/500434 (67%)] Loss: 1.408978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [348160/500434 (70%)] Loss: 1.408745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [358400/500434 (72%)] Loss: 1.425578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [368640/500434 (74%)] Loss: 1.380574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [378880/500434 (76%)] Loss: 1.346818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [389120/500434 (78%)] Loss: 1.416739\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [399360/500434 (80%)] Loss: 1.402774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [409600/500434 (82%)] Loss: 1.315836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [419840/500434 (84%)] Loss: 1.348226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [430080/500434 (86%)] Loss: 1.325966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [440320/500434 (88%)] Loss: 1.313423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [450560/500434 (90%)] Loss: 1.395257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [460800/500434 (92%)] Loss: 1.490277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [471040/500434 (94%)] Loss: 1.363498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [481280/500434 (96%)] Loss: 1.396701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 141 [491520/500434 (98%)] Loss: 1.421442\u001b[0m\n",
      "\u001b[34mcurrent epoch: 142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [0/500434 (0%)] Loss: 1.363637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [10240/500434 (2%)] Loss: 1.328477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [20480/500434 (4%)] Loss: 1.432160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [30720/500434 (6%)] Loss: 1.492947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [40960/500434 (8%)] Loss: 1.325003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [51200/500434 (10%)] Loss: 1.304599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [61440/500434 (12%)] Loss: 1.392460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [71680/500434 (14%)] Loss: 1.270126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [81920/500434 (16%)] Loss: 1.283962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [92160/500434 (18%)] Loss: 1.453608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [102400/500434 (20%)] Loss: 1.468118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [112640/500434 (22%)] Loss: 1.442891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [122880/500434 (25%)] Loss: 1.387932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [133120/500434 (27%)] Loss: 1.515776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [143360/500434 (29%)] Loss: 1.267174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [153600/500434 (31%)] Loss: 1.389171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [163840/500434 (33%)] Loss: 1.280740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [174080/500434 (35%)] Loss: 1.328866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [184320/500434 (37%)] Loss: 1.366856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [194560/500434 (39%)] Loss: 1.400563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [204800/500434 (41%)] Loss: 1.389269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [215040/500434 (43%)] Loss: 1.520265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [225280/500434 (45%)] Loss: 1.285106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [235520/500434 (47%)] Loss: 1.395061\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [245760/500434 (49%)] Loss: 1.477392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [256000/500434 (51%)] Loss: 1.322241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [266240/500434 (53%)] Loss: 1.496644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [276480/500434 (55%)] Loss: 1.389673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [286720/500434 (57%)] Loss: 1.350259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [296960/500434 (59%)] Loss: 1.379877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [307200/500434 (61%)] Loss: 1.381181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [317440/500434 (63%)] Loss: 1.492344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [327680/500434 (65%)] Loss: 1.481440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [337920/500434 (67%)] Loss: 1.400959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [348160/500434 (70%)] Loss: 1.419231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [358400/500434 (72%)] Loss: 1.426043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [368640/500434 (74%)] Loss: 1.365387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [378880/500434 (76%)] Loss: 1.408315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [389120/500434 (78%)] Loss: 1.345209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [399360/500434 (80%)] Loss: 1.370948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [409600/500434 (82%)] Loss: 1.302835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [419840/500434 (84%)] Loss: 1.349693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [430080/500434 (86%)] Loss: 1.293523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [440320/500434 (88%)] Loss: 1.264726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [450560/500434 (90%)] Loss: 1.354043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [460800/500434 (92%)] Loss: 1.357869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [471040/500434 (94%)] Loss: 1.456924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [481280/500434 (96%)] Loss: 1.361199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 142 [491520/500434 (98%)] Loss: 1.331049\u001b[0m\n",
      "\u001b[34mcurrent epoch: 143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [0/500434 (0%)] Loss: 1.327972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [10240/500434 (2%)] Loss: 1.306252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [20480/500434 (4%)] Loss: 1.370709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [30720/500434 (6%)] Loss: 1.466993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [40960/500434 (8%)] Loss: 1.354049\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [51200/500434 (10%)] Loss: 1.370964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [61440/500434 (12%)] Loss: 1.351791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [71680/500434 (14%)] Loss: 1.439215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [81920/500434 (16%)] Loss: 1.346792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [92160/500434 (18%)] Loss: 1.355477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [102400/500434 (20%)] Loss: 1.429271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [112640/500434 (22%)] Loss: 1.301234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [122880/500434 (25%)] Loss: 1.322114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [133120/500434 (27%)] Loss: 1.398564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [143360/500434 (29%)] Loss: 1.428331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [153600/500434 (31%)] Loss: 1.382909\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [163840/500434 (33%)] Loss: 1.455702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [174080/500434 (35%)] Loss: 1.428517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [184320/500434 (37%)] Loss: 1.328997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [194560/500434 (39%)] Loss: 1.308631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [204800/500434 (41%)] Loss: 1.351647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [215040/500434 (43%)] Loss: 1.279016\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [225280/500434 (45%)] Loss: 1.449043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [235520/500434 (47%)] Loss: 1.419237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [245760/500434 (49%)] Loss: 1.423684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [256000/500434 (51%)] Loss: 1.340906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [266240/500434 (53%)] Loss: 1.332442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [276480/500434 (55%)] Loss: 1.323800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [286720/500434 (57%)] Loss: 1.458147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [296960/500434 (59%)] Loss: 1.291816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [307200/500434 (61%)] Loss: 1.396456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [317440/500434 (63%)] Loss: 1.413895\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [327680/500434 (65%)] Loss: 1.411704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [337920/500434 (67%)] Loss: 1.307593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [348160/500434 (70%)] Loss: 1.378334\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [358400/500434 (72%)] Loss: 1.393785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [368640/500434 (74%)] Loss: 1.365747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [378880/500434 (76%)] Loss: 1.382750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [389120/500434 (78%)] Loss: 1.389411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [399360/500434 (80%)] Loss: 1.328248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [409600/500434 (82%)] Loss: 1.458517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [419840/500434 (84%)] Loss: 1.303167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [430080/500434 (86%)] Loss: 1.388868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [440320/500434 (88%)] Loss: 1.253428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [450560/500434 (90%)] Loss: 1.440209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [460800/500434 (92%)] Loss: 1.371483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [471040/500434 (94%)] Loss: 1.323019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [481280/500434 (96%)] Loss: 1.312799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 143 [491520/500434 (98%)] Loss: 1.342207\u001b[0m\n",
      "\u001b[34mcurrent epoch: 144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [0/500434 (0%)] Loss: 1.319105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [10240/500434 (2%)] Loss: 1.296158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [20480/500434 (4%)] Loss: 1.378569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [30720/500434 (6%)] Loss: 1.362541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [40960/500434 (8%)] Loss: 1.361194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [51200/500434 (10%)] Loss: 1.364612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [61440/500434 (12%)] Loss: 1.445858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [71680/500434 (14%)] Loss: 1.358727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [81920/500434 (16%)] Loss: 1.426571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [92160/500434 (18%)] Loss: 1.390347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [102400/500434 (20%)] Loss: 1.342216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [112640/500434 (22%)] Loss: 1.328864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [122880/500434 (25%)] Loss: 1.353897\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [133120/500434 (27%)] Loss: 1.276167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [143360/500434 (29%)] Loss: 1.266287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [153600/500434 (31%)] Loss: 1.314946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [163840/500434 (33%)] Loss: 1.377248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [174080/500434 (35%)] Loss: 1.347812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [184320/500434 (37%)] Loss: 1.370598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [194560/500434 (39%)] Loss: 1.403612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [204800/500434 (41%)] Loss: 1.376205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [215040/500434 (43%)] Loss: 1.384331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [225280/500434 (45%)] Loss: 1.337085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [235520/500434 (47%)] Loss: 1.304621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [245760/500434 (49%)] Loss: 1.328722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [256000/500434 (51%)] Loss: 1.327701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [266240/500434 (53%)] Loss: 1.417459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [276480/500434 (55%)] Loss: 1.516916\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [286720/500434 (57%)] Loss: 1.474118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [296960/500434 (59%)] Loss: 1.443480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [307200/500434 (61%)] Loss: 1.307809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [317440/500434 (63%)] Loss: 1.366513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [327680/500434 (65%)] Loss: 1.347560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [337920/500434 (67%)] Loss: 1.282435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [348160/500434 (70%)] Loss: 1.353979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [358400/500434 (72%)] Loss: 1.451784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [368640/500434 (74%)] Loss: 1.434087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [378880/500434 (76%)] Loss: 1.443748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [389120/500434 (78%)] Loss: 1.369996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [399360/500434 (80%)] Loss: 1.261594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [409600/500434 (82%)] Loss: 1.368381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [419840/500434 (84%)] Loss: 1.438367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [430080/500434 (86%)] Loss: 1.391932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [440320/500434 (88%)] Loss: 1.338106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [450560/500434 (90%)] Loss: 1.351135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [460800/500434 (92%)] Loss: 1.361328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [471040/500434 (94%)] Loss: 1.426298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [481280/500434 (96%)] Loss: 1.453024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 144 [491520/500434 (98%)] Loss: 1.318151\u001b[0m\n",
      "\u001b[34mcurrent epoch: 145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [0/500434 (0%)] Loss: 1.241011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [10240/500434 (2%)] Loss: 1.236219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [20480/500434 (4%)] Loss: 1.253564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [30720/500434 (6%)] Loss: 1.394084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [40960/500434 (8%)] Loss: 1.386952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [51200/500434 (10%)] Loss: 1.453531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [61440/500434 (12%)] Loss: 1.282407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [71680/500434 (14%)] Loss: 1.268454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [81920/500434 (16%)] Loss: 1.363618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [92160/500434 (18%)] Loss: 1.297986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [102400/500434 (20%)] Loss: 1.339117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [112640/500434 (22%)] Loss: 1.354764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [122880/500434 (25%)] Loss: 1.386241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [133120/500434 (27%)] Loss: 1.468597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [143360/500434 (29%)] Loss: 1.297072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [153600/500434 (31%)] Loss: 1.426019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [163840/500434 (33%)] Loss: 1.262834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [174080/500434 (35%)] Loss: 1.352882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [184320/500434 (37%)] Loss: 1.320616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [194560/500434 (39%)] Loss: 1.363359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [204800/500434 (41%)] Loss: 1.457295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [215040/500434 (43%)] Loss: 1.346616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [225280/500434 (45%)] Loss: 1.292716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [235520/500434 (47%)] Loss: 1.428769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [245760/500434 (49%)] Loss: 1.344574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [256000/500434 (51%)] Loss: 1.361588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [266240/500434 (53%)] Loss: 1.391165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [276480/500434 (55%)] Loss: 1.275855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [286720/500434 (57%)] Loss: 1.311493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [296960/500434 (59%)] Loss: 1.365134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [307200/500434 (61%)] Loss: 1.270695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [317440/500434 (63%)] Loss: 1.280608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [327680/500434 (65%)] Loss: 1.478777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [337920/500434 (67%)] Loss: 1.330818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [348160/500434 (70%)] Loss: 1.465308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [358400/500434 (72%)] Loss: 1.334548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [368640/500434 (74%)] Loss: 1.229377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [378880/500434 (76%)] Loss: 1.305444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [389120/500434 (78%)] Loss: 1.357857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [399360/500434 (80%)] Loss: 1.321226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [409600/500434 (82%)] Loss: 1.392533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [419840/500434 (84%)] Loss: 1.387329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [430080/500434 (86%)] Loss: 1.341600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [440320/500434 (88%)] Loss: 1.330328\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [450560/500434 (90%)] Loss: 1.266890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [460800/500434 (92%)] Loss: 1.367958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [471040/500434 (94%)] Loss: 1.355950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [481280/500434 (96%)] Loss: 1.420888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 145 [491520/500434 (98%)] Loss: 1.309788\u001b[0m\n",
      "\u001b[34mcurrent epoch: 146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [0/500434 (0%)] Loss: 1.505080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [10240/500434 (2%)] Loss: 1.393281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [20480/500434 (4%)] Loss: 1.327151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [30720/500434 (6%)] Loss: 1.422688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [40960/500434 (8%)] Loss: 1.337564\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [51200/500434 (10%)] Loss: 1.316167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [61440/500434 (12%)] Loss: 1.362397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [71680/500434 (14%)] Loss: 1.384690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [81920/500434 (16%)] Loss: 1.391487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [92160/500434 (18%)] Loss: 1.404934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [102400/500434 (20%)] Loss: 1.420703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [112640/500434 (22%)] Loss: 1.273270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [122880/500434 (25%)] Loss: 1.326509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [133120/500434 (27%)] Loss: 1.463948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [143360/500434 (29%)] Loss: 1.277659\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [153600/500434 (31%)] Loss: 1.376142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [163840/500434 (33%)] Loss: 1.348214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [174080/500434 (35%)] Loss: 1.285000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [184320/500434 (37%)] Loss: 1.388237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [194560/500434 (39%)] Loss: 1.327135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [204800/500434 (41%)] Loss: 1.384428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [215040/500434 (43%)] Loss: 1.332091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [225280/500434 (45%)] Loss: 1.429165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [235520/500434 (47%)] Loss: 1.369281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [245760/500434 (49%)] Loss: 1.305650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [256000/500434 (51%)] Loss: 1.358316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [266240/500434 (53%)] Loss: 1.384908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [276480/500434 (55%)] Loss: 1.304547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [286720/500434 (57%)] Loss: 1.400608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [296960/500434 (59%)] Loss: 1.375062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [307200/500434 (61%)] Loss: 1.280738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [317440/500434 (63%)] Loss: 1.359700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [327680/500434 (65%)] Loss: 1.361935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [337920/500434 (67%)] Loss: 1.374850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [348160/500434 (70%)] Loss: 1.393985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [358400/500434 (72%)] Loss: 1.271480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [368640/500434 (74%)] Loss: 1.515823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [378880/500434 (76%)] Loss: 1.465324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [389120/500434 (78%)] Loss: 1.242486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [399360/500434 (80%)] Loss: 1.441849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [409600/500434 (82%)] Loss: 1.305467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [419840/500434 (84%)] Loss: 1.397578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [430080/500434 (86%)] Loss: 1.376648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [440320/500434 (88%)] Loss: 1.359107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [450560/500434 (90%)] Loss: 1.342336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [460800/500434 (92%)] Loss: 1.390490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [471040/500434 (94%)] Loss: 1.299600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [481280/500434 (96%)] Loss: 1.364648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 146 [491520/500434 (98%)] Loss: 1.322263\u001b[0m\n",
      "\u001b[34mcurrent epoch: 147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [0/500434 (0%)] Loss: 1.345615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [10240/500434 (2%)] Loss: 1.294708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [20480/500434 (4%)] Loss: 1.350322\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [30720/500434 (6%)] Loss: 1.322738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [40960/500434 (8%)] Loss: 1.336788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [51200/500434 (10%)] Loss: 1.283141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [61440/500434 (12%)] Loss: 1.413931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [71680/500434 (14%)] Loss: 1.306650\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [81920/500434 (16%)] Loss: 1.412915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [92160/500434 (18%)] Loss: 1.280417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [102400/500434 (20%)] Loss: 1.268139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [112640/500434 (22%)] Loss: 1.332353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [122880/500434 (25%)] Loss: 1.343838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [133120/500434 (27%)] Loss: 1.355525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [143360/500434 (29%)] Loss: 1.388063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [153600/500434 (31%)] Loss: 1.238402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [163840/500434 (33%)] Loss: 1.417490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [174080/500434 (35%)] Loss: 1.219765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [184320/500434 (37%)] Loss: 1.389991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [194560/500434 (39%)] Loss: 1.312992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [204800/500434 (41%)] Loss: 1.342351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [215040/500434 (43%)] Loss: 1.234199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [225280/500434 (45%)] Loss: 1.404147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [235520/500434 (47%)] Loss: 1.325884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [245760/500434 (49%)] Loss: 1.312911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [256000/500434 (51%)] Loss: 1.401130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [266240/500434 (53%)] Loss: 1.376565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [276480/500434 (55%)] Loss: 1.353022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [286720/500434 (57%)] Loss: 1.318996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [296960/500434 (59%)] Loss: 1.364384\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [307200/500434 (61%)] Loss: 1.304721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [317440/500434 (63%)] Loss: 1.317839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [327680/500434 (65%)] Loss: 1.343583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [337920/500434 (67%)] Loss: 1.295906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [348160/500434 (70%)] Loss: 1.503407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [358400/500434 (72%)] Loss: 1.404145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [368640/500434 (74%)] Loss: 1.327532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [378880/500434 (76%)] Loss: 1.449903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [389120/500434 (78%)] Loss: 1.365360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [399360/500434 (80%)] Loss: 1.340047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [409600/500434 (82%)] Loss: 1.402181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [419840/500434 (84%)] Loss: 1.291871\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [430080/500434 (86%)] Loss: 1.285869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [440320/500434 (88%)] Loss: 1.455643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [450560/500434 (90%)] Loss: 1.320999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [460800/500434 (92%)] Loss: 1.248626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [471040/500434 (94%)] Loss: 1.370142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [481280/500434 (96%)] Loss: 1.375694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 147 [491520/500434 (98%)] Loss: 1.379654\u001b[0m\n",
      "\u001b[34mcurrent epoch: 148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [0/500434 (0%)] Loss: 1.352640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [10240/500434 (2%)] Loss: 1.336699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [20480/500434 (4%)] Loss: 1.329340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [30720/500434 (6%)] Loss: 1.290877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [40960/500434 (8%)] Loss: 1.273153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [51200/500434 (10%)] Loss: 1.270853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [61440/500434 (12%)] Loss: 1.280948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [71680/500434 (14%)] Loss: 1.436821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [81920/500434 (16%)] Loss: 1.375151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [92160/500434 (18%)] Loss: 1.362032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [102400/500434 (20%)] Loss: 1.245024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [112640/500434 (22%)] Loss: 1.286567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [122880/500434 (25%)] Loss: 1.351233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [133120/500434 (27%)] Loss: 1.555083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [143360/500434 (29%)] Loss: 1.352822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [153600/500434 (31%)] Loss: 1.240694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [163840/500434 (33%)] Loss: 1.233953\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [174080/500434 (35%)] Loss: 1.276408\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [184320/500434 (37%)] Loss: 1.355242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [194560/500434 (39%)] Loss: 1.276851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [204800/500434 (41%)] Loss: 1.383766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [215040/500434 (43%)] Loss: 1.281521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [225280/500434 (45%)] Loss: 1.227331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [235520/500434 (47%)] Loss: 1.343677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [245760/500434 (49%)] Loss: 1.246787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [256000/500434 (51%)] Loss: 1.351483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [266240/500434 (53%)] Loss: 1.311054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [276480/500434 (55%)] Loss: 1.373922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [286720/500434 (57%)] Loss: 1.351271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [296960/500434 (59%)] Loss: 1.317093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [307200/500434 (61%)] Loss: 1.448057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [317440/500434 (63%)] Loss: 1.273153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [327680/500434 (65%)] Loss: 1.343312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [337920/500434 (67%)] Loss: 1.251474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [348160/500434 (70%)] Loss: 1.452325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [358400/500434 (72%)] Loss: 1.411427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [368640/500434 (74%)] Loss: 1.390787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [378880/500434 (76%)] Loss: 1.268404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [389120/500434 (78%)] Loss: 1.294577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [399360/500434 (80%)] Loss: 1.348894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [409600/500434 (82%)] Loss: 1.328631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [419840/500434 (84%)] Loss: 1.413528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [430080/500434 (86%)] Loss: 1.325746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [440320/500434 (88%)] Loss: 1.278942\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [450560/500434 (90%)] Loss: 1.299380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [460800/500434 (92%)] Loss: 1.412446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [471040/500434 (94%)] Loss: 1.283022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [481280/500434 (96%)] Loss: 1.362828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 148 [491520/500434 (98%)] Loss: 1.277306\u001b[0m\n",
      "\u001b[34mcurrent epoch: 149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [0/500434 (0%)] Loss: 1.399520\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [10240/500434 (2%)] Loss: 1.359226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [20480/500434 (4%)] Loss: 1.279613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [30720/500434 (6%)] Loss: 1.245254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [40960/500434 (8%)] Loss: 1.341511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [51200/500434 (10%)] Loss: 1.324898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [61440/500434 (12%)] Loss: 1.388013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [71680/500434 (14%)] Loss: 1.391235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [81920/500434 (16%)] Loss: 1.355779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [92160/500434 (18%)] Loss: 1.351664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [102400/500434 (20%)] Loss: 1.282864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [112640/500434 (22%)] Loss: 1.299224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [122880/500434 (25%)] Loss: 1.291385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [133120/500434 (27%)] Loss: 1.328849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [143360/500434 (29%)] Loss: 1.262438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [153600/500434 (31%)] Loss: 1.199213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [163840/500434 (33%)] Loss: 1.212753\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [174080/500434 (35%)] Loss: 1.303586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [184320/500434 (37%)] Loss: 1.322078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [194560/500434 (39%)] Loss: 1.400239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [204800/500434 (41%)] Loss: 1.430401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [215040/500434 (43%)] Loss: 1.226576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [225280/500434 (45%)] Loss: 1.376471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [235520/500434 (47%)] Loss: 1.298890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [245760/500434 (49%)] Loss: 1.302021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [256000/500434 (51%)] Loss: 1.396466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [266240/500434 (53%)] Loss: 1.446891\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [276480/500434 (55%)] Loss: 1.344485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [286720/500434 (57%)] Loss: 1.331910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [296960/500434 (59%)] Loss: 1.352748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [307200/500434 (61%)] Loss: 1.369636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [317440/500434 (63%)] Loss: 1.342919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [327680/500434 (65%)] Loss: 1.336186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [337920/500434 (67%)] Loss: 1.359059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [348160/500434 (70%)] Loss: 1.308681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [358400/500434 (72%)] Loss: 1.349186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [368640/500434 (74%)] Loss: 1.260208\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [378880/500434 (76%)] Loss: 1.270758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [389120/500434 (78%)] Loss: 1.384927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [399360/500434 (80%)] Loss: 1.372586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [409600/500434 (82%)] Loss: 1.415086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [419840/500434 (84%)] Loss: 1.305883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [430080/500434 (86%)] Loss: 1.350399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [440320/500434 (88%)] Loss: 1.339952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [450560/500434 (90%)] Loss: 1.295613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [460800/500434 (92%)] Loss: 1.364090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [471040/500434 (94%)] Loss: 1.354876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [481280/500434 (96%)] Loss: 1.373934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 149 [491520/500434 (98%)] Loss: 1.438738\u001b[0m\n",
      "\u001b[34mcurrent epoch: 150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [0/500434 (0%)] Loss: 1.266845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [10240/500434 (2%)] Loss: 1.288056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [20480/500434 (4%)] Loss: 1.238318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [30720/500434 (6%)] Loss: 1.285453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [40960/500434 (8%)] Loss: 1.345103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [51200/500434 (10%)] Loss: 1.272424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [61440/500434 (12%)] Loss: 1.290407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [71680/500434 (14%)] Loss: 1.391360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [81920/500434 (16%)] Loss: 1.278787\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [92160/500434 (18%)] Loss: 1.287313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [102400/500434 (20%)] Loss: 1.286433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [112640/500434 (22%)] Loss: 1.179312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [122880/500434 (25%)] Loss: 1.280858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [133120/500434 (27%)] Loss: 1.320315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [143360/500434 (29%)] Loss: 1.340592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [153600/500434 (31%)] Loss: 1.314352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [163840/500434 (33%)] Loss: 1.205996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [174080/500434 (35%)] Loss: 1.173387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [184320/500434 (37%)] Loss: 1.245889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [194560/500434 (39%)] Loss: 1.397689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [204800/500434 (41%)] Loss: 1.439078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [215040/500434 (43%)] Loss: 1.366602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [225280/500434 (45%)] Loss: 1.246956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [235520/500434 (47%)] Loss: 1.356864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [245760/500434 (49%)] Loss: 1.351100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [256000/500434 (51%)] Loss: 1.297616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [266240/500434 (53%)] Loss: 1.269569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [276480/500434 (55%)] Loss: 1.335828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [286720/500434 (57%)] Loss: 1.343264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [296960/500434 (59%)] Loss: 1.343509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [307200/500434 (61%)] Loss: 1.379757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [317440/500434 (63%)] Loss: 1.256863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [327680/500434 (65%)] Loss: 1.345229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [337920/500434 (67%)] Loss: 1.338099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [348160/500434 (70%)] Loss: 1.344308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [358400/500434 (72%)] Loss: 1.297846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [368640/500434 (74%)] Loss: 1.389943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [378880/500434 (76%)] Loss: 1.225935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [389120/500434 (78%)] Loss: 1.282902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [399360/500434 (80%)] Loss: 1.326886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [409600/500434 (82%)] Loss: 1.309182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [419840/500434 (84%)] Loss: 1.351962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [430080/500434 (86%)] Loss: 1.264350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [440320/500434 (88%)] Loss: 1.437291\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [450560/500434 (90%)] Loss: 1.298795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [460800/500434 (92%)] Loss: 1.343672\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [471040/500434 (94%)] Loss: 1.335875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [481280/500434 (96%)] Loss: 1.379710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 150 [491520/500434 (98%)] Loss: 1.423834\u001b[0m\n",
      "\u001b[34mcurrent epoch: 151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [0/500434 (0%)] Loss: 1.245362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [10240/500434 (2%)] Loss: 1.287542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [20480/500434 (4%)] Loss: 1.209359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [30720/500434 (6%)] Loss: 1.396616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [40960/500434 (8%)] Loss: 1.251854\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [51200/500434 (10%)] Loss: 1.162347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [61440/500434 (12%)] Loss: 1.356589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [71680/500434 (14%)] Loss: 1.395217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [81920/500434 (16%)] Loss: 1.249150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [92160/500434 (18%)] Loss: 1.321052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [102400/500434 (20%)] Loss: 1.301476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [112640/500434 (22%)] Loss: 1.284616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [122880/500434 (25%)] Loss: 1.297655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [133120/500434 (27%)] Loss: 1.457430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [143360/500434 (29%)] Loss: 1.382772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [153600/500434 (31%)] Loss: 1.302809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [163840/500434 (33%)] Loss: 1.360648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [174080/500434 (35%)] Loss: 1.239168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [184320/500434 (37%)] Loss: 1.275440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [194560/500434 (39%)] Loss: 1.244828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [204800/500434 (41%)] Loss: 1.290697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [215040/500434 (43%)] Loss: 1.259735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [225280/500434 (45%)] Loss: 1.271034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [235520/500434 (47%)] Loss: 1.321335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [245760/500434 (49%)] Loss: 1.376577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [256000/500434 (51%)] Loss: 1.341698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [266240/500434 (53%)] Loss: 1.237407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [276480/500434 (55%)] Loss: 1.332392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [286720/500434 (57%)] Loss: 1.156335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [296960/500434 (59%)] Loss: 1.234807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [307200/500434 (61%)] Loss: 1.415922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [317440/500434 (63%)] Loss: 1.325395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [327680/500434 (65%)] Loss: 1.314437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [337920/500434 (67%)] Loss: 1.259183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [348160/500434 (70%)] Loss: 1.305110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [358400/500434 (72%)] Loss: 1.354733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [368640/500434 (74%)] Loss: 1.303168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [378880/500434 (76%)] Loss: 1.275533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [389120/500434 (78%)] Loss: 1.333910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [399360/500434 (80%)] Loss: 1.357141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [409600/500434 (82%)] Loss: 1.340597\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [419840/500434 (84%)] Loss: 1.327026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [430080/500434 (86%)] Loss: 1.408079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [440320/500434 (88%)] Loss: 1.255948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [450560/500434 (90%)] Loss: 1.344253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [460800/500434 (92%)] Loss: 1.339780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [471040/500434 (94%)] Loss: 1.352931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [481280/500434 (96%)] Loss: 1.284776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 151 [491520/500434 (98%)] Loss: 1.318328\u001b[0m\n",
      "\u001b[34mcurrent epoch: 152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [0/500434 (0%)] Loss: 1.315569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [10240/500434 (2%)] Loss: 1.352167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [20480/500434 (4%)] Loss: 1.300959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [30720/500434 (6%)] Loss: 1.300064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [40960/500434 (8%)] Loss: 1.284871\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [51200/500434 (10%)] Loss: 1.339717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [61440/500434 (12%)] Loss: 1.237380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [71680/500434 (14%)] Loss: 1.184214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [81920/500434 (16%)] Loss: 1.305695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [92160/500434 (18%)] Loss: 1.293993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [102400/500434 (20%)] Loss: 1.276656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [112640/500434 (22%)] Loss: 1.341176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [122880/500434 (25%)] Loss: 1.348718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [133120/500434 (27%)] Loss: 1.248298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [143360/500434 (29%)] Loss: 1.254379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [153600/500434 (31%)] Loss: 1.188416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [163840/500434 (33%)] Loss: 1.403102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [174080/500434 (35%)] Loss: 1.311051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [184320/500434 (37%)] Loss: 1.344909\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [194560/500434 (39%)] Loss: 1.321687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [204800/500434 (41%)] Loss: 1.213093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [215040/500434 (43%)] Loss: 1.267563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [225280/500434 (45%)] Loss: 1.281721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [235520/500434 (47%)] Loss: 1.322858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [245760/500434 (49%)] Loss: 1.338616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [256000/500434 (51%)] Loss: 1.251426\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [266240/500434 (53%)] Loss: 1.348912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [276480/500434 (55%)] Loss: 1.389400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [286720/500434 (57%)] Loss: 1.326669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [296960/500434 (59%)] Loss: 1.301021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [307200/500434 (61%)] Loss: 1.348904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [317440/500434 (63%)] Loss: 1.289728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [327680/500434 (65%)] Loss: 1.319641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [337920/500434 (67%)] Loss: 1.264545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [348160/500434 (70%)] Loss: 1.464919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [358400/500434 (72%)] Loss: 1.403345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [368640/500434 (74%)] Loss: 1.367063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [378880/500434 (76%)] Loss: 1.342937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [389120/500434 (78%)] Loss: 1.290405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [399360/500434 (80%)] Loss: 1.447734\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [409600/500434 (82%)] Loss: 1.291294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [419840/500434 (84%)] Loss: 1.335357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [430080/500434 (86%)] Loss: 1.410605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [440320/500434 (88%)] Loss: 1.275399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [450560/500434 (90%)] Loss: 1.352455\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [460800/500434 (92%)] Loss: 1.415444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [471040/500434 (94%)] Loss: 1.314090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [481280/500434 (96%)] Loss: 1.290603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 152 [491520/500434 (98%)] Loss: 1.227158\u001b[0m\n",
      "\u001b[34mcurrent epoch: 153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [0/500434 (0%)] Loss: 1.260069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [10240/500434 (2%)] Loss: 1.241516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [20480/500434 (4%)] Loss: 1.337039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [30720/500434 (6%)] Loss: 1.680551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [40960/500434 (8%)] Loss: 1.383086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [51200/500434 (10%)] Loss: 1.395305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [61440/500434 (12%)] Loss: 1.347441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [71680/500434 (14%)] Loss: 1.311161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [81920/500434 (16%)] Loss: 1.282631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [92160/500434 (18%)] Loss: 1.381679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [102400/500434 (20%)] Loss: 1.414145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [112640/500434 (22%)] Loss: 1.319501\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [122880/500434 (25%)] Loss: 1.269363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [133120/500434 (27%)] Loss: 1.225170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [143360/500434 (29%)] Loss: 1.339070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [153600/500434 (31%)] Loss: 1.267587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [163840/500434 (33%)] Loss: 1.309745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [174080/500434 (35%)] Loss: 1.469405\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [184320/500434 (37%)] Loss: 1.301665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [194560/500434 (39%)] Loss: 1.355915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [204800/500434 (41%)] Loss: 1.355138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [215040/500434 (43%)] Loss: 1.323913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [225280/500434 (45%)] Loss: 1.253989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [235520/500434 (47%)] Loss: 1.386008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [245760/500434 (49%)] Loss: 1.402612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [256000/500434 (51%)] Loss: 1.312764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [266240/500434 (53%)] Loss: 1.384297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [276480/500434 (55%)] Loss: 1.387906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [286720/500434 (57%)] Loss: 1.283512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [296960/500434 (59%)] Loss: 1.244268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [307200/500434 (61%)] Loss: 1.374937\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [317440/500434 (63%)] Loss: 1.359353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [327680/500434 (65%)] Loss: 1.400934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [337920/500434 (67%)] Loss: 1.285324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [348160/500434 (70%)] Loss: 1.340760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [358400/500434 (72%)] Loss: 1.470181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [368640/500434 (74%)] Loss: 1.401870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [378880/500434 (76%)] Loss: 1.281222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [389120/500434 (78%)] Loss: 1.274868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [399360/500434 (80%)] Loss: 1.371036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [409600/500434 (82%)] Loss: 1.462961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [419840/500434 (84%)] Loss: 1.466505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [430080/500434 (86%)] Loss: 1.405220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [440320/500434 (88%)] Loss: 1.359460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [450560/500434 (90%)] Loss: 1.401806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [460800/500434 (92%)] Loss: 1.416572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [471040/500434 (94%)] Loss: 1.434238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [481280/500434 (96%)] Loss: 1.463255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 153 [491520/500434 (98%)] Loss: 1.170923\u001b[0m\n",
      "\u001b[34mcurrent epoch: 154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [0/500434 (0%)] Loss: 1.276996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [10240/500434 (2%)] Loss: 1.325419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [20480/500434 (4%)] Loss: 1.260175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [30720/500434 (6%)] Loss: 1.361286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [40960/500434 (8%)] Loss: 1.349876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [51200/500434 (10%)] Loss: 1.287037\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [61440/500434 (12%)] Loss: 1.200277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [71680/500434 (14%)] Loss: 1.216062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [81920/500434 (16%)] Loss: 1.215915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [92160/500434 (18%)] Loss: 1.228316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [102400/500434 (20%)] Loss: 1.197714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [112640/500434 (22%)] Loss: 1.307584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [122880/500434 (25%)] Loss: 1.331534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [133120/500434 (27%)] Loss: 1.276066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [143360/500434 (29%)] Loss: 1.318378\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [153600/500434 (31%)] Loss: 1.216085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [163840/500434 (33%)] Loss: 1.344571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [174080/500434 (35%)] Loss: 1.368845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [184320/500434 (37%)] Loss: 1.248005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [194560/500434 (39%)] Loss: 1.419495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [204800/500434 (41%)] Loss: 1.369102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [215040/500434 (43%)] Loss: 1.290125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [225280/500434 (45%)] Loss: 1.329038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [235520/500434 (47%)] Loss: 1.475318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [245760/500434 (49%)] Loss: 1.319083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [256000/500434 (51%)] Loss: 1.337778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [266240/500434 (53%)] Loss: 1.291467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [276480/500434 (55%)] Loss: 1.321473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [286720/500434 (57%)] Loss: 1.265212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [296960/500434 (59%)] Loss: 1.363431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [307200/500434 (61%)] Loss: 1.254703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [317440/500434 (63%)] Loss: 1.329816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [327680/500434 (65%)] Loss: 1.261486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [337920/500434 (67%)] Loss: 1.269107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [348160/500434 (70%)] Loss: 1.287019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [358400/500434 (72%)] Loss: 1.272320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [368640/500434 (74%)] Loss: 1.354483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [378880/500434 (76%)] Loss: 1.375440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [389120/500434 (78%)] Loss: 1.243407\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [399360/500434 (80%)] Loss: 1.283284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [409600/500434 (82%)] Loss: 1.318516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [419840/500434 (84%)] Loss: 1.258852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [430080/500434 (86%)] Loss: 1.300608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [440320/500434 (88%)] Loss: 1.347629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [450560/500434 (90%)] Loss: 1.300604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [460800/500434 (92%)] Loss: 1.271175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [471040/500434 (94%)] Loss: 1.351117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [481280/500434 (96%)] Loss: 1.288262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 154 [491520/500434 (98%)] Loss: 1.270066\u001b[0m\n",
      "\u001b[34mcurrent epoch: 155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [0/500434 (0%)] Loss: 1.319927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [10240/500434 (2%)] Loss: 1.408245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [20480/500434 (4%)] Loss: 1.359012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [30720/500434 (6%)] Loss: 1.284610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [40960/500434 (8%)] Loss: 1.364570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [51200/500434 (10%)] Loss: 1.318215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [61440/500434 (12%)] Loss: 1.296477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [71680/500434 (14%)] Loss: 1.275571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [81920/500434 (16%)] Loss: 1.215960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [92160/500434 (18%)] Loss: 1.290563\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [102400/500434 (20%)] Loss: 1.381012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [112640/500434 (22%)] Loss: 1.385441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [122880/500434 (25%)] Loss: 1.479797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [133120/500434 (27%)] Loss: 1.323761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [143360/500434 (29%)] Loss: 1.236395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [153600/500434 (31%)] Loss: 1.287486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [163840/500434 (33%)] Loss: 1.279804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [174080/500434 (35%)] Loss: 1.341342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [184320/500434 (37%)] Loss: 1.278286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [194560/500434 (39%)] Loss: 1.310067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [204800/500434 (41%)] Loss: 1.300993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [215040/500434 (43%)] Loss: 1.245668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [225280/500434 (45%)] Loss: 1.370513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [235520/500434 (47%)] Loss: 1.269566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [245760/500434 (49%)] Loss: 1.343401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [256000/500434 (51%)] Loss: 1.267965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [266240/500434 (53%)] Loss: 1.371202\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [276480/500434 (55%)] Loss: 1.378321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [286720/500434 (57%)] Loss: 1.483577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [296960/500434 (59%)] Loss: 1.245743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [307200/500434 (61%)] Loss: 1.371081\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [317440/500434 (63%)] Loss: 1.371737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [327680/500434 (65%)] Loss: 1.339698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [337920/500434 (67%)] Loss: 1.223641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [348160/500434 (70%)] Loss: 1.351313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [358400/500434 (72%)] Loss: 1.370123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [368640/500434 (74%)] Loss: 1.275773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [378880/500434 (76%)] Loss: 1.505988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [389120/500434 (78%)] Loss: 1.337163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [399360/500434 (80%)] Loss: 1.260848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [409600/500434 (82%)] Loss: 1.460117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [419840/500434 (84%)] Loss: 1.373236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [430080/500434 (86%)] Loss: 1.296375\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [440320/500434 (88%)] Loss: 1.311648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [450560/500434 (90%)] Loss: 1.349981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [460800/500434 (92%)] Loss: 1.206648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [471040/500434 (94%)] Loss: 1.195971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [481280/500434 (96%)] Loss: 1.217225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 155 [491520/500434 (98%)] Loss: 1.318861\u001b[0m\n",
      "\u001b[34mcurrent epoch: 156\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [0/500434 (0%)] Loss: 1.461474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [10240/500434 (2%)] Loss: 1.311895\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [20480/500434 (4%)] Loss: 1.261645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [30720/500434 (6%)] Loss: 1.253522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [40960/500434 (8%)] Loss: 1.298864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [51200/500434 (10%)] Loss: 1.276102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [61440/500434 (12%)] Loss: 1.319917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [71680/500434 (14%)] Loss: 1.291343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [81920/500434 (16%)] Loss: 1.250234\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [92160/500434 (18%)] Loss: 1.198572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [102400/500434 (20%)] Loss: 1.268040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [112640/500434 (22%)] Loss: 1.310688\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [122880/500434 (25%)] Loss: 1.337545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [133120/500434 (27%)] Loss: 1.267223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [143360/500434 (29%)] Loss: 1.331537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [153600/500434 (31%)] Loss: 1.347828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [163840/500434 (33%)] Loss: 1.303905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [174080/500434 (35%)] Loss: 1.346410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [184320/500434 (37%)] Loss: 1.260955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [194560/500434 (39%)] Loss: 1.344596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [204800/500434 (41%)] Loss: 1.352062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [215040/500434 (43%)] Loss: 1.324025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [225280/500434 (45%)] Loss: 1.269245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [235520/500434 (47%)] Loss: 1.293490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [245760/500434 (49%)] Loss: 1.282708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [256000/500434 (51%)] Loss: 1.376963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [266240/500434 (53%)] Loss: 1.409008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [276480/500434 (55%)] Loss: 1.417976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [286720/500434 (57%)] Loss: 1.306636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [296960/500434 (59%)] Loss: 1.310301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [307200/500434 (61%)] Loss: 1.245849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [317440/500434 (63%)] Loss: 1.368599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [327680/500434 (65%)] Loss: 1.302162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [337920/500434 (67%)] Loss: 1.421434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [348160/500434 (70%)] Loss: 1.385711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [358400/500434 (72%)] Loss: 1.267154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [368640/500434 (74%)] Loss: 1.397289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [378880/500434 (76%)] Loss: 1.296986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [389120/500434 (78%)] Loss: 1.329491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [399360/500434 (80%)] Loss: 1.345111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [409600/500434 (82%)] Loss: 1.336432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [419840/500434 (84%)] Loss: 1.276625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [430080/500434 (86%)] Loss: 1.476941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [440320/500434 (88%)] Loss: 1.410880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [450560/500434 (90%)] Loss: 1.321867\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [460800/500434 (92%)] Loss: 1.322319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [471040/500434 (94%)] Loss: 1.449352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [481280/500434 (96%)] Loss: 1.407045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 156 [491520/500434 (98%)] Loss: 1.238462\u001b[0m\n",
      "\u001b[34mcurrent epoch: 157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [0/500434 (0%)] Loss: 1.370498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [10240/500434 (2%)] Loss: 1.330080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [20480/500434 (4%)] Loss: 1.339804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [30720/500434 (6%)] Loss: 1.309666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [40960/500434 (8%)] Loss: 1.193814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [51200/500434 (10%)] Loss: 1.329532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [61440/500434 (12%)] Loss: 1.280789\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [71680/500434 (14%)] Loss: 1.298945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [81920/500434 (16%)] Loss: 1.379042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [92160/500434 (18%)] Loss: 1.389686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [102400/500434 (20%)] Loss: 1.410763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [112640/500434 (22%)] Loss: 1.278214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [122880/500434 (25%)] Loss: 1.329442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [133120/500434 (27%)] Loss: 1.212189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [143360/500434 (29%)] Loss: 1.437973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [153600/500434 (31%)] Loss: 1.446761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [163840/500434 (33%)] Loss: 1.385901\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [174080/500434 (35%)] Loss: 1.321418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [184320/500434 (37%)] Loss: 1.260875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [194560/500434 (39%)] Loss: 1.349425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [204800/500434 (41%)] Loss: 1.364116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [215040/500434 (43%)] Loss: 1.345567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [225280/500434 (45%)] Loss: 1.355212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [235520/500434 (47%)] Loss: 1.299675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [245760/500434 (49%)] Loss: 1.362327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [256000/500434 (51%)] Loss: 1.352358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [266240/500434 (53%)] Loss: 1.239676\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [276480/500434 (55%)] Loss: 1.371003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [286720/500434 (57%)] Loss: 1.303389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [296960/500434 (59%)] Loss: 1.441450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [307200/500434 (61%)] Loss: 1.386320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [317440/500434 (63%)] Loss: 1.324889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [327680/500434 (65%)] Loss: 1.221438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [337920/500434 (67%)] Loss: 1.350960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [348160/500434 (70%)] Loss: 1.322342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [358400/500434 (72%)] Loss: 1.322471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [368640/500434 (74%)] Loss: 1.286651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [378880/500434 (76%)] Loss: 1.350944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [389120/500434 (78%)] Loss: 1.389478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [399360/500434 (80%)] Loss: 1.302466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [409600/500434 (82%)] Loss: 1.363800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [419840/500434 (84%)] Loss: 1.256445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [430080/500434 (86%)] Loss: 1.262614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [440320/500434 (88%)] Loss: 1.374245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [450560/500434 (90%)] Loss: 1.264240\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [460800/500434 (92%)] Loss: 1.305371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [471040/500434 (94%)] Loss: 1.322710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [481280/500434 (96%)] Loss: 1.277943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 157 [491520/500434 (98%)] Loss: 1.273634\u001b[0m\n",
      "\u001b[34mcurrent epoch: 158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [0/500434 (0%)] Loss: 1.209588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [10240/500434 (2%)] Loss: 1.380068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [20480/500434 (4%)] Loss: 1.419510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [30720/500434 (6%)] Loss: 1.319662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [40960/500434 (8%)] Loss: 1.378482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [51200/500434 (10%)] Loss: 1.443946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [61440/500434 (12%)] Loss: 1.335648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [71680/500434 (14%)] Loss: 1.270381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [81920/500434 (16%)] Loss: 1.338116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [92160/500434 (18%)] Loss: 1.321051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [102400/500434 (20%)] Loss: 1.377582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [112640/500434 (22%)] Loss: 1.277951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [122880/500434 (25%)] Loss: 1.231547\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [133120/500434 (27%)] Loss: 1.375348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [143360/500434 (29%)] Loss: 1.482097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [153600/500434 (31%)] Loss: 1.328554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [163840/500434 (33%)] Loss: 1.359814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [174080/500434 (35%)] Loss: 1.278630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [184320/500434 (37%)] Loss: 1.294008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [194560/500434 (39%)] Loss: 1.286270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [204800/500434 (41%)] Loss: 1.362695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [215040/500434 (43%)] Loss: 1.432828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [225280/500434 (45%)] Loss: 1.309559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [235520/500434 (47%)] Loss: 1.271258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [245760/500434 (49%)] Loss: 1.318640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [256000/500434 (51%)] Loss: 1.311840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [266240/500434 (53%)] Loss: 1.323250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [276480/500434 (55%)] Loss: 1.259823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [286720/500434 (57%)] Loss: 1.430414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [296960/500434 (59%)] Loss: 1.372403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [307200/500434 (61%)] Loss: 1.349213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [317440/500434 (63%)] Loss: 1.375976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [327680/500434 (65%)] Loss: 1.414572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [337920/500434 (67%)] Loss: 1.360324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [348160/500434 (70%)] Loss: 1.326985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [358400/500434 (72%)] Loss: 1.423201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [368640/500434 (74%)] Loss: 1.465420\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [378880/500434 (76%)] Loss: 1.586776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [389120/500434 (78%)] Loss: 1.598013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [399360/500434 (80%)] Loss: 1.563693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [409600/500434 (82%)] Loss: 1.432554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [419840/500434 (84%)] Loss: 1.354671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [430080/500434 (86%)] Loss: 1.485710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [440320/500434 (88%)] Loss: 1.371313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [450560/500434 (90%)] Loss: 1.271767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [460800/500434 (92%)] Loss: 1.376252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [471040/500434 (94%)] Loss: 1.423362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [481280/500434 (96%)] Loss: 1.447190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 158 [491520/500434 (98%)] Loss: 1.358678\u001b[0m\n",
      "\u001b[34mcurrent epoch: 159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [0/500434 (0%)] Loss: 1.489582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [10240/500434 (2%)] Loss: 1.525472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [20480/500434 (4%)] Loss: 1.395137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [30720/500434 (6%)] Loss: 1.312231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [40960/500434 (8%)] Loss: 1.496921\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [51200/500434 (10%)] Loss: 1.393319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [61440/500434 (12%)] Loss: 1.456848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [71680/500434 (14%)] Loss: 1.492013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [81920/500434 (16%)] Loss: 1.392213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [92160/500434 (18%)] Loss: 1.353428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [102400/500434 (20%)] Loss: 1.354093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [112640/500434 (22%)] Loss: 1.231108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [122880/500434 (25%)] Loss: 1.382698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [133120/500434 (27%)] Loss: 1.342551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [143360/500434 (29%)] Loss: 1.298884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [153600/500434 (31%)] Loss: 1.374431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [163840/500434 (33%)] Loss: 1.359416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [174080/500434 (35%)] Loss: 1.224045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [184320/500434 (37%)] Loss: 1.234152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [194560/500434 (39%)] Loss: 1.315347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [204800/500434 (41%)] Loss: 1.406172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [215040/500434 (43%)] Loss: 1.328549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [225280/500434 (45%)] Loss: 1.261737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [235520/500434 (47%)] Loss: 1.374674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [245760/500434 (49%)] Loss: 1.366131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [256000/500434 (51%)] Loss: 1.405208\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [266240/500434 (53%)] Loss: 1.275163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [276480/500434 (55%)] Loss: 1.346602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [286720/500434 (57%)] Loss: 1.330975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [296960/500434 (59%)] Loss: 1.403414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [307200/500434 (61%)] Loss: 1.371421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [317440/500434 (63%)] Loss: 1.464637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [327680/500434 (65%)] Loss: 1.412881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [337920/500434 (67%)] Loss: 1.483825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [348160/500434 (70%)] Loss: 1.368704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [358400/500434 (72%)] Loss: 1.446130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [368640/500434 (74%)] Loss: 1.247089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [378880/500434 (76%)] Loss: 1.370758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [389120/500434 (78%)] Loss: 1.248759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [399360/500434 (80%)] Loss: 1.355598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [409600/500434 (82%)] Loss: 1.359976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [419840/500434 (84%)] Loss: 1.403531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [430080/500434 (86%)] Loss: 1.401415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [440320/500434 (88%)] Loss: 1.411211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [450560/500434 (90%)] Loss: 1.303477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [460800/500434 (92%)] Loss: 1.712655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [471040/500434 (94%)] Loss: 1.456664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [481280/500434 (96%)] Loss: 1.358868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 159 [491520/500434 (98%)] Loss: 1.389174\u001b[0m\n",
      "\u001b[34mcurrent epoch: 160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [0/500434 (0%)] Loss: 1.252436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [10240/500434 (2%)] Loss: 1.388717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [20480/500434 (4%)] Loss: 1.399336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [30720/500434 (6%)] Loss: 1.254490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [40960/500434 (8%)] Loss: 1.270349\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [51200/500434 (10%)] Loss: 1.365594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [61440/500434 (12%)] Loss: 1.216437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [71680/500434 (14%)] Loss: 1.294167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [81920/500434 (16%)] Loss: 1.294072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [92160/500434 (18%)] Loss: 1.347414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [102400/500434 (20%)] Loss: 1.387932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [112640/500434 (22%)] Loss: 1.341516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [122880/500434 (25%)] Loss: 1.328835\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [133120/500434 (27%)] Loss: 1.249123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [143360/500434 (29%)] Loss: 1.379507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [153600/500434 (31%)] Loss: 1.268197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [163840/500434 (33%)] Loss: 1.397346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [174080/500434 (35%)] Loss: 1.355742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [184320/500434 (37%)] Loss: 1.305655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [194560/500434 (39%)] Loss: 1.360960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [204800/500434 (41%)] Loss: 1.227447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [215040/500434 (43%)] Loss: 1.231076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [225280/500434 (45%)] Loss: 1.385572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [235520/500434 (47%)] Loss: 1.373340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [245760/500434 (49%)] Loss: 1.308306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [256000/500434 (51%)] Loss: 1.316342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [266240/500434 (53%)] Loss: 1.293248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [276480/500434 (55%)] Loss: 1.384780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [286720/500434 (57%)] Loss: 1.510561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [296960/500434 (59%)] Loss: 1.418690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [307200/500434 (61%)] Loss: 1.262852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [317440/500434 (63%)] Loss: 1.239600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [327680/500434 (65%)] Loss: 1.390671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [337920/500434 (67%)] Loss: 1.360647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [348160/500434 (70%)] Loss: 1.400527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [358400/500434 (72%)] Loss: 1.375997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [368640/500434 (74%)] Loss: 1.269970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [378880/500434 (76%)] Loss: 1.269722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [389120/500434 (78%)] Loss: 1.274342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [399360/500434 (80%)] Loss: 1.379935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [409600/500434 (82%)] Loss: 1.355879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [419840/500434 (84%)] Loss: 1.384578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [430080/500434 (86%)] Loss: 1.280379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [440320/500434 (88%)] Loss: 1.203675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [450560/500434 (90%)] Loss: 1.328108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [460800/500434 (92%)] Loss: 1.389717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [471040/500434 (94%)] Loss: 1.375964\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [481280/500434 (96%)] Loss: 1.290189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 160 [491520/500434 (98%)] Loss: 1.323022\u001b[0m\n",
      "\u001b[34mcurrent epoch: 161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [0/500434 (0%)] Loss: 1.289457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [10240/500434 (2%)] Loss: 1.341727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [20480/500434 (4%)] Loss: 1.354994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [30720/500434 (6%)] Loss: 1.264450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [40960/500434 (8%)] Loss: 1.388140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [51200/500434 (10%)] Loss: 1.228913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [61440/500434 (12%)] Loss: 1.318434\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [71680/500434 (14%)] Loss: 1.162842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [81920/500434 (16%)] Loss: 1.407509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [92160/500434 (18%)] Loss: 1.413007\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [102400/500434 (20%)] Loss: 1.342300\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [112640/500434 (22%)] Loss: 1.338512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [122880/500434 (25%)] Loss: 1.281683\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [133120/500434 (27%)] Loss: 1.344047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [143360/500434 (29%)] Loss: 1.232497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [153600/500434 (31%)] Loss: 1.389715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [163840/500434 (33%)] Loss: 1.394163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [174080/500434 (35%)] Loss: 1.451711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [184320/500434 (37%)] Loss: 1.407408\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [194560/500434 (39%)] Loss: 1.235059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [204800/500434 (41%)] Loss: 1.412333\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [215040/500434 (43%)] Loss: 1.297184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [225280/500434 (45%)] Loss: 1.220541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [235520/500434 (47%)] Loss: 1.284615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [245760/500434 (49%)] Loss: 1.366189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [256000/500434 (51%)] Loss: 1.360227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [266240/500434 (53%)] Loss: 1.172747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [276480/500434 (55%)] Loss: 1.268211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [286720/500434 (57%)] Loss: 1.260552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [296960/500434 (59%)] Loss: 1.397920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [307200/500434 (61%)] Loss: 1.221204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [317440/500434 (63%)] Loss: 1.213669\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [327680/500434 (65%)] Loss: 1.392572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [337920/500434 (67%)] Loss: 1.296777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [348160/500434 (70%)] Loss: 1.471906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [358400/500434 (72%)] Loss: 1.399164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [368640/500434 (74%)] Loss: 1.335402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [378880/500434 (76%)] Loss: 1.362128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [389120/500434 (78%)] Loss: 1.379062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [399360/500434 (80%)] Loss: 1.236562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [409600/500434 (82%)] Loss: 1.433044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [419840/500434 (84%)] Loss: 1.307311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [430080/500434 (86%)] Loss: 1.284373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [440320/500434 (88%)] Loss: 1.375250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [450560/500434 (90%)] Loss: 1.322398\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [460800/500434 (92%)] Loss: 1.317991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [471040/500434 (94%)] Loss: 1.291480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [481280/500434 (96%)] Loss: 1.333770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 161 [491520/500434 (98%)] Loss: 1.343589\u001b[0m\n",
      "\u001b[34mcurrent epoch: 162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [0/500434 (0%)] Loss: 1.361800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [10240/500434 (2%)] Loss: 1.258627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [20480/500434 (4%)] Loss: 1.284021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [30720/500434 (6%)] Loss: 1.186410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [40960/500434 (8%)] Loss: 1.360801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [51200/500434 (10%)] Loss: 1.233699\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [61440/500434 (12%)] Loss: 1.271759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [71680/500434 (14%)] Loss: 1.280548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [81920/500434 (16%)] Loss: 1.208140\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [92160/500434 (18%)] Loss: 1.380410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [102400/500434 (20%)] Loss: 1.366382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [112640/500434 (22%)] Loss: 1.302150\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [122880/500434 (25%)] Loss: 1.347432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [133120/500434 (27%)] Loss: 1.346963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [143360/500434 (29%)] Loss: 1.295604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [153600/500434 (31%)] Loss: 1.411052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [163840/500434 (33%)] Loss: 1.268862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [174080/500434 (35%)] Loss: 1.379908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [184320/500434 (37%)] Loss: 1.309996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [194560/500434 (39%)] Loss: 1.277451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [204800/500434 (41%)] Loss: 1.366442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [215040/500434 (43%)] Loss: 1.372404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [225280/500434 (45%)] Loss: 1.299892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [235520/500434 (47%)] Loss: 1.389823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [245760/500434 (49%)] Loss: 1.355399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [256000/500434 (51%)] Loss: 1.420185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [266240/500434 (53%)] Loss: 1.320812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [276480/500434 (55%)] Loss: 1.270023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [286720/500434 (57%)] Loss: 1.471091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [296960/500434 (59%)] Loss: 1.383120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [307200/500434 (61%)] Loss: 1.356700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [317440/500434 (63%)] Loss: 1.280780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [327680/500434 (65%)] Loss: 1.408296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [337920/500434 (67%)] Loss: 1.332748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [348160/500434 (70%)] Loss: 1.331286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [358400/500434 (72%)] Loss: 1.234729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [368640/500434 (74%)] Loss: 1.320495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [378880/500434 (76%)] Loss: 1.186302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [389120/500434 (78%)] Loss: 1.243568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [399360/500434 (80%)] Loss: 1.304508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [409600/500434 (82%)] Loss: 1.279713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [419840/500434 (84%)] Loss: 1.258198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [430080/500434 (86%)] Loss: 1.403261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [440320/500434 (88%)] Loss: 1.256172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [450560/500434 (90%)] Loss: 1.342704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [460800/500434 (92%)] Loss: 1.321325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [471040/500434 (94%)] Loss: 1.374488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [481280/500434 (96%)] Loss: 1.249193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 162 [491520/500434 (98%)] Loss: 1.309448\u001b[0m\n",
      "\u001b[34mcurrent epoch: 163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [0/500434 (0%)] Loss: 1.249053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [10240/500434 (2%)] Loss: 1.155167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [20480/500434 (4%)] Loss: 1.323671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [30720/500434 (6%)] Loss: 1.276449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [40960/500434 (8%)] Loss: 1.360008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [51200/500434 (10%)] Loss: 1.297860\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [61440/500434 (12%)] Loss: 1.457976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [71680/500434 (14%)] Loss: 1.298928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [81920/500434 (16%)] Loss: 1.297798\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [92160/500434 (18%)] Loss: 1.336837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [102400/500434 (20%)] Loss: 1.300403\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [112640/500434 (22%)] Loss: 1.417863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [122880/500434 (25%)] Loss: 1.297469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [133120/500434 (27%)] Loss: 1.345682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [143360/500434 (29%)] Loss: 1.289631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [153600/500434 (31%)] Loss: 1.402471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [163840/500434 (33%)] Loss: 1.339298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [174080/500434 (35%)] Loss: 1.357276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [184320/500434 (37%)] Loss: 1.305229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [194560/500434 (39%)] Loss: 1.283219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [204800/500434 (41%)] Loss: 1.201684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [215040/500434 (43%)] Loss: 1.257729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [225280/500434 (45%)] Loss: 1.206038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [235520/500434 (47%)] Loss: 1.303852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [245760/500434 (49%)] Loss: 1.369708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [256000/500434 (51%)] Loss: 1.402171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [266240/500434 (53%)] Loss: 1.411115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [276480/500434 (55%)] Loss: 1.381507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [286720/500434 (57%)] Loss: 1.306671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [296960/500434 (59%)] Loss: 1.406852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [307200/500434 (61%)] Loss: 1.299886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [317440/500434 (63%)] Loss: 1.293187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [327680/500434 (65%)] Loss: 1.260453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [337920/500434 (67%)] Loss: 1.388744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [348160/500434 (70%)] Loss: 1.256704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [358400/500434 (72%)] Loss: 1.289655\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [368640/500434 (74%)] Loss: 1.399922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [378880/500434 (76%)] Loss: 1.246224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [389120/500434 (78%)] Loss: 1.312312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [399360/500434 (80%)] Loss: 1.378873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [409600/500434 (82%)] Loss: 1.255807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [419840/500434 (84%)] Loss: 1.287249\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [430080/500434 (86%)] Loss: 1.285997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [440320/500434 (88%)] Loss: 1.328023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [450560/500434 (90%)] Loss: 1.331159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [460800/500434 (92%)] Loss: 1.225097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [471040/500434 (94%)] Loss: 1.352985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [481280/500434 (96%)] Loss: 1.284471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 163 [491520/500434 (98%)] Loss: 1.289153\u001b[0m\n",
      "\u001b[34mcurrent epoch: 164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [0/500434 (0%)] Loss: 1.229413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [10240/500434 (2%)] Loss: 1.179847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [20480/500434 (4%)] Loss: 1.301230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [30720/500434 (6%)] Loss: 1.352816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [40960/500434 (8%)] Loss: 1.278124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [51200/500434 (10%)] Loss: 1.293357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [61440/500434 (12%)] Loss: 1.242831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [71680/500434 (14%)] Loss: 1.359496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [81920/500434 (16%)] Loss: 1.318063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [92160/500434 (18%)] Loss: 1.265646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [102400/500434 (20%)] Loss: 1.193283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [112640/500434 (22%)] Loss: 1.213569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [122880/500434 (25%)] Loss: 1.398893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [133120/500434 (27%)] Loss: 1.363154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [143360/500434 (29%)] Loss: 1.161002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [153600/500434 (31%)] Loss: 1.346210\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [163840/500434 (33%)] Loss: 1.218502\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [174080/500434 (35%)] Loss: 1.338611\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [184320/500434 (37%)] Loss: 1.302810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [194560/500434 (39%)] Loss: 1.382019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [204800/500434 (41%)] Loss: 1.311457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [215040/500434 (43%)] Loss: 1.269374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [225280/500434 (45%)] Loss: 1.339453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [235520/500434 (47%)] Loss: 1.271011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [245760/500434 (49%)] Loss: 1.291548\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [256000/500434 (51%)] Loss: 1.622387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [266240/500434 (53%)] Loss: 1.495453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [276480/500434 (55%)] Loss: 1.451157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [286720/500434 (57%)] Loss: 1.493011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [296960/500434 (59%)] Loss: 1.462681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [307200/500434 (61%)] Loss: 1.476242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [317440/500434 (63%)] Loss: 1.498260\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [327680/500434 (65%)] Loss: 1.448534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [337920/500434 (67%)] Loss: 1.392822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [348160/500434 (70%)] Loss: 1.448014\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [358400/500434 (72%)] Loss: 1.477248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [368640/500434 (74%)] Loss: 1.326991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [378880/500434 (76%)] Loss: 1.357729\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [389120/500434 (78%)] Loss: 1.336761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [399360/500434 (80%)] Loss: 1.505599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [409600/500434 (82%)] Loss: 1.358030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [419840/500434 (84%)] Loss: 1.260114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [430080/500434 (86%)] Loss: 1.350677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [440320/500434 (88%)] Loss: 1.324181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [450560/500434 (90%)] Loss: 1.328954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [460800/500434 (92%)] Loss: 1.391468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [471040/500434 (94%)] Loss: 1.344104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [481280/500434 (96%)] Loss: 1.292823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 164 [491520/500434 (98%)] Loss: 1.396750\u001b[0m\n",
      "\u001b[34mcurrent epoch: 165\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [0/500434 (0%)] Loss: 1.251806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [10240/500434 (2%)] Loss: 1.328445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [20480/500434 (4%)] Loss: 1.339399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [30720/500434 (6%)] Loss: 1.386442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [40960/500434 (8%)] Loss: 1.369741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [51200/500434 (10%)] Loss: 1.433697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [61440/500434 (12%)] Loss: 1.280042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [71680/500434 (14%)] Loss: 1.333336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [81920/500434 (16%)] Loss: 1.282831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [92160/500434 (18%)] Loss: 1.309665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [102400/500434 (20%)] Loss: 1.265356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [112640/500434 (22%)] Loss: 1.269097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [122880/500434 (25%)] Loss: 1.285090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [133120/500434 (27%)] Loss: 1.308366\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [143360/500434 (29%)] Loss: 1.317070\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [153600/500434 (31%)] Loss: 1.346294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [163840/500434 (33%)] Loss: 1.278992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [174080/500434 (35%)] Loss: 1.267875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [184320/500434 (37%)] Loss: 1.254449\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [194560/500434 (39%)] Loss: 1.309911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [204800/500434 (41%)] Loss: 1.266418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [215040/500434 (43%)] Loss: 1.224036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [225280/500434 (45%)] Loss: 1.320526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [235520/500434 (47%)] Loss: 1.363008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [245760/500434 (49%)] Loss: 1.300689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [256000/500434 (51%)] Loss: 1.308341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [266240/500434 (53%)] Loss: 1.349620\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [276480/500434 (55%)] Loss: 1.337394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [286720/500434 (57%)] Loss: 1.290907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [296960/500434 (59%)] Loss: 1.413464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [307200/500434 (61%)] Loss: 1.360850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [317440/500434 (63%)] Loss: 1.249705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [327680/500434 (65%)] Loss: 1.306559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [337920/500434 (67%)] Loss: 1.264238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [348160/500434 (70%)] Loss: 1.329331\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [358400/500434 (72%)] Loss: 1.268456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [368640/500434 (74%)] Loss: 1.246646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [378880/500434 (76%)] Loss: 1.313154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [389120/500434 (78%)] Loss: 1.219404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [399360/500434 (80%)] Loss: 1.267351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [409600/500434 (82%)] Loss: 1.287341\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [419840/500434 (84%)] Loss: 1.339730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [430080/500434 (86%)] Loss: 1.247098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [440320/500434 (88%)] Loss: 1.411131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [450560/500434 (90%)] Loss: 1.397009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [460800/500434 (92%)] Loss: 1.267743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [471040/500434 (94%)] Loss: 1.163681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [481280/500434 (96%)] Loss: 1.190815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 165 [491520/500434 (98%)] Loss: 1.206529\u001b[0m\n",
      "\u001b[34mcurrent epoch: 166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [0/500434 (0%)] Loss: 1.431958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [10240/500434 (2%)] Loss: 1.224714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [20480/500434 (4%)] Loss: 1.191619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [30720/500434 (6%)] Loss: 1.231543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [40960/500434 (8%)] Loss: 1.283026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [51200/500434 (10%)] Loss: 1.283125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [61440/500434 (12%)] Loss: 1.230261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [71680/500434 (14%)] Loss: 1.182121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [81920/500434 (16%)] Loss: 1.208507\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [92160/500434 (18%)] Loss: 1.210858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [102400/500434 (20%)] Loss: 1.326371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [112640/500434 (22%)] Loss: 1.237186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [122880/500434 (25%)] Loss: 1.300874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [133120/500434 (27%)] Loss: 1.267113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [143360/500434 (29%)] Loss: 1.337750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [153600/500434 (31%)] Loss: 1.199096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [163840/500434 (33%)] Loss: 1.244553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [174080/500434 (35%)] Loss: 1.323677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [184320/500434 (37%)] Loss: 1.402090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [194560/500434 (39%)] Loss: 1.331550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [204800/500434 (41%)] Loss: 1.366724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [215040/500434 (43%)] Loss: 1.248955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [225280/500434 (45%)] Loss: 1.186870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [235520/500434 (47%)] Loss: 1.257708\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [245760/500434 (49%)] Loss: 1.332778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [256000/500434 (51%)] Loss: 1.264352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [266240/500434 (53%)] Loss: 1.227656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [276480/500434 (55%)] Loss: 1.237340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [286720/500434 (57%)] Loss: 1.309113\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [296960/500434 (59%)] Loss: 1.340736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [307200/500434 (61%)] Loss: 1.230945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [317440/500434 (63%)] Loss: 1.360546\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [327680/500434 (65%)] Loss: 1.224770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [337920/500434 (67%)] Loss: 1.231044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [348160/500434 (70%)] Loss: 1.338689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [358400/500434 (72%)] Loss: 1.265977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [368640/500434 (74%)] Loss: 1.242638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [378880/500434 (76%)] Loss: 1.374662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [389120/500434 (78%)] Loss: 1.313500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [399360/500434 (80%)] Loss: 1.223157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [409600/500434 (82%)] Loss: 1.145519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [419840/500434 (84%)] Loss: 1.311741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [430080/500434 (86%)] Loss: 1.341036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [440320/500434 (88%)] Loss: 1.266945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [450560/500434 (90%)] Loss: 1.255354\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [460800/500434 (92%)] Loss: 1.235385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [471040/500434 (94%)] Loss: 1.214717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [481280/500434 (96%)] Loss: 1.263206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 166 [491520/500434 (98%)] Loss: 1.385257\u001b[0m\n",
      "\u001b[34mcurrent epoch: 167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [0/500434 (0%)] Loss: 1.290351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [10240/500434 (2%)] Loss: 1.265172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [20480/500434 (4%)] Loss: 1.342643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [30720/500434 (6%)] Loss: 1.367493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [40960/500434 (8%)] Loss: 1.289294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [51200/500434 (10%)] Loss: 1.328508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [61440/500434 (12%)] Loss: 1.378248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [71680/500434 (14%)] Loss: 1.213645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [81920/500434 (16%)] Loss: 1.295614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [92160/500434 (18%)] Loss: 1.266486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [102400/500434 (20%)] Loss: 1.239931\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [112640/500434 (22%)] Loss: 1.373411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [122880/500434 (25%)] Loss: 1.280912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [133120/500434 (27%)] Loss: 1.280516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [143360/500434 (29%)] Loss: 1.343898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [153600/500434 (31%)] Loss: 1.247526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [163840/500434 (33%)] Loss: 1.246900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [174080/500434 (35%)] Loss: 1.211917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [184320/500434 (37%)] Loss: 1.332678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [194560/500434 (39%)] Loss: 1.298001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [204800/500434 (41%)] Loss: 1.316137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [215040/500434 (43%)] Loss: 1.198599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [225280/500434 (45%)] Loss: 1.207075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [235520/500434 (47%)] Loss: 1.236161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [245760/500434 (49%)] Loss: 1.211400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [256000/500434 (51%)] Loss: 1.264985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [266240/500434 (53%)] Loss: 1.229638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [276480/500434 (55%)] Loss: 1.269340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [286720/500434 (57%)] Loss: 1.268080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [296960/500434 (59%)] Loss: 1.301274\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [307200/500434 (61%)] Loss: 1.349416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [317440/500434 (63%)] Loss: 1.339831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [327680/500434 (65%)] Loss: 1.316253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [337920/500434 (67%)] Loss: 1.298138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [348160/500434 (70%)] Loss: 1.215064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [358400/500434 (72%)] Loss: 1.282768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [368640/500434 (74%)] Loss: 1.274385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [378880/500434 (76%)] Loss: 1.257987\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [389120/500434 (78%)] Loss: 1.282087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [399360/500434 (80%)] Loss: 1.213283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [409600/500434 (82%)] Loss: 1.359684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [419840/500434 (84%)] Loss: 1.313199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [430080/500434 (86%)] Loss: 1.267544\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [440320/500434 (88%)] Loss: 1.283515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [450560/500434 (90%)] Loss: 1.223487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [460800/500434 (92%)] Loss: 1.281910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [471040/500434 (94%)] Loss: 1.229268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [481280/500434 (96%)] Loss: 1.335244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 167 [491520/500434 (98%)] Loss: 1.232697\u001b[0m\n",
      "\u001b[34mcurrent epoch: 168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [0/500434 (0%)] Loss: 1.208495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [10240/500434 (2%)] Loss: 1.279617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [20480/500434 (4%)] Loss: 1.283153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [30720/500434 (6%)] Loss: 1.236343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [40960/500434 (8%)] Loss: 1.274961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [51200/500434 (10%)] Loss: 1.234945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [61440/500434 (12%)] Loss: 1.361368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [71680/500434 (14%)] Loss: 1.254841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [81920/500434 (16%)] Loss: 1.241687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [92160/500434 (18%)] Loss: 1.190488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [102400/500434 (20%)] Loss: 1.350450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [112640/500434 (22%)] Loss: 1.224830\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [122880/500434 (25%)] Loss: 1.180273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [133120/500434 (27%)] Loss: 1.237324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [143360/500434 (29%)] Loss: 1.236417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [153600/500434 (31%)] Loss: 1.239080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [163840/500434 (33%)] Loss: 1.342440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [174080/500434 (35%)] Loss: 1.249914\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [184320/500434 (37%)] Loss: 1.195539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [194560/500434 (39%)] Loss: 1.280498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [204800/500434 (41%)] Loss: 1.276361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [215040/500434 (43%)] Loss: 1.257967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [225280/500434 (45%)] Loss: 1.265446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [235520/500434 (47%)] Loss: 1.195821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [245760/500434 (49%)] Loss: 1.251404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [256000/500434 (51%)] Loss: 1.320424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [266240/500434 (53%)] Loss: 1.299875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [276480/500434 (55%)] Loss: 1.317006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [286720/500434 (57%)] Loss: 1.234904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [296960/500434 (59%)] Loss: 1.275615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [307200/500434 (61%)] Loss: 1.210256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [317440/500434 (63%)] Loss: 1.280371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [327680/500434 (65%)] Loss: 1.232815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [337920/500434 (67%)] Loss: 1.287839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [348160/500434 (70%)] Loss: 1.241604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [358400/500434 (72%)] Loss: 1.285577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [368640/500434 (74%)] Loss: 1.273910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [378880/500434 (76%)] Loss: 1.147668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [389120/500434 (78%)] Loss: 1.118393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [399360/500434 (80%)] Loss: 1.254111\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [409600/500434 (82%)] Loss: 1.399286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [419840/500434 (84%)] Loss: 1.304858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [430080/500434 (86%)] Loss: 1.220883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [440320/500434 (88%)] Loss: 1.263400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [450560/500434 (90%)] Loss: 1.415303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [460800/500434 (92%)] Loss: 1.355258\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [471040/500434 (94%)] Loss: 1.235325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [481280/500434 (96%)] Loss: 1.273703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 168 [491520/500434 (98%)] Loss: 1.343743\u001b[0m\n",
      "\u001b[34mcurrent epoch: 169\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [0/500434 (0%)] Loss: 1.184670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [10240/500434 (2%)] Loss: 1.274704\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [20480/500434 (4%)] Loss: 1.280091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [30720/500434 (6%)] Loss: 1.208093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [40960/500434 (8%)] Loss: 1.254842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [51200/500434 (10%)] Loss: 1.241103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [61440/500434 (12%)] Loss: 1.307948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [71680/500434 (14%)] Loss: 1.273963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [81920/500434 (16%)] Loss: 1.143427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [92160/500434 (18%)] Loss: 1.261273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [102400/500434 (20%)] Loss: 1.186772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [112640/500434 (22%)] Loss: 1.253309\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [122880/500434 (25%)] Loss: 1.254191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [133120/500434 (27%)] Loss: 1.268722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [143360/500434 (29%)] Loss: 1.316999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [153600/500434 (31%)] Loss: 1.230357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [163840/500434 (33%)] Loss: 1.165163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [174080/500434 (35%)] Loss: 1.252786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [184320/500434 (37%)] Loss: 1.351616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [194560/500434 (39%)] Loss: 1.289346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [204800/500434 (41%)] Loss: 1.210238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [215040/500434 (43%)] Loss: 1.274308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [225280/500434 (45%)] Loss: 1.224748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [235520/500434 (47%)] Loss: 1.241741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [245760/500434 (49%)] Loss: 1.309347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [256000/500434 (51%)] Loss: 1.214056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [266240/500434 (53%)] Loss: 1.358988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [276480/500434 (55%)] Loss: 1.170138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [286720/500434 (57%)] Loss: 1.231707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [296960/500434 (59%)] Loss: 1.311130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [307200/500434 (61%)] Loss: 1.341893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [317440/500434 (63%)] Loss: 1.261873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [327680/500434 (65%)] Loss: 1.168772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [337920/500434 (67%)] Loss: 1.237568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [348160/500434 (70%)] Loss: 1.349321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [358400/500434 (72%)] Loss: 1.290816\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [368640/500434 (74%)] Loss: 1.308571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [378880/500434 (76%)] Loss: 1.326595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [389120/500434 (78%)] Loss: 1.237301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [399360/500434 (80%)] Loss: 1.341106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [409600/500434 (82%)] Loss: 1.281396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [419840/500434 (84%)] Loss: 1.258773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [430080/500434 (86%)] Loss: 1.166263\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [440320/500434 (88%)] Loss: 1.338795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [450560/500434 (90%)] Loss: 1.272311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [460800/500434 (92%)] Loss: 1.268069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [471040/500434 (94%)] Loss: 1.333276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [481280/500434 (96%)] Loss: 1.275000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 169 [491520/500434 (98%)] Loss: 1.145749\u001b[0m\n",
      "\u001b[34mcurrent epoch: 170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [0/500434 (0%)] Loss: 1.246218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [10240/500434 (2%)] Loss: 1.213644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [20480/500434 (4%)] Loss: 1.231545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [30720/500434 (6%)] Loss: 1.213629\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [40960/500434 (8%)] Loss: 1.240884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [51200/500434 (10%)] Loss: 1.160553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [61440/500434 (12%)] Loss: 1.265468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [71680/500434 (14%)] Loss: 1.285439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [81920/500434 (16%)] Loss: 1.279276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [92160/500434 (18%)] Loss: 1.267947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [102400/500434 (20%)] Loss: 1.289829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [112640/500434 (22%)] Loss: 1.206642\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [122880/500434 (25%)] Loss: 1.253387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [133120/500434 (27%)] Loss: 1.227358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [143360/500434 (29%)] Loss: 1.246535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [153600/500434 (31%)] Loss: 1.229778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [163840/500434 (33%)] Loss: 1.265714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [174080/500434 (35%)] Loss: 1.256588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [184320/500434 (37%)] Loss: 1.131103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [194560/500434 (39%)] Loss: 1.228916\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [204800/500434 (41%)] Loss: 1.288110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [215040/500434 (43%)] Loss: 1.240039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [225280/500434 (45%)] Loss: 1.267001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [235520/500434 (47%)] Loss: 1.276162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [245760/500434 (49%)] Loss: 1.290934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [256000/500434 (51%)] Loss: 1.253254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [266240/500434 (53%)] Loss: 1.337698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [276480/500434 (55%)] Loss: 1.200148\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [286720/500434 (57%)] Loss: 1.280978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [296960/500434 (59%)] Loss: 1.296713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [307200/500434 (61%)] Loss: 1.267633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [317440/500434 (63%)] Loss: 1.176031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [327680/500434 (65%)] Loss: 1.243517\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [337920/500434 (67%)] Loss: 1.158516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [348160/500434 (70%)] Loss: 1.275723\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [358400/500434 (72%)] Loss: 1.317639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [368640/500434 (74%)] Loss: 1.215025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [378880/500434 (76%)] Loss: 1.231731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [389120/500434 (78%)] Loss: 1.219043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [399360/500434 (80%)] Loss: 1.179339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [409600/500434 (82%)] Loss: 1.286932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [419840/500434 (84%)] Loss: 1.246454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [430080/500434 (86%)] Loss: 1.218359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [440320/500434 (88%)] Loss: 1.287555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [450560/500434 (90%)] Loss: 1.272858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [460800/500434 (92%)] Loss: 1.369595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [471040/500434 (94%)] Loss: 1.298096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [481280/500434 (96%)] Loss: 1.235717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 170 [491520/500434 (98%)] Loss: 1.158892\u001b[0m\n",
      "\u001b[34mcurrent epoch: 171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [0/500434 (0%)] Loss: 1.405461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [10240/500434 (2%)] Loss: 1.179269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [20480/500434 (4%)] Loss: 1.150725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [30720/500434 (6%)] Loss: 1.331177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [40960/500434 (8%)] Loss: 1.287032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [51200/500434 (10%)] Loss: 1.255714\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [61440/500434 (12%)] Loss: 1.499083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [71680/500434 (14%)] Loss: 1.238157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [81920/500434 (16%)] Loss: 1.232576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [92160/500434 (18%)] Loss: 1.077464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [102400/500434 (20%)] Loss: 1.274837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [112640/500434 (22%)] Loss: 1.210459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [122880/500434 (25%)] Loss: 1.234981\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [133120/500434 (27%)] Loss: 1.262050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [143360/500434 (29%)] Loss: 1.248022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [153600/500434 (31%)] Loss: 1.251910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [163840/500434 (33%)] Loss: 1.206370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [174080/500434 (35%)] Loss: 1.136536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [184320/500434 (37%)] Loss: 1.218320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [194560/500434 (39%)] Loss: 1.238042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [204800/500434 (41%)] Loss: 1.224412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [215040/500434 (43%)] Loss: 1.309083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [225280/500434 (45%)] Loss: 1.282211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [235520/500434 (47%)] Loss: 1.272543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [245760/500434 (49%)] Loss: 1.281221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [256000/500434 (51%)] Loss: 1.201305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [266240/500434 (53%)] Loss: 1.283848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [276480/500434 (55%)] Loss: 1.266454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [286720/500434 (57%)] Loss: 1.219955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [296960/500434 (59%)] Loss: 1.268889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [307200/500434 (61%)] Loss: 1.186308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [317440/500434 (63%)] Loss: 1.229662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [327680/500434 (65%)] Loss: 1.264701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [337920/500434 (67%)] Loss: 1.212607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [348160/500434 (70%)] Loss: 1.290638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [358400/500434 (72%)] Loss: 1.306433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [368640/500434 (74%)] Loss: 1.241781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [378880/500434 (76%)] Loss: 1.210871\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [389120/500434 (78%)] Loss: 1.230934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [399360/500434 (80%)] Loss: 1.186625\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [409600/500434 (82%)] Loss: 1.164715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [419840/500434 (84%)] Loss: 1.195965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [430080/500434 (86%)] Loss: 1.147447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [440320/500434 (88%)] Loss: 1.230040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [450560/500434 (90%)] Loss: 1.213966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [460800/500434 (92%)] Loss: 1.222539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [471040/500434 (94%)] Loss: 1.238139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [481280/500434 (96%)] Loss: 1.258052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 171 [491520/500434 (98%)] Loss: 1.234878\u001b[0m\n",
      "\u001b[34mcurrent epoch: 172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [0/500434 (0%)] Loss: 1.133168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [10240/500434 (2%)] Loss: 1.165927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [20480/500434 (4%)] Loss: 1.266067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [30720/500434 (6%)] Loss: 1.262314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [40960/500434 (8%)] Loss: 1.202756\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [51200/500434 (10%)] Loss: 1.238418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [61440/500434 (12%)] Loss: 1.291110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [71680/500434 (14%)] Loss: 1.250774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [81920/500434 (16%)] Loss: 1.190649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [92160/500434 (18%)] Loss: 1.280193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [102400/500434 (20%)] Loss: 1.248155\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [112640/500434 (22%)] Loss: 1.252874\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [122880/500434 (25%)] Loss: 1.262769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [133120/500434 (27%)] Loss: 1.213352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [143360/500434 (29%)] Loss: 1.186458\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [153600/500434 (31%)] Loss: 1.219485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [163840/500434 (33%)] Loss: 1.104524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [174080/500434 (35%)] Loss: 1.121193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [184320/500434 (37%)] Loss: 1.349124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [194560/500434 (39%)] Loss: 1.142516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [204800/500434 (41%)] Loss: 1.143013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [215040/500434 (43%)] Loss: 1.258278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [225280/500434 (45%)] Loss: 1.295783\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [235520/500434 (47%)] Loss: 1.214635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [245760/500434 (49%)] Loss: 1.141910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [256000/500434 (51%)] Loss: 1.262850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [266240/500434 (53%)] Loss: 1.191856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [276480/500434 (55%)] Loss: 1.238510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [286720/500434 (57%)] Loss: 1.267180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [296960/500434 (59%)] Loss: 1.235877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [307200/500434 (61%)] Loss: 1.204211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [317440/500434 (63%)] Loss: 1.163951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [327680/500434 (65%)] Loss: 1.245457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [337920/500434 (67%)] Loss: 1.217644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [348160/500434 (70%)] Loss: 1.334640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [358400/500434 (72%)] Loss: 1.314237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [368640/500434 (74%)] Loss: 1.167318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [378880/500434 (76%)] Loss: 1.279350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [389120/500434 (78%)] Loss: 1.233119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [399360/500434 (80%)] Loss: 1.272763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [409600/500434 (82%)] Loss: 1.228628\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [419840/500434 (84%)] Loss: 1.280514\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [430080/500434 (86%)] Loss: 1.187925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [440320/500434 (88%)] Loss: 1.184638\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [450560/500434 (90%)] Loss: 1.316605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [460800/500434 (92%)] Loss: 1.241842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [471040/500434 (94%)] Loss: 1.101387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [481280/500434 (96%)] Loss: 1.261692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 172 [491520/500434 (98%)] Loss: 1.246835\u001b[0m\n",
      "\u001b[34mcurrent epoch: 173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [0/500434 (0%)] Loss: 1.292086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [10240/500434 (2%)] Loss: 1.207508\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [20480/500434 (4%)] Loss: 1.319852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [30720/500434 (6%)] Loss: 1.251647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [40960/500434 (8%)] Loss: 1.180933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [51200/500434 (10%)] Loss: 1.198273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [61440/500434 (12%)] Loss: 1.222253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [71680/500434 (14%)] Loss: 1.230648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [81920/500434 (16%)] Loss: 1.184913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [92160/500434 (18%)] Loss: 1.190589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [102400/500434 (20%)] Loss: 1.372430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [112640/500434 (22%)] Loss: 1.249483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [122880/500434 (25%)] Loss: 1.281839\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [133120/500434 (27%)] Loss: 1.283781\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [143360/500434 (29%)] Loss: 1.251983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [153600/500434 (31%)] Loss: 1.119369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [163840/500434 (33%)] Loss: 1.285099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [174080/500434 (35%)] Loss: 1.184089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [184320/500434 (37%)] Loss: 1.207571\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [194560/500434 (39%)] Loss: 1.303212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [204800/500434 (41%)] Loss: 1.183228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [215040/500434 (43%)] Loss: 1.254853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [225280/500434 (45%)] Loss: 1.233933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [235520/500434 (47%)] Loss: 1.245097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [245760/500434 (49%)] Loss: 1.244561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [256000/500434 (51%)] Loss: 1.222857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [266240/500434 (53%)] Loss: 1.230033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [276480/500434 (55%)] Loss: 1.299362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [286720/500434 (57%)] Loss: 1.166944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [296960/500434 (59%)] Loss: 1.218128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [307200/500434 (61%)] Loss: 1.254766\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [317440/500434 (63%)] Loss: 1.290670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [327680/500434 (65%)] Loss: 1.252259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [337920/500434 (67%)] Loss: 1.338870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [348160/500434 (70%)] Loss: 1.246261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [358400/500434 (72%)] Loss: 1.176053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [368640/500434 (74%)] Loss: 1.300359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [378880/500434 (76%)] Loss: 1.290530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [389120/500434 (78%)] Loss: 1.324850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [399360/500434 (80%)] Loss: 1.293138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [409600/500434 (82%)] Loss: 1.208652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [419840/500434 (84%)] Loss: 1.201499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [430080/500434 (86%)] Loss: 1.207290\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [440320/500434 (88%)] Loss: 1.275859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [450560/500434 (90%)] Loss: 1.171561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [460800/500434 (92%)] Loss: 1.251973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [471040/500434 (94%)] Loss: 1.203368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [481280/500434 (96%)] Loss: 1.121772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 173 [491520/500434 (98%)] Loss: 1.203381\u001b[0m\n",
      "\u001b[34mcurrent epoch: 174\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [0/500434 (0%)] Loss: 1.204845\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [10240/500434 (2%)] Loss: 1.175472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [20480/500434 (4%)] Loss: 1.305079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [30720/500434 (6%)] Loss: 1.328401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [40960/500434 (8%)] Loss: 1.287647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [51200/500434 (10%)] Loss: 1.296193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [61440/500434 (12%)] Loss: 1.157074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [71680/500434 (14%)] Loss: 1.159310\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [81920/500434 (16%)] Loss: 1.165451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [92160/500434 (18%)] Loss: 1.319776\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [102400/500434 (20%)] Loss: 1.179091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [112640/500434 (22%)] Loss: 1.137246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [122880/500434 (25%)] Loss: 1.123008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [133120/500434 (27%)] Loss: 1.199125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [143360/500434 (29%)] Loss: 1.198227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [153600/500434 (31%)] Loss: 1.222955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [163840/500434 (33%)] Loss: 1.177649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [174080/500434 (35%)] Loss: 1.184945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [184320/500434 (37%)] Loss: 1.133324\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [194560/500434 (39%)] Loss: 1.269566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [204800/500434 (41%)] Loss: 1.199958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [215040/500434 (43%)] Loss: 1.204114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [225280/500434 (45%)] Loss: 1.247086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [235520/500434 (47%)] Loss: 1.109938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [245760/500434 (49%)] Loss: 1.116400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [256000/500434 (51%)] Loss: 1.123262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [266240/500434 (53%)] Loss: 1.229448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [276480/500434 (55%)] Loss: 1.247160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [286720/500434 (57%)] Loss: 1.243945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [296960/500434 (59%)] Loss: 1.287541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [307200/500434 (61%)] Loss: 1.273330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [317440/500434 (63%)] Loss: 1.341075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [327680/500434 (65%)] Loss: 1.178856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [337920/500434 (67%)] Loss: 1.109333\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [348160/500434 (70%)] Loss: 1.356601\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [358400/500434 (72%)] Loss: 1.249191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [368640/500434 (74%)] Loss: 1.161400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [378880/500434 (76%)] Loss: 1.206551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [389120/500434 (78%)] Loss: 1.187522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [399360/500434 (80%)] Loss: 1.226828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [409600/500434 (82%)] Loss: 1.283540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [419840/500434 (84%)] Loss: 1.239527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [430080/500434 (86%)] Loss: 1.219036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [440320/500434 (88%)] Loss: 1.322849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [450560/500434 (90%)] Loss: 1.234460\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [460800/500434 (92%)] Loss: 1.268647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [471040/500434 (94%)] Loss: 1.243046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [481280/500434 (96%)] Loss: 1.336741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 174 [491520/500434 (98%)] Loss: 1.208894\u001b[0m\n",
      "\u001b[34mcurrent epoch: 175\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [0/500434 (0%)] Loss: 1.183402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [10240/500434 (2%)] Loss: 1.282568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [20480/500434 (4%)] Loss: 1.229764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [30720/500434 (6%)] Loss: 1.318067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [40960/500434 (8%)] Loss: 1.283511\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [51200/500434 (10%)] Loss: 1.215673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [61440/500434 (12%)] Loss: 1.239266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [71680/500434 (14%)] Loss: 1.203146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [81920/500434 (16%)] Loss: 1.149201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [92160/500434 (18%)] Loss: 1.193733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [102400/500434 (20%)] Loss: 1.186825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [112640/500434 (22%)] Loss: 1.300396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [122880/500434 (25%)] Loss: 1.140726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [133120/500434 (27%)] Loss: 1.271718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [143360/500434 (29%)] Loss: 1.180447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [153600/500434 (31%)] Loss: 1.156975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [163840/500434 (33%)] Loss: 1.203524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [174080/500434 (35%)] Loss: 1.171523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [184320/500434 (37%)] Loss: 1.275135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [194560/500434 (39%)] Loss: 1.197907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [204800/500434 (41%)] Loss: 1.228537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [215040/500434 (43%)] Loss: 1.167056\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [225280/500434 (45%)] Loss: 1.125284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [235520/500434 (47%)] Loss: 1.205689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [245760/500434 (49%)] Loss: 1.247318\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [256000/500434 (51%)] Loss: 1.217518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [266240/500434 (53%)] Loss: 1.294404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [276480/500434 (55%)] Loss: 1.193635\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [286720/500434 (57%)] Loss: 1.237268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [296960/500434 (59%)] Loss: 1.263933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [307200/500434 (61%)] Loss: 1.258707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [317440/500434 (63%)] Loss: 1.214900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [327680/500434 (65%)] Loss: 1.185051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [337920/500434 (67%)] Loss: 1.198817\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [348160/500434 (70%)] Loss: 1.194949\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [358400/500434 (72%)] Loss: 1.206915\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [368640/500434 (74%)] Loss: 1.271790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [378880/500434 (76%)] Loss: 1.153236\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [389120/500434 (78%)] Loss: 1.189158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [399360/500434 (80%)] Loss: 1.239433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [409600/500434 (82%)] Loss: 1.217765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [419840/500434 (84%)] Loss: 1.201757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [430080/500434 (86%)] Loss: 1.201347\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [440320/500434 (88%)] Loss: 1.235453\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [450560/500434 (90%)] Loss: 1.235224\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [460800/500434 (92%)] Loss: 1.341054\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [471040/500434 (94%)] Loss: 1.269657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [481280/500434 (96%)] Loss: 1.263710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 175 [491520/500434 (98%)] Loss: 1.302239\u001b[0m\n",
      "\u001b[34mcurrent epoch: 176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [0/500434 (0%)] Loss: 1.242698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [10240/500434 (2%)] Loss: 1.133335\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [20480/500434 (4%)] Loss: 1.182651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [30720/500434 (6%)] Loss: 1.309554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [40960/500434 (8%)] Loss: 1.198073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [51200/500434 (10%)] Loss: 1.124352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [61440/500434 (12%)] Loss: 1.149742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [71680/500434 (14%)] Loss: 1.207457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [81920/500434 (16%)] Loss: 1.134873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [92160/500434 (18%)] Loss: 1.245151\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [102400/500434 (20%)] Loss: 1.279850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [112640/500434 (22%)] Loss: 1.353352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [122880/500434 (25%)] Loss: 1.208865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [133120/500434 (27%)] Loss: 1.232046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [143360/500434 (29%)] Loss: 1.266535\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [153600/500434 (31%)] Loss: 1.304941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [163840/500434 (33%)] Loss: 1.110615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [174080/500434 (35%)] Loss: 1.177486\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [184320/500434 (37%)] Loss: 1.207265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [194560/500434 (39%)] Loss: 1.232094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [204800/500434 (41%)] Loss: 1.178372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [215040/500434 (43%)] Loss: 1.121423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [225280/500434 (45%)] Loss: 1.232353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [235520/500434 (47%)] Loss: 1.213153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [245760/500434 (49%)] Loss: 1.265189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [256000/500434 (51%)] Loss: 1.264675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [266240/500434 (53%)] Loss: 1.326469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [276480/500434 (55%)] Loss: 1.204821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [286720/500434 (57%)] Loss: 1.234329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [296960/500434 (59%)] Loss: 1.255404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [307200/500434 (61%)] Loss: 1.226671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [317440/500434 (63%)] Loss: 1.180519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [327680/500434 (65%)] Loss: 1.232473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [337920/500434 (67%)] Loss: 1.199432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [348160/500434 (70%)] Loss: 1.128116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [358400/500434 (72%)] Loss: 1.216833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [368640/500434 (74%)] Loss: 1.301085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [378880/500434 (76%)] Loss: 1.393008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [389120/500434 (78%)] Loss: 1.363076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [399360/500434 (80%)] Loss: 1.259206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [409600/500434 (82%)] Loss: 1.134219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [419840/500434 (84%)] Loss: 1.057685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [430080/500434 (86%)] Loss: 1.251166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [440320/500434 (88%)] Loss: 1.284555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [450560/500434 (90%)] Loss: 1.274422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [460800/500434 (92%)] Loss: 1.316011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [471040/500434 (94%)] Loss: 1.330639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [481280/500434 (96%)] Loss: 1.189631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 176 [491520/500434 (98%)] Loss: 1.257992\u001b[0m\n",
      "\u001b[34mcurrent epoch: 177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [0/500434 (0%)] Loss: 1.284497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [10240/500434 (2%)] Loss: 1.204193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [20480/500434 (4%)] Loss: 1.171079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [30720/500434 (6%)] Loss: 1.212999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [40960/500434 (8%)] Loss: 1.170919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [51200/500434 (10%)] Loss: 1.226504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [61440/500434 (12%)] Loss: 1.120201\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [71680/500434 (14%)] Loss: 1.201932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [81920/500434 (16%)] Loss: 1.318698\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [92160/500434 (18%)] Loss: 1.185646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [102400/500434 (20%)] Loss: 1.168624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [112640/500434 (22%)] Loss: 1.338071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [122880/500434 (25%)] Loss: 1.254496\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [133120/500434 (27%)] Loss: 1.101810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [143360/500434 (29%)] Loss: 1.243809\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [153600/500434 (31%)] Loss: 1.259232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [163840/500434 (33%)] Loss: 1.194438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [174080/500434 (35%)] Loss: 1.272436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [184320/500434 (37%)] Loss: 1.252446\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [194560/500434 (39%)] Loss: 1.190159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [204800/500434 (41%)] Loss: 1.178490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [215040/500434 (43%)] Loss: 1.207439\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [225280/500434 (45%)] Loss: 1.081944\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [235520/500434 (47%)] Loss: 1.279099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [245760/500434 (49%)] Loss: 1.171497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [256000/500434 (51%)] Loss: 1.210640\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [266240/500434 (53%)] Loss: 1.297418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [276480/500434 (55%)] Loss: 1.101261\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [286720/500434 (57%)] Loss: 1.120802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [296960/500434 (59%)] Loss: 1.299170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [307200/500434 (61%)] Loss: 1.106604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [317440/500434 (63%)] Loss: 1.159181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [327680/500434 (65%)] Loss: 1.187101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [337920/500434 (67%)] Loss: 1.236368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [348160/500434 (70%)] Loss: 1.270969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [358400/500434 (72%)] Loss: 1.147294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [368640/500434 (74%)] Loss: 1.163924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [378880/500434 (76%)] Loss: 1.159829\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [389120/500434 (78%)] Loss: 1.210768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [399360/500434 (80%)] Loss: 1.223873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [409600/500434 (82%)] Loss: 1.144357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [419840/500434 (84%)] Loss: 1.259504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [430080/500434 (86%)] Loss: 1.225515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [440320/500434 (88%)] Loss: 1.315922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [450560/500434 (90%)] Loss: 1.107540\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [460800/500434 (92%)] Loss: 1.235710\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [471040/500434 (94%)] Loss: 1.307903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [481280/500434 (96%)] Loss: 1.199908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 177 [491520/500434 (98%)] Loss: 1.243638\u001b[0m\n",
      "\u001b[34mcurrent epoch: 178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [0/500434 (0%)] Loss: 1.152431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [10240/500434 (2%)] Loss: 1.184738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [20480/500434 (4%)] Loss: 1.142908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [30720/500434 (6%)] Loss: 1.197397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [40960/500434 (8%)] Loss: 1.176762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [51200/500434 (10%)] Loss: 1.210032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [61440/500434 (12%)] Loss: 1.174840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [71680/500434 (14%)] Loss: 1.261645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [81920/500434 (16%)] Loss: 1.189943\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [92160/500434 (18%)] Loss: 1.176551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [102400/500434 (20%)] Loss: 1.226923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [112640/500434 (22%)] Loss: 1.187008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [122880/500434 (25%)] Loss: 1.080294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [133120/500434 (27%)] Loss: 1.244427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [143360/500434 (29%)] Loss: 1.185883\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [153600/500434 (31%)] Loss: 1.201805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [163840/500434 (33%)] Loss: 1.294570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [174080/500434 (35%)] Loss: 1.279879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [184320/500434 (37%)] Loss: 1.198177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [194560/500434 (39%)] Loss: 1.165059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [204800/500434 (41%)] Loss: 1.282382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [215040/500434 (43%)] Loss: 1.135153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [225280/500434 (45%)] Loss: 1.277213\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [235520/500434 (47%)] Loss: 1.180904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [245760/500434 (49%)] Loss: 1.064984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [256000/500434 (51%)] Loss: 1.178575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [266240/500434 (53%)] Loss: 1.213646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [276480/500434 (55%)] Loss: 1.233414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [286720/500434 (57%)] Loss: 1.271380\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [296960/500434 (59%)] Loss: 1.228359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [307200/500434 (61%)] Loss: 1.248679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [317440/500434 (63%)] Loss: 1.198397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [327680/500434 (65%)] Loss: 1.326843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [337920/500434 (67%)] Loss: 1.136889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [348160/500434 (70%)] Loss: 1.131952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [358400/500434 (72%)] Loss: 1.208849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [368640/500434 (74%)] Loss: 1.258609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [378880/500434 (76%)] Loss: 1.144658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [389120/500434 (78%)] Loss: 1.203068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [399360/500434 (80%)] Loss: 1.255355\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [409600/500434 (82%)] Loss: 1.222679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [419840/500434 (84%)] Loss: 1.211006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [430080/500434 (86%)] Loss: 1.096596\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [440320/500434 (88%)] Loss: 1.256584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [450560/500434 (90%)] Loss: 1.290562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [460800/500434 (92%)] Loss: 1.411251\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [471040/500434 (94%)] Loss: 1.130024\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [481280/500434 (96%)] Loss: 1.314553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 178 [491520/500434 (98%)] Loss: 1.225137\u001b[0m\n",
      "\u001b[34mcurrent epoch: 179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [0/500434 (0%)] Loss: 1.177701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [10240/500434 (2%)] Loss: 1.316984\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [20480/500434 (4%)] Loss: 1.167428\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [30720/500434 (6%)] Loss: 1.234241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [40960/500434 (8%)] Loss: 1.133788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [51200/500434 (10%)] Loss: 1.188934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [61440/500434 (12%)] Loss: 1.153846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [71680/500434 (14%)] Loss: 1.133025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [81920/500434 (16%)] Loss: 1.155970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [92160/500434 (18%)] Loss: 1.166157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [102400/500434 (20%)] Loss: 1.292977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [112640/500434 (22%)] Loss: 1.155842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [122880/500434 (25%)] Loss: 1.120399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [133120/500434 (27%)] Loss: 1.201938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [143360/500434 (29%)] Loss: 1.197993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [153600/500434 (31%)] Loss: 1.202902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [163840/500434 (33%)] Loss: 1.156228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [174080/500434 (35%)] Loss: 1.160588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [184320/500434 (37%)] Loss: 1.208864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [194560/500434 (39%)] Loss: 1.237842\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [204800/500434 (41%)] Loss: 1.232281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [215040/500434 (43%)] Loss: 1.187469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [225280/500434 (45%)] Loss: 1.155735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [235520/500434 (47%)] Loss: 1.233722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [245760/500434 (49%)] Loss: 1.220658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [256000/500434 (51%)] Loss: 1.201843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [266240/500434 (53%)] Loss: 1.253632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [276480/500434 (55%)] Loss: 1.180248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [286720/500434 (57%)] Loss: 1.192690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [296960/500434 (59%)] Loss: 1.197851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [307200/500434 (61%)] Loss: 1.135142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [317440/500434 (63%)] Loss: 1.305724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [327680/500434 (65%)] Loss: 1.179064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [337920/500434 (67%)] Loss: 1.231323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [348160/500434 (70%)] Loss: 1.187919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [358400/500434 (72%)] Loss: 1.165094\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [368640/500434 (74%)] Loss: 1.226285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [378880/500434 (76%)] Loss: 1.212971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [389120/500434 (78%)] Loss: 1.087273\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [399360/500434 (80%)] Loss: 1.307969\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [409600/500434 (82%)] Loss: 1.210977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [419840/500434 (84%)] Loss: 1.147083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [430080/500434 (86%)] Loss: 1.198979\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [440320/500434 (88%)] Loss: 1.138206\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [450560/500434 (90%)] Loss: 1.199967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [460800/500434 (92%)] Loss: 1.220972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [471040/500434 (94%)] Loss: 1.114737\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [481280/500434 (96%)] Loss: 1.124272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 179 [491520/500434 (98%)] Loss: 1.246832\u001b[0m\n",
      "\u001b[34mcurrent epoch: 180\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [0/500434 (0%)] Loss: 1.125833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [10240/500434 (2%)] Loss: 1.186332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [20480/500434 (4%)] Loss: 1.229468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [30720/500434 (6%)] Loss: 1.228137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [40960/500434 (8%)] Loss: 1.300272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [51200/500434 (10%)] Loss: 1.082283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [61440/500434 (12%)] Loss: 1.188479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [71680/500434 (14%)] Loss: 1.141686\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [81920/500434 (16%)] Loss: 1.127599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [92160/500434 (18%)] Loss: 1.224948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [102400/500434 (20%)] Loss: 1.271237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [112640/500434 (22%)] Loss: 1.233849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [122880/500434 (25%)] Loss: 1.180946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [133120/500434 (27%)] Loss: 1.180999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [143360/500434 (29%)] Loss: 1.264991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [153600/500434 (31%)] Loss: 1.174102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [163840/500434 (33%)] Loss: 1.216171\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [174080/500434 (35%)] Loss: 1.173743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [184320/500434 (37%)] Loss: 1.132765\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [194560/500434 (39%)] Loss: 1.267579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [204800/500434 (41%)] Loss: 1.233122\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [215040/500434 (43%)] Loss: 1.145865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [225280/500434 (45%)] Loss: 1.148461\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [235520/500434 (47%)] Loss: 1.306130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [245760/500434 (49%)] Loss: 1.152665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [256000/500434 (51%)] Loss: 1.227493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [266240/500434 (53%)] Loss: 1.236905\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [276480/500434 (55%)] Loss: 1.299032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [286720/500434 (57%)] Loss: 1.202247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [296960/500434 (59%)] Loss: 1.159454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [307200/500434 (61%)] Loss: 1.210047\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [317440/500434 (63%)] Loss: 1.050615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [327680/500434 (65%)] Loss: 1.212911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [337920/500434 (67%)] Loss: 1.246917\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [348160/500434 (70%)] Loss: 1.205382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [358400/500434 (72%)] Loss: 1.226069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [368640/500434 (74%)] Loss: 1.220301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [378880/500434 (76%)] Loss: 1.238836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [389120/500434 (78%)] Loss: 1.165275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [399360/500434 (80%)] Loss: 1.246531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [409600/500434 (82%)] Loss: 1.075974\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [419840/500434 (84%)] Loss: 1.232786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [430080/500434 (86%)] Loss: 1.147680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [440320/500434 (88%)] Loss: 1.096222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [450560/500434 (90%)] Loss: 1.238052\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [460800/500434 (92%)] Loss: 1.157531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [471040/500434 (94%)] Loss: 1.178427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [481280/500434 (96%)] Loss: 1.201970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 180 [491520/500434 (98%)] Loss: 1.119818\u001b[0m\n",
      "\u001b[34mcurrent epoch: 181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [0/500434 (0%)] Loss: 1.137483\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [10240/500434 (2%)] Loss: 1.298613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [20480/500434 (4%)] Loss: 1.198372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [30720/500434 (6%)] Loss: 1.080096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [40960/500434 (8%)] Loss: 1.199925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [51200/500434 (10%)] Loss: 1.230172\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [61440/500434 (12%)] Loss: 1.208749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [71680/500434 (14%)] Loss: 1.123583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [81920/500434 (16%)] Loss: 1.145803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [92160/500434 (18%)] Loss: 1.173630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [102400/500434 (20%)] Loss: 1.164760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [112640/500434 (22%)] Loss: 1.175051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [122880/500434 (25%)] Loss: 1.248105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [133120/500434 (27%)] Loss: 1.154728\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [143360/500434 (29%)] Loss: 1.184295\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [153600/500434 (31%)] Loss: 1.157376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [163840/500434 (33%)] Loss: 1.149614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [174080/500434 (35%)] Loss: 1.170594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [184320/500434 (37%)] Loss: 1.293248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [194560/500434 (39%)] Loss: 1.121099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [204800/500434 (41%)] Loss: 1.181302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [215040/500434 (43%)] Loss: 1.191907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [225280/500434 (45%)] Loss: 1.297951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [235520/500434 (47%)] Loss: 1.206810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [245760/500434 (49%)] Loss: 1.163216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [256000/500434 (51%)] Loss: 1.216941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [266240/500434 (53%)] Loss: 1.273792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [276480/500434 (55%)] Loss: 1.145802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [286720/500434 (57%)] Loss: 1.199912\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [296960/500434 (59%)] Loss: 1.165284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [307200/500434 (61%)] Loss: 1.181988\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [317440/500434 (63%)] Loss: 1.103346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [327680/500434 (65%)] Loss: 1.134159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [337920/500434 (67%)] Loss: 1.293345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [348160/500434 (70%)] Loss: 1.209447\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [358400/500434 (72%)] Loss: 1.250985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [368640/500434 (74%)] Loss: 1.168936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [378880/500434 (76%)] Loss: 1.138703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [389120/500434 (78%)] Loss: 1.173115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [399360/500434 (80%)] Loss: 1.219133\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [409600/500434 (82%)] Loss: 1.266857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [419840/500434 (84%)] Loss: 1.332330\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [430080/500434 (86%)] Loss: 1.229913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [440320/500434 (88%)] Loss: 1.084123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [450560/500434 (90%)] Loss: 1.258003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [460800/500434 (92%)] Loss: 1.141493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [471040/500434 (94%)] Loss: 1.213297\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [481280/500434 (96%)] Loss: 1.115645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 181 [491520/500434 (98%)] Loss: 1.230154\u001b[0m\n",
      "\u001b[34mcurrent epoch: 182\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [0/500434 (0%)] Loss: 1.118184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [10240/500434 (2%)] Loss: 1.209500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [20480/500434 (4%)] Loss: 1.149050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [30720/500434 (6%)] Loss: 1.126197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [40960/500434 (8%)] Loss: 1.206902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [51200/500434 (10%)] Loss: 1.233053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [61440/500434 (12%)] Loss: 1.161141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [71680/500434 (14%)] Loss: 1.239185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [81920/500434 (16%)] Loss: 1.181116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [92160/500434 (18%)] Loss: 1.087499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [102400/500434 (20%)] Loss: 1.214264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [112640/500434 (22%)] Loss: 1.213566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [122880/500434 (25%)] Loss: 1.267529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [133120/500434 (27%)] Loss: 1.156682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [143360/500434 (29%)] Loss: 1.129161\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [153600/500434 (31%)] Loss: 1.162304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [163840/500434 (33%)] Loss: 1.291587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [174080/500434 (35%)] Loss: 1.226998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [184320/500434 (37%)] Loss: 1.219352\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [194560/500434 (39%)] Loss: 1.263900\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [204800/500434 (41%)] Loss: 1.136223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [215040/500434 (43%)] Loss: 1.227538\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [225280/500434 (45%)] Loss: 1.240104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [235520/500434 (47%)] Loss: 1.173923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [245760/500434 (49%)] Loss: 1.150338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [256000/500434 (51%)] Loss: 1.112763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [266240/500434 (53%)] Loss: 1.324730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [276480/500434 (55%)] Loss: 1.144908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [286720/500434 (57%)] Loss: 1.304612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [296960/500434 (59%)] Loss: 1.206525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [307200/500434 (61%)] Loss: 1.119550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [317440/500434 (63%)] Loss: 1.187689\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [327680/500434 (65%)] Loss: 1.212832\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [337920/500434 (67%)] Loss: 1.304870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [348160/500434 (70%)] Loss: 1.237414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [358400/500434 (72%)] Loss: 1.199044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [368640/500434 (74%)] Loss: 1.144763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [378880/500434 (76%)] Loss: 1.122630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [389120/500434 (78%)] Loss: 1.390939\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [399360/500434 (80%)] Loss: 1.354947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [409600/500434 (82%)] Loss: 1.243040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [419840/500434 (84%)] Loss: 1.262469\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [430080/500434 (86%)] Loss: 1.318592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [440320/500434 (88%)] Loss: 1.100866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [450560/500434 (90%)] Loss: 1.247190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [460800/500434 (92%)] Loss: 1.240933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [471040/500434 (94%)] Loss: 1.240216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [481280/500434 (96%)] Loss: 1.204138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 182 [491520/500434 (98%)] Loss: 1.199215\u001b[0m\n",
      "\u001b[34mcurrent epoch: 183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [0/500434 (0%)] Loss: 1.150348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [10240/500434 (2%)] Loss: 1.221013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [20480/500434 (4%)] Loss: 1.095807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [30720/500434 (6%)] Loss: 1.123632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [40960/500434 (8%)] Loss: 1.091238\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [51200/500434 (10%)] Loss: 1.278265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [61440/500434 (12%)] Loss: 1.241319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [71680/500434 (14%)] Loss: 1.118503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [81920/500434 (16%)] Loss: 1.121437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [92160/500434 (18%)] Loss: 1.191764\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [102400/500434 (20%)] Loss: 1.198991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [112640/500434 (22%)] Loss: 1.225038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [122880/500434 (25%)] Loss: 1.227634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [133120/500434 (27%)] Loss: 1.159233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [143360/500434 (29%)] Loss: 1.176048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [153600/500434 (31%)] Loss: 1.167599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [163840/500434 (33%)] Loss: 1.219652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [174080/500434 (35%)] Loss: 1.087115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [184320/500434 (37%)] Loss: 1.239009\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [194560/500434 (39%)] Loss: 1.186301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [204800/500434 (41%)] Loss: 1.171663\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [215040/500434 (43%)] Loss: 1.173215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [225280/500434 (45%)] Loss: 1.151200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [235520/500434 (47%)] Loss: 1.246785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [245760/500434 (49%)] Loss: 1.154430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [256000/500434 (51%)] Loss: 1.185590\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [266240/500434 (53%)] Loss: 1.159401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [276480/500434 (55%)] Loss: 1.128131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [286720/500434 (57%)] Loss: 1.129114\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [296960/500434 (59%)] Loss: 1.221801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [307200/500434 (61%)] Loss: 1.213793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [317440/500434 (63%)] Loss: 1.167325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [327680/500434 (65%)] Loss: 1.265464\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [337920/500434 (67%)] Loss: 1.273755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [348160/500434 (70%)] Loss: 1.152370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [358400/500434 (72%)] Loss: 1.196652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [368640/500434 (74%)] Loss: 1.112552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [378880/500434 (76%)] Loss: 1.215982\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [389120/500434 (78%)] Loss: 1.126582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [399360/500434 (80%)] Loss: 1.261512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [409600/500434 (82%)] Loss: 1.219697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [419840/500434 (84%)] Loss: 1.303135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [430080/500434 (86%)] Loss: 1.127098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [440320/500434 (88%)] Loss: 1.221685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [450560/500434 (90%)] Loss: 1.128072\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [460800/500434 (92%)] Loss: 1.231760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [471040/500434 (94%)] Loss: 1.179726\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [481280/500434 (96%)] Loss: 1.163884\u001b[0m\n",
      "\u001b[34mTrain Epoch: 183 [491520/500434 (98%)] Loss: 1.102517\u001b[0m\n",
      "\u001b[34mcurrent epoch: 184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [0/500434 (0%)] Loss: 1.192820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [10240/500434 (2%)] Loss: 1.176102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [20480/500434 (4%)] Loss: 1.149940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [30720/500434 (6%)] Loss: 1.207956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [40960/500434 (8%)] Loss: 1.075160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [51200/500434 (10%)] Loss: 1.071239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [61440/500434 (12%)] Loss: 1.232513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [71680/500434 (14%)] Loss: 1.245527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [81920/500434 (16%)] Loss: 1.183294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [92160/500434 (18%)] Loss: 1.118702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [102400/500434 (20%)] Loss: 1.245170\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [112640/500434 (22%)] Loss: 1.218568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [122880/500434 (25%)] Loss: 1.244624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [133120/500434 (27%)] Loss: 1.204549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [143360/500434 (29%)] Loss: 1.226490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [153600/500434 (31%)] Loss: 1.269166\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [163840/500434 (33%)] Loss: 1.177956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [174080/500434 (35%)] Loss: 1.209731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [184320/500434 (37%)] Loss: 1.171514\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [194560/500434 (39%)] Loss: 1.181387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [204800/500434 (41%)] Loss: 1.328622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [215040/500434 (43%)] Loss: 1.157730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [225280/500434 (45%)] Loss: 1.275414\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [235520/500434 (47%)] Loss: 1.201226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [245760/500434 (49%)] Loss: 1.158384\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [256000/500434 (51%)] Loss: 1.218575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [266240/500434 (53%)] Loss: 1.298241\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [276480/500434 (55%)] Loss: 1.208008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [286720/500434 (57%)] Loss: 1.202746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [296960/500434 (59%)] Loss: 1.168975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [307200/500434 (61%)] Loss: 1.200968\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [317440/500434 (63%)] Loss: 1.185305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [327680/500434 (65%)] Loss: 1.156634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [337920/500434 (67%)] Loss: 1.111545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [348160/500434 (70%)] Loss: 1.284102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [358400/500434 (72%)] Loss: 1.191775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [368640/500434 (74%)] Loss: 1.233305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [378880/500434 (76%)] Loss: 1.214877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [389120/500434 (78%)] Loss: 1.260751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [399360/500434 (80%)] Loss: 1.243730\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [409600/500434 (82%)] Loss: 1.159067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [419840/500434 (84%)] Loss: 1.255138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [430080/500434 (86%)] Loss: 1.236831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [440320/500434 (88%)] Loss: 1.251695\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [450560/500434 (90%)] Loss: 1.158992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [460800/500434 (92%)] Loss: 1.193282\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [471040/500434 (94%)] Loss: 1.145077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [481280/500434 (96%)] Loss: 1.099594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 184 [491520/500434 (98%)] Loss: 1.260665\u001b[0m\n",
      "\u001b[34mcurrent epoch: 185\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [0/500434 (0%)] Loss: 1.088796\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [10240/500434 (2%)] Loss: 1.192869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [20480/500434 (4%)] Loss: 1.107017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [30720/500434 (6%)] Loss: 1.125121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [40960/500434 (8%)] Loss: 1.213522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [51200/500434 (10%)] Loss: 1.227209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [61440/500434 (12%)] Loss: 1.162288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [71680/500434 (14%)] Loss: 1.107740\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [81920/500434 (16%)] Loss: 1.144513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [92160/500434 (18%)] Loss: 1.192457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [102400/500434 (20%)] Loss: 1.181980\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [112640/500434 (22%)] Loss: 1.187990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [122880/500434 (25%)] Loss: 1.199400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [133120/500434 (27%)] Loss: 1.131053\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [143360/500434 (29%)] Loss: 1.156411\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [153600/500434 (31%)] Loss: 1.205313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [163840/500434 (33%)] Loss: 1.155102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [174080/500434 (35%)] Loss: 1.202851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [184320/500434 (37%)] Loss: 1.100296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [194560/500434 (39%)] Loss: 1.261058\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [204800/500434 (41%)] Loss: 1.150827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [215040/500434 (43%)] Loss: 1.141084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [225280/500434 (45%)] Loss: 1.183833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [235520/500434 (47%)] Loss: 1.132725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [245760/500434 (49%)] Loss: 1.183022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [256000/500434 (51%)] Loss: 1.151012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [266240/500434 (53%)] Loss: 1.149894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [276480/500434 (55%)] Loss: 1.132177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [286720/500434 (57%)] Loss: 1.215975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [296960/500434 (59%)] Loss: 1.317803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [307200/500434 (61%)] Loss: 1.145882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [317440/500434 (63%)] Loss: 1.129495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [327680/500434 (65%)] Loss: 1.201568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [337920/500434 (67%)] Loss: 1.133493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [348160/500434 (70%)] Loss: 1.179314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [358400/500434 (72%)] Loss: 1.135945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [368640/500434 (74%)] Loss: 1.219613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [378880/500434 (76%)] Loss: 1.137895\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [389120/500434 (78%)] Loss: 1.162707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [399360/500434 (80%)] Loss: 1.092743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [409600/500434 (82%)] Loss: 1.217435\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [419840/500434 (84%)] Loss: 1.197005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [430080/500434 (86%)] Loss: 1.162218\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [440320/500434 (88%)] Loss: 1.083792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [450560/500434 (90%)] Loss: 1.208429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [460800/500434 (92%)] Loss: 1.139251\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [471040/500434 (94%)] Loss: 1.158363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [481280/500434 (96%)] Loss: 1.304528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 185 [491520/500434 (98%)] Loss: 1.174431\u001b[0m\n",
      "\u001b[34mcurrent epoch: 186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [0/500434 (0%)] Loss: 1.189000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [10240/500434 (2%)] Loss: 1.185231\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [20480/500434 (4%)] Loss: 1.155427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [30720/500434 (6%)] Loss: 1.164204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [40960/500434 (8%)] Loss: 1.151075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [51200/500434 (10%)] Loss: 1.092513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [61440/500434 (12%)] Loss: 1.071537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [71680/500434 (14%)] Loss: 1.102143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [81920/500434 (16%)] Loss: 1.090792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [92160/500434 (18%)] Loss: 1.175253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [102400/500434 (20%)] Loss: 1.265846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [112640/500434 (22%)] Loss: 1.279438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [122880/500434 (25%)] Loss: 1.201191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [133120/500434 (27%)] Loss: 1.211536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [143360/500434 (29%)] Loss: 1.192614\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [153600/500434 (31%)] Loss: 1.144889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [163840/500434 (33%)] Loss: 1.092860\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [174080/500434 (35%)] Loss: 1.174529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [184320/500434 (37%)] Loss: 1.184920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [194560/500434 (39%)] Loss: 1.307195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [204800/500434 (41%)] Loss: 1.149296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [215040/500434 (43%)] Loss: 1.207876\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [225280/500434 (45%)] Loss: 1.159649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [235520/500434 (47%)] Loss: 1.183569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [245760/500434 (49%)] Loss: 1.298194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [256000/500434 (51%)] Loss: 1.200018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [266240/500434 (53%)] Loss: 1.136204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [276480/500434 (55%)] Loss: 1.332250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [286720/500434 (57%)] Loss: 1.222864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [296960/500434 (59%)] Loss: 1.243821\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [307200/500434 (61%)] Loss: 1.275827\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [317440/500434 (63%)] Loss: 1.234353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [327680/500434 (65%)] Loss: 1.234478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [337920/500434 (67%)] Loss: 1.310344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [348160/500434 (70%)] Loss: 1.068490\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [358400/500434 (72%)] Loss: 1.153076\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [368640/500434 (74%)] Loss: 1.264468\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [378880/500434 (76%)] Loss: 1.095472\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [389120/500434 (78%)] Loss: 1.163505\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [399360/500434 (80%)] Loss: 1.176004\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [409600/500434 (82%)] Loss: 1.190825\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [419840/500434 (84%)] Loss: 1.207251\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [430080/500434 (86%)] Loss: 1.192536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [440320/500434 (88%)] Loss: 1.150085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [450560/500434 (90%)] Loss: 1.183624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [460800/500434 (92%)] Loss: 1.152034\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [471040/500434 (94%)] Loss: 1.063296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [481280/500434 (96%)] Loss: 1.208124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 186 [491520/500434 (98%)] Loss: 1.270083\u001b[0m\n",
      "\u001b[34mcurrent epoch: 187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [0/500434 (0%)] Loss: 1.133470\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [10240/500434 (2%)] Loss: 1.183675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [20480/500434 (4%)] Loss: 1.209806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [30720/500434 (6%)] Loss: 1.178773\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [40960/500434 (8%)] Loss: 1.105002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [51200/500434 (10%)] Loss: 1.077869\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [61440/500434 (12%)] Loss: 1.227604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [71680/500434 (14%)] Loss: 1.070643\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [81920/500434 (16%)] Loss: 1.252413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [92160/500434 (18%)] Loss: 1.171158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [102400/500434 (20%)] Loss: 1.174383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [112640/500434 (22%)] Loss: 1.279675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [122880/500434 (25%)] Loss: 1.212063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [133120/500434 (27%)] Loss: 1.156030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [143360/500434 (29%)] Loss: 1.188992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [153600/500434 (31%)] Loss: 1.196373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [163840/500434 (33%)] Loss: 1.098383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [174080/500434 (35%)] Loss: 1.150524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [184320/500434 (37%)] Loss: 1.178365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [194560/500434 (39%)] Loss: 1.192376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [204800/500434 (41%)] Loss: 1.223126\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [215040/500434 (43%)] Loss: 1.216843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [225280/500434 (45%)] Loss: 1.106784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [235520/500434 (47%)] Loss: 1.156742\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [245760/500434 (49%)] Loss: 1.077203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [256000/500434 (51%)] Loss: 1.212217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [266240/500434 (53%)] Loss: 1.177739\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [276480/500434 (55%)] Loss: 1.160864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [286720/500434 (57%)] Loss: 1.134115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [296960/500434 (59%)] Loss: 1.067152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [307200/500434 (61%)] Loss: 1.198579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [317440/500434 (63%)] Loss: 1.173770\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [327680/500434 (65%)] Loss: 1.207219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [337920/500434 (67%)] Loss: 1.173320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [348160/500434 (70%)] Loss: 1.179187\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [358400/500434 (72%)] Loss: 1.235700\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [368640/500434 (74%)] Loss: 1.022893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [378880/500434 (76%)] Loss: 1.234383\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [389120/500434 (78%)] Loss: 1.156400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [399360/500434 (80%)] Loss: 1.208387\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [409600/500434 (82%)] Loss: 1.191467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [419840/500434 (84%)] Loss: 1.031353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [430080/500434 (86%)] Loss: 1.139894\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [440320/500434 (88%)] Loss: 1.256065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [450560/500434 (90%)] Loss: 1.194319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [460800/500434 (92%)] Loss: 1.163957\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [471040/500434 (94%)] Loss: 1.143005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [481280/500434 (96%)] Loss: 1.194679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 187 [491520/500434 (98%)] Loss: 1.131000\u001b[0m\n",
      "\u001b[34mcurrent epoch: 188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [0/500434 (0%)] Loss: 1.183177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [10240/500434 (2%)] Loss: 1.124559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [20480/500434 (4%)] Loss: 1.214265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [30720/500434 (6%)] Loss: 1.166308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [40960/500434 (8%)] Loss: 1.149879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [51200/500434 (10%)] Loss: 1.100556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [61440/500434 (12%)] Loss: 1.229605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [71680/500434 (14%)] Loss: 1.084043\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [81920/500434 (16%)] Loss: 1.052940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [92160/500434 (18%)] Loss: 1.133890\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [102400/500434 (20%)] Loss: 1.129785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [112640/500434 (22%)] Loss: 1.177259\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [122880/500434 (25%)] Loss: 1.165755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [133120/500434 (27%)] Loss: 1.400362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [143360/500434 (29%)] Loss: 1.206790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [153600/500434 (31%)] Loss: 1.204646\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [163840/500434 (33%)] Loss: 1.258361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [174080/500434 (35%)] Loss: 1.133367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [184320/500434 (37%)] Loss: 1.170673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [194560/500434 (39%)] Loss: 1.147115\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [204800/500434 (41%)] Loss: 1.079117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [215040/500434 (43%)] Loss: 1.251057\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [225280/500434 (45%)] Loss: 1.167993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [235520/500434 (47%)] Loss: 1.301353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [245760/500434 (49%)] Loss: 1.199276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [256000/500434 (51%)] Loss: 1.281501\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [266240/500434 (53%)] Loss: 1.109169\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [276480/500434 (55%)] Loss: 1.197857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [286720/500434 (57%)] Loss: 1.216962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [296960/500434 (59%)] Loss: 1.130899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [307200/500434 (61%)] Loss: 1.132557\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [317440/500434 (63%)] Loss: 1.201647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [327680/500434 (65%)] Loss: 1.262637\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [337920/500434 (67%)] Loss: 1.182745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [348160/500434 (70%)] Loss: 1.092777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [358400/500434 (72%)] Loss: 1.145455\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [368640/500434 (74%)] Loss: 1.149152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [378880/500434 (76%)] Loss: 1.231928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [389120/500434 (78%)] Loss: 1.147990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [399360/500434 (80%)] Loss: 1.089580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [409600/500434 (82%)] Loss: 1.130088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [419840/500434 (84%)] Loss: 1.178717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [430080/500434 (86%)] Loss: 1.160142\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [440320/500434 (88%)] Loss: 1.087002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [450560/500434 (90%)] Loss: 1.133898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [460800/500434 (92%)] Loss: 1.170789\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [471040/500434 (94%)] Loss: 1.087304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [481280/500434 (96%)] Loss: 1.159096\u001b[0m\n",
      "\u001b[34mTrain Epoch: 188 [491520/500434 (98%)] Loss: 1.146796\u001b[0m\n",
      "\u001b[34mcurrent epoch: 189\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [0/500434 (0%)] Loss: 1.145153\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [10240/500434 (2%)] Loss: 1.137118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [20480/500434 (4%)] Loss: 1.206392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [30720/500434 (6%)] Loss: 1.189768\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [40960/500434 (8%)] Loss: 1.150284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [51200/500434 (10%)] Loss: 1.169522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [61440/500434 (12%)] Loss: 1.106709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [71680/500434 (14%)] Loss: 1.194670\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [81920/500434 (16%)] Loss: 1.266904\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [92160/500434 (18%)] Loss: 1.063591\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [102400/500434 (20%)] Loss: 1.270763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [112640/500434 (22%)] Loss: 1.130872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [122880/500434 (25%)] Loss: 1.260176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [133120/500434 (27%)] Loss: 1.139340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [143360/500434 (29%)] Loss: 1.147882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [153600/500434 (31%)] Loss: 1.129962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [163840/500434 (33%)] Loss: 1.208804\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [174080/500434 (35%)] Loss: 1.216959\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [184320/500434 (37%)] Loss: 1.252337\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [194560/500434 (39%)] Loss: 1.155082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [204800/500434 (41%)] Loss: 1.182147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [215040/500434 (43%)] Loss: 1.107602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [225280/500434 (45%)] Loss: 1.275058\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [235520/500434 (47%)] Loss: 1.075019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [245760/500434 (49%)] Loss: 1.213534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [256000/500434 (51%)] Loss: 1.152639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [266240/500434 (53%)] Loss: 1.120806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [276480/500434 (55%)] Loss: 1.088991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [286720/500434 (57%)] Loss: 1.140393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [296960/500434 (59%)] Loss: 1.159237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [307200/500434 (61%)] Loss: 1.119586\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [317440/500434 (63%)] Loss: 1.238159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [327680/500434 (65%)] Loss: 1.138654\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [337920/500434 (67%)] Loss: 1.125911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [348160/500434 (70%)] Loss: 1.113534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [358400/500434 (72%)] Loss: 1.096499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [368640/500434 (74%)] Loss: 1.248947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [378880/500434 (76%)] Loss: 1.150123\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [389120/500434 (78%)] Loss: 1.093313\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [399360/500434 (80%)] Loss: 1.135530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [409600/500434 (82%)] Loss: 1.182497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [419840/500434 (84%)] Loss: 1.186186\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [430080/500434 (86%)] Loss: 1.118922\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [440320/500434 (88%)] Loss: 1.139068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [450560/500434 (90%)] Loss: 1.325212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [460800/500434 (92%)] Loss: 1.070118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [471040/500434 (94%)] Loss: 1.170445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [481280/500434 (96%)] Loss: 1.128521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 189 [491520/500434 (98%)] Loss: 1.196403\u001b[0m\n",
      "\u001b[34mcurrent epoch: 190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [0/500434 (0%)] Loss: 1.108930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [10240/500434 (2%)] Loss: 1.150287\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [20480/500434 (4%)] Loss: 1.217751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [30720/500434 (6%)] Loss: 1.116738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [40960/500434 (8%)] Loss: 1.095793\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [51200/500434 (10%)] Loss: 1.152751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [61440/500434 (12%)] Loss: 1.126445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [71680/500434 (14%)] Loss: 1.146853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [81920/500434 (16%)] Loss: 1.139852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [92160/500434 (18%)] Loss: 1.047797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [102400/500434 (20%)] Loss: 1.118177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [112640/500434 (22%)] Loss: 1.093572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [122880/500434 (25%)] Loss: 1.167747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [133120/500434 (27%)] Loss: 1.211086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [143360/500434 (29%)] Loss: 1.170200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [153600/500434 (31%)] Loss: 1.206396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [163840/500434 (33%)] Loss: 1.060882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [174080/500434 (35%)] Loss: 1.196417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [184320/500434 (37%)] Loss: 1.144214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [194560/500434 (39%)] Loss: 1.157086\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [204800/500434 (41%)] Loss: 1.187834\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [215040/500434 (43%)] Loss: 1.121552\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [225280/500434 (45%)] Loss: 1.151038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [235520/500434 (47%)] Loss: 1.174134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [245760/500434 (49%)] Loss: 1.165580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [256000/500434 (51%)] Loss: 1.191005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [266240/500434 (53%)] Loss: 1.208858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [276480/500434 (55%)] Loss: 1.184392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [286720/500434 (57%)] Loss: 1.165946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [296960/500434 (59%)] Loss: 1.170967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [307200/500434 (61%)] Loss: 1.138448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [317440/500434 (63%)] Loss: 1.158526\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [327680/500434 (65%)] Loss: 1.164488\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [337920/500434 (67%)] Loss: 1.144272\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [348160/500434 (70%)] Loss: 1.134962\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [358400/500434 (72%)] Loss: 1.182627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [368640/500434 (74%)] Loss: 1.100278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [378880/500434 (76%)] Loss: 1.155545\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [389120/500434 (78%)] Loss: 1.202235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [399360/500434 (80%)] Loss: 1.158144\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [409600/500434 (82%)] Loss: 1.200709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [419840/500434 (84%)] Loss: 1.081457\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [430080/500434 (86%)] Loss: 1.158661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [440320/500434 (88%)] Loss: 1.194465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [450560/500434 (90%)] Loss: 1.073139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [460800/500434 (92%)] Loss: 1.139863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [471040/500434 (94%)] Loss: 1.116603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [481280/500434 (96%)] Loss: 1.087594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 190 [491520/500434 (98%)] Loss: 1.140998\u001b[0m\n",
      "\u001b[34mcurrent epoch: 191\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [0/500434 (0%)] Loss: 1.186928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [10240/500434 (2%)] Loss: 1.153840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [20480/500434 (4%)] Loss: 1.157721\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [30720/500434 (6%)] Loss: 1.196477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [40960/500434 (8%)] Loss: 1.260120\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [51200/500434 (10%)] Loss: 1.128237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [61440/500434 (12%)] Loss: 1.112927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [71680/500434 (14%)] Loss: 1.047246\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [81920/500434 (16%)] Loss: 1.181516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [92160/500434 (18%)] Loss: 1.280027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [102400/500434 (20%)] Loss: 1.116594\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [112640/500434 (22%)] Loss: 1.155631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [122880/500434 (25%)] Loss: 1.180732\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [133120/500434 (27%)] Loss: 1.123877\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [143360/500434 (29%)] Loss: 1.153961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [153600/500434 (31%)] Loss: 1.111713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [163840/500434 (33%)] Loss: 1.055415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [174080/500434 (35%)] Loss: 1.192200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [184320/500434 (37%)] Loss: 1.186920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [194560/500434 (39%)] Loss: 1.122656\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [204800/500434 (41%)] Loss: 1.101125\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [215040/500434 (43%)] Loss: 1.204705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [225280/500434 (45%)] Loss: 1.132999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [235520/500434 (47%)] Loss: 1.160902\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [245760/500434 (49%)] Loss: 1.208350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [256000/500434 (51%)] Loss: 1.284008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [266240/500434 (53%)] Loss: 1.141983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [276480/500434 (55%)] Loss: 1.142325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [286720/500434 (57%)] Loss: 1.135864\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [296960/500434 (59%)] Loss: 1.246778\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [307200/500434 (61%)] Loss: 1.196275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [317440/500434 (63%)] Loss: 1.164271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [327680/500434 (65%)] Loss: 1.212589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [337920/500434 (67%)] Loss: 1.290516\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [348160/500434 (70%)] Loss: 1.129389\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [358400/500434 (72%)] Loss: 1.051021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [368640/500434 (74%)] Loss: 1.161286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [378880/500434 (76%)] Loss: 1.244657\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [389120/500434 (78%)] Loss: 1.084252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [399360/500434 (80%)] Loss: 1.096950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [409600/500434 (82%)] Loss: 1.122372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [419840/500434 (84%)] Loss: 1.132194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [430080/500434 (86%)] Loss: 1.227703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [440320/500434 (88%)] Loss: 1.164105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [450560/500434 (90%)] Loss: 1.171919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [460800/500434 (92%)] Loss: 1.162639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [471040/500434 (94%)] Loss: 1.194026\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [481280/500434 (96%)] Loss: 1.149808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 191 [491520/500434 (98%)] Loss: 1.067027\u001b[0m\n",
      "\u001b[34mcurrent epoch: 192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [0/500434 (0%)] Loss: 1.198556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [10240/500434 (2%)] Loss: 1.044312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [20480/500434 (4%)] Loss: 1.142801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [30720/500434 (6%)] Loss: 1.184077\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [40960/500434 (8%)] Loss: 1.139071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [51200/500434 (10%)] Loss: 1.156327\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [61440/500434 (12%)] Loss: 1.122476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [71680/500434 (14%)] Loss: 1.197314\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [81920/500434 (16%)] Loss: 1.134050\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [92160/500434 (18%)] Loss: 1.268930\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [102400/500434 (20%)] Loss: 1.201478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [112640/500434 (22%)] Loss: 1.175088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [122880/500434 (25%)] Loss: 1.170452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [133120/500434 (27%)] Loss: 1.165289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [143360/500434 (29%)] Loss: 1.171820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [153600/500434 (31%)] Loss: 1.146870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [163840/500434 (33%)] Loss: 1.172067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [174080/500434 (35%)] Loss: 1.268033\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [184320/500434 (37%)] Loss: 1.139015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [194560/500434 (39%)] Loss: 1.219648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [204800/500434 (41%)] Loss: 1.223197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [215040/500434 (43%)] Loss: 1.204668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [225280/500434 (45%)] Loss: 1.116799\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [235520/500434 (47%)] Loss: 1.108634\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [245760/500434 (49%)] Loss: 1.172585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [256000/500434 (51%)] Loss: 1.146855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [266240/500434 (53%)] Loss: 1.082826\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [276480/500434 (55%)] Loss: 1.182438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [286720/500434 (57%)] Loss: 1.134485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [296960/500434 (59%)] Loss: 1.103481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [307200/500434 (61%)] Loss: 1.191198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [317440/500434 (63%)] Loss: 1.155679\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [327680/500434 (65%)] Loss: 1.087173\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [337920/500434 (67%)] Loss: 1.181204\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [348160/500434 (70%)] Loss: 1.199995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [358400/500434 (72%)] Loss: 1.136648\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [368640/500434 (74%)] Loss: 1.115911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [378880/500434 (76%)] Loss: 1.220709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [389120/500434 (78%)] Loss: 1.100397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [399360/500434 (80%)] Loss: 1.089760\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [409600/500434 (82%)] Loss: 1.231785\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [419840/500434 (84%)] Loss: 1.128938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [430080/500434 (86%)] Loss: 1.262690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [440320/500434 (88%)] Loss: 1.198668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [450560/500434 (90%)] Loss: 1.096802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [460800/500434 (92%)] Loss: 1.193848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [471040/500434 (94%)] Loss: 1.113786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [481280/500434 (96%)] Loss: 1.176720\u001b[0m\n",
      "\u001b[34mTrain Epoch: 192 [491520/500434 (98%)] Loss: 1.135516\u001b[0m\n",
      "\u001b[34mcurrent epoch: 193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [0/500434 (0%)] Loss: 1.205289\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [10240/500434 (2%)] Loss: 1.173178\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [20480/500434 (4%)] Loss: 1.187235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [30720/500434 (6%)] Loss: 1.055216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [40960/500434 (8%)] Loss: 1.160025\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [51200/500434 (10%)] Loss: 1.186606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [61440/500434 (12%)] Loss: 1.121777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [71680/500434 (14%)] Loss: 1.108786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [81920/500434 (16%)] Loss: 1.171585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [92160/500434 (18%)] Loss: 1.072320\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [102400/500434 (20%)] Loss: 1.196229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [112640/500434 (22%)] Loss: 1.098392\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [122880/500434 (25%)] Loss: 1.186212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [133120/500434 (27%)] Loss: 1.200577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [143360/500434 (29%)] Loss: 1.180419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [153600/500434 (31%)] Loss: 1.198587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [163840/500434 (33%)] Loss: 1.252164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [174080/500434 (35%)] Loss: 1.149454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [184320/500434 (37%)] Loss: 1.097525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [194560/500434 (39%)] Loss: 1.174758\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [204800/500434 (41%)] Loss: 1.201431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [215040/500434 (43%)] Loss: 1.298533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [225280/500434 (45%)] Loss: 1.190554\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [235520/500434 (47%)] Loss: 1.125527\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [245760/500434 (49%)] Loss: 1.140473\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [256000/500434 (51%)] Loss: 1.057012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [266240/500434 (53%)] Loss: 1.140275\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [276480/500434 (55%)] Loss: 1.132223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [286720/500434 (57%)] Loss: 1.226395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [296960/500434 (59%)] Loss: 1.148154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [307200/500434 (61%)] Loss: 1.125636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [317440/500434 (63%)] Loss: 1.077534\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [327680/500434 (65%)] Loss: 1.114450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [337920/500434 (67%)] Loss: 1.204775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [348160/500434 (70%)] Loss: 1.214716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [358400/500434 (72%)] Loss: 1.144092\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [368640/500434 (74%)] Loss: 1.177801\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [378880/500434 (76%)] Loss: 1.217886\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [389120/500434 (78%)] Loss: 1.134152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [399360/500434 (80%)] Loss: 1.257417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [409600/500434 (82%)] Loss: 1.159674\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [419840/500434 (84%)] Loss: 1.153618\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [430080/500434 (86%)] Loss: 1.080613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [440320/500434 (88%)] Loss: 1.110495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [450560/500434 (90%)] Loss: 1.229214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [460800/500434 (92%)] Loss: 1.038384\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [471040/500434 (94%)] Loss: 1.146222\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [481280/500434 (96%)] Loss: 1.100528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 193 [491520/500434 (98%)] Loss: 1.228468\u001b[0m\n",
      "\u001b[34mcurrent epoch: 194\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [0/500434 (0%)] Loss: 1.229833\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [10240/500434 (2%)] Loss: 1.114849\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [20480/500434 (4%)] Loss: 1.237250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [30720/500434 (6%)] Loss: 1.100010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [40960/500434 (8%)] Loss: 1.249986\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [51200/500434 (10%)] Loss: 1.212116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [61440/500434 (12%)] Loss: 1.144494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [71680/500434 (14%)] Loss: 1.159167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [81920/500434 (16%)] Loss: 1.152536\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [92160/500434 (18%)] Loss: 1.111565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [102400/500434 (20%)] Loss: 1.097593\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [112640/500434 (22%)] Loss: 1.089193\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [122880/500434 (25%)] Loss: 1.069828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [133120/500434 (27%)] Loss: 1.160722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [143360/500434 (29%)] Loss: 1.014619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [153600/500434 (31%)] Loss: 1.154245\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [163840/500434 (33%)] Loss: 1.148940\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [174080/500434 (35%)] Loss: 1.244847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [184320/500434 (37%)] Loss: 1.180416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [194560/500434 (39%)] Loss: 1.163492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [204800/500434 (41%)] Loss: 1.162296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [215040/500434 (43%)] Loss: 1.088492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [225280/500434 (45%)] Loss: 1.138583\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [235520/500434 (47%)] Loss: 1.233288\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [245760/500434 (49%)] Loss: 1.210118\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [256000/500434 (51%)] Loss: 1.161377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [266240/500434 (53%)] Loss: 1.126482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [276480/500434 (55%)] Loss: 1.196992\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [286720/500434 (57%)] Loss: 1.179448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [296960/500434 (59%)] Loss: 1.099846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [307200/500434 (61%)] Loss: 1.089390\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [317440/500434 (63%)] Loss: 1.154566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [327680/500434 (65%)] Loss: 1.166078\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [337920/500434 (67%)] Loss: 1.239443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [348160/500434 (70%)] Loss: 1.163386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [358400/500434 (72%)] Loss: 1.201837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [368640/500434 (74%)] Loss: 1.218859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [378880/500434 (76%)] Loss: 1.194242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [389120/500434 (78%)] Loss: 1.142694\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [399360/500434 (80%)] Loss: 1.177997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [409600/500434 (82%)] Loss: 1.189020\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [419840/500434 (84%)] Loss: 1.222574\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [430080/500434 (86%)] Loss: 1.127542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [440320/500434 (88%)] Loss: 1.186192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [450560/500434 (90%)] Loss: 1.176442\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [460800/500434 (92%)] Loss: 1.146190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [471040/500434 (94%)] Loss: 1.171813\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [481280/500434 (96%)] Loss: 1.109491\u001b[0m\n",
      "\u001b[34mTrain Epoch: 194 [491520/500434 (98%)] Loss: 1.193634\u001b[0m\n",
      "\u001b[34mcurrent epoch: 195\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [0/500434 (0%)] Loss: 1.178948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [10240/500434 (2%)] Loss: 1.128498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [20480/500434 (4%)] Loss: 1.149605\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [30720/500434 (6%)] Loss: 1.159066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [40960/500434 (8%)] Loss: 1.150104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [51200/500434 (10%)] Loss: 1.065137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [61440/500434 (12%)] Loss: 1.119283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [71680/500434 (14%)] Loss: 1.175176\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [81920/500434 (16%)] Loss: 1.069837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [92160/500434 (18%)] Loss: 1.109209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [102400/500434 (20%)] Loss: 1.102198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [112640/500434 (22%)] Loss: 1.295346\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [122880/500434 (25%)] Loss: 1.115348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [133120/500434 (27%)] Loss: 1.148662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [143360/500434 (29%)] Loss: 1.296851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [153600/500434 (31%)] Loss: 1.149802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [163840/500434 (33%)] Loss: 1.095671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [174080/500434 (35%)] Loss: 1.058934\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [184320/500434 (37%)] Loss: 1.092237\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [194560/500434 (39%)] Loss: 1.046089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [204800/500434 (41%)] Loss: 1.240177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [215040/500434 (43%)] Loss: 1.265223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [225280/500434 (45%)] Loss: 1.127788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [235520/500434 (47%)] Loss: 1.072659\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [245760/500434 (49%)] Loss: 1.131595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [256000/500434 (51%)] Loss: 1.215312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [266240/500434 (53%)] Loss: 1.155105\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [276480/500434 (55%)] Loss: 1.106820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [286720/500434 (57%)] Loss: 1.078251\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [296960/500434 (59%)] Loss: 1.126560\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [307200/500434 (61%)] Loss: 1.142989\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [317440/500434 (63%)] Loss: 1.320066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [327680/500434 (65%)] Loss: 1.119088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [337920/500434 (67%)] Loss: 1.051281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [348160/500434 (70%)] Loss: 1.180498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [358400/500434 (72%)] Loss: 1.156896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [368640/500434 (74%)] Loss: 1.112326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [378880/500434 (76%)] Loss: 1.163067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [389120/500434 (78%)] Loss: 1.021575\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [399360/500434 (80%)] Loss: 1.079323\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [409600/500434 (82%)] Loss: 1.166256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [419840/500434 (84%)] Loss: 1.226359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [430080/500434 (86%)] Loss: 1.254717\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [440320/500434 (88%)] Loss: 1.198658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [450560/500434 (90%)] Loss: 1.351932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [460800/500434 (92%)] Loss: 1.094359\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [471040/500434 (94%)] Loss: 1.147216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [481280/500434 (96%)] Loss: 1.191421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 195 [491520/500434 (98%)] Loss: 1.213434\u001b[0m\n",
      "\u001b[34mcurrent epoch: 196\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [0/500434 (0%)] Loss: 1.130001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [10240/500434 (2%)] Loss: 1.183393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [20480/500434 (4%)] Loss: 1.080119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [30720/500434 (6%)] Loss: 1.139965\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [40960/500434 (8%)] Loss: 1.160372\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [51200/500434 (10%)] Loss: 1.237233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [61440/500434 (12%)] Loss: 1.204775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [71680/500434 (14%)] Loss: 1.051418\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [81920/500434 (16%)] Loss: 1.081064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [92160/500434 (18%)] Loss: 1.258227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [102400/500434 (20%)] Loss: 1.088603\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [112640/500434 (22%)] Loss: 1.077856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [122880/500434 (25%)] Loss: 1.248079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [133120/500434 (27%)] Loss: 1.090152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [143360/500434 (29%)] Loss: 1.219960\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [153600/500434 (31%)] Loss: 1.170010\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [163840/500434 (33%)] Loss: 1.134476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [174080/500434 (35%)] Loss: 1.166229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [184320/500434 (37%)] Loss: 1.164164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [194560/500434 (39%)] Loss: 1.129719\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [204800/500434 (41%)] Loss: 1.128836\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [215040/500434 (43%)] Loss: 1.220002\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [225280/500434 (45%)] Loss: 1.149401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [235520/500434 (47%)] Loss: 1.078983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [245760/500434 (49%)] Loss: 1.168961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [256000/500434 (51%)] Loss: 1.127325\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [266240/500434 (53%)] Loss: 1.126846\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [276480/500434 (55%)] Loss: 1.198492\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [286720/500434 (57%)] Loss: 1.165215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [296960/500434 (59%)] Loss: 1.140553\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [307200/500434 (61%)] Loss: 1.203713\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [317440/500434 (63%)] Loss: 1.160425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [327680/500434 (65%)] Loss: 1.208576\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [337920/500434 (67%)] Loss: 1.280525\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [348160/500434 (70%)] Loss: 1.261376\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [358400/500434 (72%)] Loss: 1.195365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [368640/500434 (74%)] Loss: 1.210497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [378880/500434 (76%)] Loss: 1.180985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [389120/500434 (78%)] Loss: 1.224744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [399360/500434 (80%)] Loss: 1.059978\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [409600/500434 (82%)] Loss: 1.125893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [419840/500434 (84%)] Loss: 1.104941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [430080/500434 (86%)] Loss: 1.080402\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [440320/500434 (88%)] Loss: 1.089738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [450560/500434 (90%)] Loss: 1.159693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [460800/500434 (92%)] Loss: 1.183652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [471040/500434 (94%)] Loss: 1.132627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [481280/500434 (96%)] Loss: 1.131599\u001b[0m\n",
      "\u001b[34mTrain Epoch: 196 [491520/500434 (98%)] Loss: 1.077366\u001b[0m\n",
      "\u001b[34mcurrent epoch: 197\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [0/500434 (0%)] Loss: 1.144023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [10240/500434 (2%)] Loss: 1.196935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [20480/500434 (4%)] Loss: 1.154621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [30720/500434 (6%)] Loss: 1.141296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [40960/500434 (8%)] Loss: 1.050124\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [51200/500434 (10%)] Loss: 1.162413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [61440/500434 (12%)] Loss: 1.123910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [71680/500434 (14%)] Loss: 1.144626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [81920/500434 (16%)] Loss: 1.135616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [92160/500434 (18%)] Loss: 1.109582\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [102400/500434 (20%)] Loss: 1.197219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [112640/500434 (22%)] Loss: 1.105991\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [122880/500434 (25%)] Loss: 1.172073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [133120/500434 (27%)] Loss: 1.144230\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [143360/500434 (29%)] Loss: 1.199350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [153600/500434 (31%)] Loss: 1.108812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [163840/500434 (33%)] Loss: 1.075690\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [174080/500434 (35%)] Loss: 1.160270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [184320/500434 (37%)] Loss: 1.097843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [194560/500434 (39%)] Loss: 1.257233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [204800/500434 (41%)] Loss: 1.230840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [215040/500434 (43%)] Loss: 1.084022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [225280/500434 (45%)] Loss: 1.044653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [235520/500434 (47%)] Loss: 1.076619\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [245760/500434 (49%)] Loss: 1.162735\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [256000/500434 (51%)] Loss: 1.166338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [266240/500434 (53%)] Loss: 1.155139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [276480/500434 (55%)] Loss: 1.275616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [286720/500434 (57%)] Loss: 1.096139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [296960/500434 (59%)] Loss: 1.118653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [307200/500434 (61%)] Loss: 1.170351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [317440/500434 (63%)] Loss: 1.168608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [327680/500434 (65%)] Loss: 1.183382\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [337920/500434 (67%)] Loss: 1.228555\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [348160/500434 (70%)] Loss: 1.150138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [358400/500434 (72%)] Loss: 1.132465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [368640/500434 (74%)] Loss: 1.199660\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [378880/500434 (76%)] Loss: 1.083928\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [389120/500434 (78%)] Loss: 1.190627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [399360/500434 (80%)] Loss: 1.193725\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [409600/500434 (82%)] Loss: 1.106956\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [419840/500434 (84%)] Loss: 1.184807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [430080/500434 (86%)] Loss: 1.225244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [440320/500434 (88%)] Loss: 1.067482\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [450560/500434 (90%)] Loss: 1.251942\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [460800/500434 (92%)] Loss: 1.158423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [471040/500434 (94%)] Loss: 1.100229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [481280/500434 (96%)] Loss: 1.094413\u001b[0m\n",
      "\u001b[34mTrain Epoch: 197 [491520/500434 (98%)] Loss: 1.173358\u001b[0m\n",
      "\u001b[34mcurrent epoch: 198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [0/500434 (0%)] Loss: 1.160892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [10240/500434 (2%)] Loss: 1.222374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [20480/500434 (4%)] Loss: 1.073098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [30720/500434 (6%)] Loss: 1.238985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [40960/500434 (8%)] Loss: 1.086681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [51200/500434 (10%)] Loss: 1.121745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [61440/500434 (12%)] Loss: 1.108374\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [71680/500434 (14%)] Loss: 1.148636\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [81920/500434 (16%)] Loss: 1.101673\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [92160/500434 (18%)] Loss: 1.164415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [102400/500434 (20%)] Loss: 1.231927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [112640/500434 (22%)] Loss: 1.131440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [122880/500434 (25%)] Loss: 1.241954\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [133120/500434 (27%)] Loss: 1.142062\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [143360/500434 (29%)] Loss: 1.127703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [153600/500434 (31%)] Loss: 1.122412\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [163840/500434 (33%)] Loss: 1.123135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [174080/500434 (35%)] Loss: 1.137990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [184320/500434 (37%)] Loss: 1.239621\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [194560/500434 (39%)] Loss: 1.118253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [204800/500434 (41%)] Loss: 1.090584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [215040/500434 (43%)] Loss: 1.099515\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [225280/500434 (45%)] Loss: 1.185790\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [235520/500434 (47%)] Loss: 1.101147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [245760/500434 (49%)] Loss: 1.244584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [256000/500434 (51%)] Loss: 1.177039\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [266240/500434 (53%)] Loss: 1.046338\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [276480/500434 (55%)] Loss: 1.128268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [286720/500434 (57%)] Loss: 1.071247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [296960/500434 (59%)] Loss: 1.256064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [307200/500434 (61%)] Loss: 1.110139\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [317440/500434 (63%)] Loss: 1.161108\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [327680/500434 (65%)] Loss: 1.054644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [337920/500434 (67%)] Loss: 1.122678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [348160/500434 (70%)] Loss: 1.175294\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [358400/500434 (72%)] Loss: 1.145422\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [368640/500434 (74%)] Loss: 1.028932\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [378880/500434 (76%)] Loss: 1.039873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [389120/500434 (78%)] Loss: 1.144838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [399360/500434 (80%)] Loss: 1.189319\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [409600/500434 (82%)] Loss: 1.119137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [419840/500434 (84%)] Loss: 1.210181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [430080/500434 (86%)] Loss: 1.185350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [440320/500434 (88%)] Loss: 1.113647\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [450560/500434 (90%)] Loss: 1.086950\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [460800/500434 (92%)] Loss: 1.227995\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [471040/500434 (94%)] Loss: 1.281110\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [481280/500434 (96%)] Loss: 1.121743\u001b[0m\n",
      "\u001b[34mTrain Epoch: 198 [491520/500434 (98%)] Loss: 1.173752\u001b[0m\n",
      "\u001b[34mcurrent epoch: 199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [0/500434 (0%)] Loss: 1.146774\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [10240/500434 (2%)] Loss: 1.027624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [20480/500434 (4%)] Loss: 1.198283\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [30720/500434 (6%)] Loss: 1.078542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [40960/500434 (8%)] Loss: 1.137961\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [51200/500434 (10%)] Loss: 1.197976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [61440/500434 (12%)] Loss: 1.132066\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [71680/500434 (14%)] Loss: 1.072677\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [81920/500434 (16%)] Loss: 1.142107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [92160/500434 (18%)] Loss: 1.217757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [102400/500434 (20%)] Loss: 1.068848\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [112640/500434 (22%)] Loss: 1.102433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [122880/500434 (25%)] Loss: 1.180841\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [133120/500434 (27%)] Loss: 1.188099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [143360/500434 (29%)] Loss: 1.105075\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [153600/500434 (31%)] Loss: 1.209788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [163840/500434 (33%)] Loss: 1.143898\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [174080/500434 (35%)] Loss: 1.103281\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [184320/500434 (37%)] Loss: 1.078616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [194560/500434 (39%)] Loss: 1.154227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [204800/500434 (41%)] Loss: 1.207814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [215040/500434 (43%)] Loss: 1.121164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [225280/500434 (45%)] Loss: 1.051485\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [235520/500434 (47%)] Loss: 1.153381\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [245760/500434 (49%)] Loss: 1.190255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [256000/500434 (51%)] Loss: 1.099203\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [266240/500434 (53%)] Loss: 1.066431\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [276480/500434 (55%)] Loss: 1.097211\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [286720/500434 (57%)] Loss: 1.049385\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [296960/500434 (59%)] Loss: 1.130855\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [307200/500434 (61%)] Loss: 1.097963\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [317440/500434 (63%)] Loss: 1.171498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [327680/500434 (65%)] Loss: 1.169999\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [337920/500434 (67%)] Loss: 1.136433\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [348160/500434 (70%)] Loss: 1.139342\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [358400/500434 (72%)] Loss: 1.088662\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [368640/500434 (74%)] Loss: 1.115906\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [378880/500434 (76%)] Loss: 1.125298\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [389120/500434 (78%)] Loss: 1.097011\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [399360/500434 (80%)] Loss: 1.269862\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [409600/500434 (82%)] Loss: 1.089437\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [419840/500434 (84%)] Loss: 1.149198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [430080/500434 (86%)] Loss: 1.135181\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [440320/500434 (88%)] Loss: 1.201343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [450560/500434 (90%)] Loss: 1.141510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [460800/500434 (92%)] Loss: 1.163038\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [471040/500434 (94%)] Loss: 1.159107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [481280/500434 (96%)] Loss: 1.138370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 199 [491520/500434 (98%)] Loss: 1.057634\u001b[0m\n",
      "\u001b[34mcurrent epoch: 200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [0/500434 (0%)] Loss: 1.074794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [10240/500434 (2%)] Loss: 1.163265\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [20480/500434 (4%)] Loss: 1.118653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [30720/500434 (6%)] Loss: 1.086684\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [40960/500434 (8%)] Loss: 1.018051\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [51200/500434 (10%)] Loss: 1.090157\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [61440/500434 (12%)] Loss: 1.113134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [71680/500434 (14%)] Loss: 1.169693\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [81920/500434 (16%)] Loss: 1.064947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [92160/500434 (18%)] Loss: 1.168506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [102400/500434 (20%)] Loss: 1.065391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [112640/500434 (22%)] Loss: 1.217147\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [122880/500434 (25%)] Loss: 0.987852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [133120/500434 (27%)] Loss: 1.211755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [143360/500434 (29%)] Loss: 1.208158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [153600/500434 (31%)] Loss: 1.105179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [163840/500434 (33%)] Loss: 1.128588\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [174080/500434 (35%)] Loss: 1.112252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [184320/500434 (37%)] Loss: 1.214386\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [194560/500434 (39%)] Loss: 1.150244\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [204800/500434 (41%)] Loss: 1.098705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [215040/500434 (43%)] Loss: 1.169983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [225280/500434 (45%)] Loss: 1.174666\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [235520/500434 (47%)] Loss: 1.168993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [245760/500434 (49%)] Loss: 1.079936\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [256000/500434 (51%)] Loss: 1.126610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [266240/500434 (53%)] Loss: 1.119444\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [276480/500434 (55%)] Loss: 1.064598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [286720/500434 (57%)] Loss: 1.195479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [296960/500434 (59%)] Loss: 1.173158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [307200/500434 (61%)] Loss: 1.070264\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [317440/500434 (63%)] Loss: 1.194000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [327680/500434 (65%)] Loss: 1.099913\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [337920/500434 (67%)] Loss: 1.160615\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [348160/500434 (70%)] Loss: 1.201200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [358400/500434 (72%)] Loss: 0.991023\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [368640/500434 (74%)] Loss: 1.146088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [378880/500434 (76%)] Loss: 1.121063\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [389120/500434 (78%)] Loss: 1.217497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [399360/500434 (80%)] Loss: 1.102471\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [409600/500434 (82%)] Loss: 1.094199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [419840/500434 (84%)] Loss: 1.176598\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [430080/500434 (86%)] Loss: 1.109344\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [440320/500434 (88%)] Loss: 1.222368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [450560/500434 (90%)] Loss: 1.188529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [460800/500434 (92%)] Loss: 1.240452\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [471040/500434 (94%)] Loss: 1.263746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [481280/500434 (96%)] Loss: 1.055565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 200 [491520/500434 (98%)] Loss: 1.066490\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 0, Accuracy: 0.4375, Balanced Accuracy: 0.38666666666666666\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 10, Accuracy: 0.6748046875, Balanced Accuracy: 0.47064295334758555\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 20, Accuracy: 0.69921875, Balanced Accuracy: 0.5472292003626255\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 30, Accuracy: 0.681640625, Balanced Accuracy: 0.5070755090435596\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 40, Accuracy: 0.720703125, Balanced Accuracy: 0.5783993817422237\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 50, Accuracy: 0.6875, Balanced Accuracy: 0.5189798457509982\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 60, Accuracy: 0.7119140625, Balanced Accuracy: 0.504910206450649\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 70, Accuracy: 0.7041015625, Balanced Accuracy: 0.5296225434166382\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 80, Accuracy: 0.70703125, Balanced Accuracy: 0.516721479987588\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 90, Accuracy: 0.7275390625, Balanced Accuracy: 0.5437054309615409\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 100, Accuracy: 0.732421875, Balanced Accuracy: 0.5574490988577459\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 110, Accuracy: 0.7041015625, Balanced Accuracy: 0.5240325115795923\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 120, Accuracy: 0.73046875, Balanced Accuracy: 0.5250565862589706\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 130, Accuracy: 0.728515625, Balanced Accuracy: 0.5387238239459043\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 140, Accuracy: 0.724609375, Balanced Accuracy: 0.5212604696729194\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 150, Accuracy: 0.72265625, Balanced Accuracy: 0.5169463424968973\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 160, Accuracy: 0.712890625, Balanced Accuracy: 0.5014501373939692\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 170, Accuracy: 0.6884765625, Balanced Accuracy: 0.5281853406593903\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 180, Accuracy: 0.69921875, Balanced Accuracy: 0.51047304588191\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 190, Accuracy: 0.712890625, Balanced Accuracy: 0.534588462644004\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 200, Accuracy: 0.7119140625, Balanced Accuracy: 0.5291336796427434\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 210, Accuracy: 0.701171875, Balanced Accuracy: 0.48093232026799926\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 220, Accuracy: 0.6923828125, Balanced Accuracy: 0.5197641578771384\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 230, Accuracy: 0.6943359375, Balanced Accuracy: 0.4693791848366211\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 240, Accuracy: 0.7041015625, Balanced Accuracy: 0.5132011494767894\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 250, Accuracy: 0.697265625, Balanced Accuracy: 0.5052853735503379\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 260, Accuracy: 0.7001953125, Balanced Accuracy: 0.5189760585413394\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 270, Accuracy: 0.681640625, Balanced Accuracy: 0.4985933144668324\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 280, Accuracy: 0.708984375, Balanced Accuracy: 0.516965378843315\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 290, Accuracy: 0.7041015625, Balanced Accuracy: 0.5310431364707769\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 300, Accuracy: 0.70703125, Balanced Accuracy: 0.5096196641423828\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 310, Accuracy: 0.685546875, Balanced Accuracy: 0.504425678144859\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 320, Accuracy: 0.701171875, Balanced Accuracy: 0.4975245041016019\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 330, Accuracy: 0.724609375, Balanced Accuracy: 0.552469238117431\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 340, Accuracy: 0.7099609375, Balanced Accuracy: 0.5492346661131674\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 350, Accuracy: 0.6923828125, Balanced Accuracy: 0.49836755598414245\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 360, Accuracy: 0.7119140625, Balanced Accuracy: 0.5232036270905721\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 370, Accuracy: 0.6962890625, Balanced Accuracy: 0.4836553970027348\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 380, Accuracy: 0.71484375, Balanced Accuracy: 0.5299041924529639\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 390, Accuracy: 0.6796875, Balanced Accuracy: 0.5088770943979279\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 400, Accuracy: 0.6953125, Balanced Accuracy: 0.49829603525050176\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 410, Accuracy: 0.7060546875, Balanced Accuracy: 0.5498134502970604\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 420, Accuracy: 0.703125, Balanced Accuracy: 0.5118208912459026\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 430, Accuracy: 0.6943359375, Balanced Accuracy: 0.498763982448086\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 440, Accuracy: 0.7109375, Balanced Accuracy: 0.5548851513887215\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 450, Accuracy: 0.71875, Balanced Accuracy: 0.5200445436072918\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 460, Accuracy: 0.7216796875, Balanced Accuracy: 0.5234819706380843\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 470, Accuracy: 0.7294921875, Balanced Accuracy: 0.547424900062841\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest step: 480, Accuracy: 0.7119140625, Balanced Accuracy: 0.5127564368009921\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2006: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\u001b[0m\n",
      "\u001b[34mTest set: Accuracy:  0.7036172602181306 Balanced Accuracy: 0.5161409937757544\u001b[0m\n",
      "\u001b[34m2022-09-18 04:23:29,242 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-09-18 04:23:29,242 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-09-18 04:23:29,243 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-09-18 04:23:51 Uploading - Uploading generated training model\n",
      "2022-09-18 04:23:51 Completed - Training job completed\n",
      "Training seconds: 2564\n",
      "Billable seconds: 2564\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training':data_input}, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b05df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_accuracy = 0.7036172602181306\n",
    "rnn_bal_acc = 0.5161409937757544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097d8f6",
   "metadata": {},
   "source": [
    "# Distilbert HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ee096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0539c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://unspsc-data/segment_training'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b228d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc0d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='unspsc_distilbert_sagemaker_hpo.py',\n",
    "                                    instance_count=1,\n",
    "                                    instance_type=\"ml.g4dn.xlarge\",\n",
    "                                    transformers_version='4.12',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "                                    role=role)\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(1e-5, 1e-3),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128]),\n",
    "    \"epochs\": IntegerParameter(1, 2),\n",
    "    'eps': ContinuousParameter(1e-8, 1e-7)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"Balanced Accuracy Final:\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": \"Balanced Accuracy Final:\", \"Regex\": \"Balanced Accuracy Final: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    huggingface_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34193ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\"training\": data_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5625ce8",
   "metadata": {},
   "source": [
    "# Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aaeb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tuner.attach(tuning_job_name='huggingface-pytorch--220910-1109')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874ef68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6992034316062927"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best hyperparameter outcome.\n",
    "tuner.describe()['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce6d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-09-10 18:53:30 Starting - Found matching resource for reuse\n",
      "2022-09-10 18:53:30 Downloading - Downloading input data\n",
      "2022-09-10 18:53:30 Training - Training image download completed. Training in progress.\n",
      "2022-09-10 18:53:30 Uploading - Uploading generated training model\n",
      "2022-09-10 18:53:30 Completed - Resource released due to keep alive period expiry\n"
     ]
    }
   ],
   "source": [
    "best_estimator= tuner.best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71263671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_tuning_objective_metric': '\"Balanced Accuracy Final:\"', 'batch-size': '\"128\"', 'epochs': '2', 'eps': '3.848853581472183e-08', 'lr': '9.300726878687543e-05', 'sagemaker_container_log_level': '20', 'sagemaker_estimator_class_name': '\"HuggingFace\"', 'sagemaker_estimator_module': '\"sagemaker.huggingface.estimator\"', 'sagemaker_job_name': '\"huggingface-pytorch-training-2022-09-10-11-09-13-895\"', 'sagemaker_program': '\"unspsc_distilbert_sagemaker_hpo.py\"', 'sagemaker_region': '\"us-east-1\"', 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-10-11-09-13-895/source/sourcedir.tar.gz\"'}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = best_estimator.hyperparameters()\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ddee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_hyperparameter_names = ['batch-size', 'epochs', 'eps', 'lr']\n",
    "\n",
    "hyperparameters_for_profiling = {key:value for key, value in hyperparameters.items() if key in actual_hyperparameter_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8c126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch-size': '\"128\"',\n",
       " 'epochs': '2',\n",
       " 'eps': '3.848853581472183e-08',\n",
       " 'lr': '9.300726878687543e-05'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_for_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb409ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for restart\n",
    "\n",
    "hyperparameters_for_profiling = {'batch-size': '\"128\"',\n",
    "                                 'epochs': '2',\n",
    "                                 'eps': '3.848853581472183e-08',\n",
    "                                 'lr': '9.300726878687543e-05'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495327c5",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5815e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs, CollectionConfig\n",
    "from sagemaker.debugger import DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    "\n",
    "rules = [Rule.sagemaker(rule_configs.vanishing_gradient(), collections_to_save=[CollectionConfig(name = 'gradients',\n",
    "                                                                                                        parameters = {\n",
    "                                                                                                            'train.save_interval': '50',\n",
    "                                                                                                            'eval.save_interval': '5'\n",
    "                                                                                                        }\n",
    "                                                                                                        )]),\n",
    "         Rule.sagemaker(rule_configs.overfit()),\n",
    "         Rule.sagemaker(rule_configs.overtraining()),\n",
    "         Rule.sagemaker(rule_configs.poor_weight_initialization(), collections_to_save=[CollectionConfig(name = 'weights',\n",
    "                                                                                                        parameters = {\n",
    "                                                                                                            'train.save_interval': '50',\n",
    "                                                                                                            'eval.save_interval': '5'\n",
    "                                                                                                        }\n",
    "                                                                                                        )]),\n",
    "         Rule.sagemaker(rule_configs.loss_not_decreasing(), collections_to_save=[CollectionConfig(name=\"losses\",\n",
    "                                                                                                  parameters={\n",
    "                                                                                                      \"train.save_interval\": \"50\",\n",
    "                                                                                                      \"eval.save_interval\": \"5\"\n",
    "                                                                                                  }\n",
    "                                                                                                 )]),\n",
    "         ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "         ProfilerRule.sagemaker(rule_configs.ProfilerReport())\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "profiler_config = ProfilerConfig(system_monitor_interval_millis=500, \n",
    "                                 framework_profile_params=FrameworkProfile(num_steps=10))\n",
    "\n",
    "debugger_config = DebuggerHookConfig(hook_parameters={\"train.save_interval\": \"50\",\n",
    "                                                      \"eval.save_interval\": \"5\"\n",
    "                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3f331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_profile = HuggingFace(entry_point='train_distilbert.py',\n",
    "                                    instance_count=1,\n",
    "                                    instance_type=\"ml.g4dn.xlarge\",\n",
    "                                    hyperparameters=hyperparameters_for_profiling,\n",
    "                                    transformers_version='4.12',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "                                    role=role, \n",
    "                                    profiler_config = profiler_config,\n",
    "                                    debugger_hook_config= debugger_config,\n",
    "                                    rules = rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b7e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-12 10:18:39 Starting - Starting the training job...\n",
      "2022-09-12 10:19:07 Starting - Preparing the instances for trainingVanishingGradient: InProgress\n",
      "Overfit: InProgress\n",
      "Overtraining: InProgress\n",
      "PoorWeightInitialization: InProgress\n",
      "LossNotDecreasing: InProgress\n",
      "LowGPUUtilization: InProgress\n",
      "ProfilerReport: InProgress\n",
      "......\n",
      "2022-09-12 10:20:08 Downloading - Downloading input data...\n",
      "2022-09-12 10:20:37 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-12 10:24:46,987 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-12 10:24:47,018 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-12 10:24:47,025 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-12 10:24:47,593 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": \"\\\"128\\\"\",\n",
      "        \"epochs\": \"2\",\n",
      "        \"eps\": \"3.848853581472183e-08\",\n",
      "        \"lr\": \"9.300726878687543e-05\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-09-12-10-18-38-954\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-12-10-18-38-954/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_distilbert\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_distilbert.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":\"\\\"128\\\"\",\"epochs\":\"2\",\"eps\":\"3.848853581472183e-08\",\"lr\":\"9.300726878687543e-05\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_distilbert.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_distilbert\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-12-10-18-38-954/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":\"\\\"128\\\"\",\"epochs\":\"2\",\"eps\":\"3.848853581472183e-08\",\"lr\":\"9.300726878687543e-05\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-09-12-10-18-38-954\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-12-10-18-38-954/source/sourcedir.tar.gz\",\"module_name\":\"train_distilbert\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_distilbert.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"\\\"128\\\"\",\"--epochs\",\"2\",\"--eps\",\"3.848853581472183e-08\",\"--lr\",\"9.300726878687543e-05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=\"128\"\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_EPS=3.848853581472183e-08\u001b[0m\n",
      "\u001b[34mSM_HP_LR=9.300726878687543e-05\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_distilbert.py --batch-size \"128\" --epochs 2 --eps 3.848853581472183e-08 --lr 9.300726878687543e-05\u001b[0m\n",
      "\u001b[34m[2022-09-12 10:24:51.619 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-09-12 10:24:51.718 algo-1:27 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 15.7kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 8.26MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 483/483 [00:00<00:00, 378kB/s]\u001b[0m\n",
      "\u001b[34m[2022-09-12 10:24:52.469 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-12 10:24:52.471 algo-1:27 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-12 10:24:52.473 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-12 10:24:52.473 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\n",
      "2022-09-12 10:25:08 Training - Training image download completed. Training in progress.LowGPUUtilization: IssuesFound\n",
      "ProfilerReport: InProgress\n",
      "\u001b[34mDownloading:   0%|          | 0.00/256M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 802k/256M [00:00<00:32, 8.20MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 1.66M/256M [00:00<00:30, 8.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 2.56M/256M [00:00<00:29, 9.10MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|▏         | 3.43M/256M [00:00<00:30, 8.56MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 4.25M/256M [00:00<00:31, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 5.08M/256M [00:00<00:30, 8.52MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 5.94M/256M [00:00<00:30, 8.67MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 6.77M/256M [00:00<00:30, 8.59MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 7.59M/256M [00:00<00:30, 8.40MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 8.39M/256M [00:01<00:30, 8.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▎         | 9.23M/256M [00:01<00:30, 8.52MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 10.0M/256M [00:01<00:31, 8.10MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 10.8M/256M [00:01<00:31, 8.12MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 11.6M/256M [00:01<00:31, 8.22MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 12.5M/256M [00:01<00:30, 8.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 13.3M/256M [00:01<00:30, 8.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 14.1M/256M [00:01<00:30, 8.25MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 14.9M/256M [00:01<00:30, 8.33MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 15.8M/256M [00:01<00:29, 8.48MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 16.6M/256M [00:02<00:28, 8.70MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 17.5M/256M [00:02<00:27, 8.96MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 18.4M/256M [00:02<00:27, 8.95MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 19.3M/256M [00:02<00:27, 9.00MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 20.1M/256M [00:02<00:28, 8.68MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 21.0M/256M [00:02<00:29, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▊         | 21.8M/256M [00:02<00:29, 8.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 22.6M/256M [00:02<00:28, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 23.4M/256M [00:02<00:29, 8.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 24.2M/256M [00:03<00:29, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 25.0M/256M [00:03<00:29, 8.07MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 25.8M/256M [00:03<00:29, 8.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 26.6M/256M [00:03<00:29, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 27.5M/256M [00:03<00:28, 8.40MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 28.3M/256M [00:03<00:28, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█▏        | 29.1M/256M [00:03<00:27, 8.55MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 30.0M/256M [00:03<00:26, 8.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 30.9M/256M [00:03<00:26, 9.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 31.8M/256M [00:03<00:26, 8.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 32.7M/256M [00:04<00:26, 8.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 33.5M/256M [00:04<00:25, 9.02MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 34.4M/256M [00:04<00:26, 8.79MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 35.2M/256M [00:04<00:27, 8.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 36.1M/256M [00:04<00:27, 8.51MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 36.9M/256M [00:04<00:27, 8.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 37.7M/256M [00:04<00:27, 8.34MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 38.5M/256M [00:04<00:28, 8.00MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 39.3M/256M [00:04<00:27, 8.19MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 40.2M/256M [00:04<00:26, 8.44MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 41.0M/256M [00:05<00:26, 8.58MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 41.8M/256M [00:05<00:26, 8.36MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 42.7M/256M [00:05<00:25, 8.63MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 43.6M/256M [00:05<00:26, 8.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 44.4M/256M [00:05<00:27, 8.00MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 45.2M/256M [00:05<00:27, 8.08MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 46.1M/256M [00:05<00:25, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 46.9M/256M [00:05<00:25, 8.63MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▊        | 47.8M/256M [00:05<00:24, 8.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 48.7M/256M [00:06<00:25, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 49.5M/256M [00:06<00:25, 8.50MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 50.3M/256M [00:06<00:25, 8.53MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 51.1M/256M [00:06<00:26, 8.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 51.9M/256M [00:06<00:27, 7.84MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 52.7M/256M [00:06<00:26, 7.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 53.4M/256M [00:06<00:26, 7.96MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 54.2M/256M [00:06<00:27, 7.82MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 54.9M/256M [00:06<00:27, 7.77MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 55.7M/256M [00:06<00:27, 7.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 56.5M/256M [00:07<00:26, 7.82MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 57.2M/256M [00:07<00:26, 7.72MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 58.0M/256M [00:07<00:26, 7.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 58.9M/256M [00:07<00:24, 8.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 59.7M/256M [00:07<00:24, 8.34MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 60.5M/256M [00:07<00:25, 8.16MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 61.3M/256M [00:07<00:24, 8.22MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 62.1M/256M [00:07<00:24, 8.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 62.9M/256M [00:07<00:24, 8.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 63.8M/256M [00:07<00:23, 8.65MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 64.6M/256M [00:08<00:24, 8.33MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 65.4M/256M [00:08<00:24, 8.28MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 66.4M/256M [00:08<00:22, 8.67MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▋       | 67.2M/256M [00:08<00:22, 8.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 68.1M/256M [00:08<00:21, 8.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 69.0M/256M [00:08<00:22, 8.81MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 69.8M/256M [00:08<00:23, 8.42MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 70.6M/256M [00:08<00:22, 8.47MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 71.5M/256M [00:08<00:22, 8.42MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 72.3M/256M [00:09<00:22, 8.50MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▊       | 73.1M/256M [00:09<00:22, 8.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 73.9M/256M [00:09<00:23, 8.17MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 74.7M/256M [00:09<00:23, 8.03MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 75.6M/256M [00:09<00:22, 8.32MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 76.3M/256M [00:09<00:22, 8.32MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 77.1M/256M [00:09<00:22, 8.17MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 77.9M/256M [00:09<00:23, 7.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 78.7M/256M [00:09<00:23, 8.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 79.6M/256M [00:09<00:21, 8.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 80.5M/256M [00:10<00:20, 8.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 81.4M/256M [00:10<00:25, 7.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 82.3M/256M [00:10<00:23, 7.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 83.1M/256M [00:10<00:22, 8.09MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 84.0M/256M [00:10<00:21, 8.34MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 84.8M/256M [00:10<00:21, 8.38MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 85.6M/256M [00:10<00:22, 8.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 86.4M/256M [00:10<00:23, 7.69MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 87.1M/256M [00:10<00:22, 7.71MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 88.0M/256M [00:11<00:21, 8.03MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 88.8M/256M [00:11<00:21, 8.26MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 89.7M/256M [00:11<00:20, 8.53MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 90.6M/256M [00:11<00:19, 8.78MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 91.4M/256M [00:11<00:19, 8.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 92.3M/256M [00:11<00:20, 8.53MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▋      | 93.1M/256M [00:11<00:20, 8.21MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 94.0M/256M [00:11<00:20, 8.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 94.8M/256M [00:11<00:19, 8.47MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 95.6M/256M [00:11<00:19, 8.58MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 96.4M/256M [00:12<00:20, 8.19MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 97.2M/256M [00:12<00:20, 8.21MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 98.0M/256M [00:12<00:20, 8.02MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▊      | 98.8M/256M [00:12<00:20, 8.09MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 99.6M/256M [00:12<00:19, 8.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 100M/256M [00:12<00:19, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 101M/256M [00:12<00:19, 8.36MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 102M/256M [00:12<00:19, 8.39MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 103M/256M [00:12<00:20, 7.94MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 104M/256M [00:13<00:19, 8.14MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 105M/256M [00:13<00:18, 8.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████▏     | 105M/256M [00:13<00:18, 8.53MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 106M/256M [00:13<00:17, 8.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 107M/256M [00:13<00:17, 8.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 108M/256M [00:13<00:17, 8.68MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 109M/256M [00:13<00:18, 8.39MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 110M/256M [00:13<00:18, 8.28MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 110M/256M [00:13<00:19, 7.96MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 111M/256M [00:13<00:18, 7.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 112M/256M [00:14<00:17, 8.36MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 113M/256M [00:14<00:18, 8.13MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 114M/256M [00:14<00:19, 7.78MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 114M/256M [00:14<00:18, 7.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 115M/256M [00:14<00:18, 7.75MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 116M/256M [00:14<00:18, 7.86MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 117M/256M [00:14<00:18, 8.05MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 118M/256M [00:14<00:18, 7.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 118M/256M [00:14<00:18, 7.65MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 119M/256M [00:15<00:17, 8.02MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 120M/256M [00:15<00:17, 8.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 121M/256M [00:15<00:16, 8.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 122M/256M [00:15<00:17, 7.92MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 122M/256M [00:15<00:17, 7.79MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 123M/256M [00:15<00:17, 7.84MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▊     | 124M/256M [00:15<00:17, 8.03MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 125M/256M [00:15<00:16, 8.21MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 126M/256M [00:15<00:17, 7.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 126M/256M [00:15<00:17, 7.79MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 127M/256M [00:16<00:17, 7.77MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 128M/256M [00:16<00:16, 8.21MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 129M/256M [00:16<00:16, 8.05MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 130M/256M [00:16<00:16, 7.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 130M/256M [00:16<00:16, 7.93MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████▏    | 131M/256M [00:16<00:16, 7.81MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 132M/256M [00:16<00:15, 8.38MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 133M/256M [00:16<00:15, 8.26MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 134M/256M [00:16<00:15, 8.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 134M/256M [00:17<00:15, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 135M/256M [00:17<00:15, 8.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 136M/256M [00:17<00:15, 8.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▎    | 137M/256M [00:17<00:15, 8.09MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 138M/256M [00:17<00:15, 7.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 138M/256M [00:17<00:15, 8.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 139M/256M [00:17<00:15, 7.78MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 140M/256M [00:17<00:15, 7.81MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 141M/256M [00:17<00:14, 8.25MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 142M/256M [00:17<00:13, 8.56MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 143M/256M [00:18<00:13, 8.63MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 143M/256M [00:18<00:14, 8.35MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▋    | 144M/256M [00:18<00:14, 8.30MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 145M/256M [00:18<00:14, 8.20MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 146M/256M [00:18<00:14, 8.12MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 147M/256M [00:18<00:13, 8.36MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 148M/256M [00:18<00:12, 8.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 148M/256M [00:18<00:12, 8.94MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 149M/256M [00:18<00:12, 9.16MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 150M/256M [00:18<00:11, 9.28MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 151M/256M [00:19<00:11, 9.15MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 152M/256M [00:19<00:12, 8.56MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 153M/256M [00:19<00:13, 8.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 154M/256M [00:19<00:12, 8.36MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 155M/256M [00:19<00:12, 8.44MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 155M/256M [00:19<00:12, 8.28MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 156M/256M [00:19<00:12, 8.27MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████▏   | 157M/256M [00:19<00:12, 8.09MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 158M/256M [00:19<00:13, 7.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 158M/256M [00:20<00:13, 7.55MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 159M/256M [00:20<00:13, 7.57MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 160M/256M [00:20<00:13, 7.67MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 161M/256M [00:20<00:14, 6.86MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 161M/256M [00:20<00:13, 7.18MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 162M/256M [00:20<00:13, 7.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 163M/256M [00:20<00:12, 7.48MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 164M/256M [00:20<00:12, 7.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 165M/256M [00:20<00:12, 7.64MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 165M/256M [00:21<00:11, 8.22MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 166M/256M [00:21<00:11, 8.44MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 167M/256M [00:21<00:10, 8.81MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 168M/256M [00:21<00:10, 8.93MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 169M/256M [00:21<00:09, 9.13MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 170M/256M [00:21<00:09, 9.24MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 171M/256M [00:21<00:09, 9.13MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 172M/256M [00:21<00:09, 8.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 173M/256M [00:21<00:09, 8.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 173M/256M [00:21<00:10, 8.34MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 174M/256M [00:22<00:10, 8.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 175M/256M [00:22<00:10, 8.44MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 176M/256M [00:22<00:10, 8.19MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 177M/256M [00:22<00:10, 8.19MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 177M/256M [00:22<00:09, 8.29MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 178M/256M [00:22<00:10, 8.07MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 179M/256M [00:22<00:10, 7.81MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 180M/256M [00:22<00:10, 7.86MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 181M/256M [00:22<00:10, 7.80MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 181M/256M [00:22<00:09, 7.99MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████▏  | 182M/256M [00:23<00:09, 8.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 183M/256M [00:23<00:09, 8.18MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 184M/256M [00:23<00:09, 8.15MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 185M/256M [00:23<00:08, 8.38MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 185M/256M [00:23<00:08, 8.33MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 186M/256M [00:23<00:08, 8.16MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 187M/256M [00:23<00:08, 8.03MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 188M/256M [00:23<00:08, 8.01MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 188M/256M [00:23<00:08, 7.84MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 189M/256M [00:24<00:09, 7.56MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 190M/256M [00:24<00:09, 7.58MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 191M/256M [00:24<00:08, 7.64MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 191M/256M [00:24<00:08, 7.61MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 192M/256M [00:24<00:08, 7.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 193M/256M [00:24<00:07, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 194M/256M [00:24<00:07, 8.23MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 195M/256M [00:24<00:08, 7.91MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 196M/256M [00:24<00:07, 8.27MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 196M/256M [00:24<00:07, 8.17MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 197M/256M [00:25<00:07, 8.31MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 198M/256M [00:25<00:06, 8.69MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 199M/256M [00:25<00:06, 8.93MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 200M/256M [00:25<00:06, 8.37MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 201M/256M [00:25<00:06, 8.39MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 201M/256M [00:25<00:06, 8.35MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 202M/256M [00:25<00:06, 8.48MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 203M/256M [00:25<00:06, 8.69MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 204M/256M [00:25<00:06, 8.62MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 205M/256M [00:25<00:06, 8.33MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 206M/256M [00:26<00:05, 8.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 207M/256M [00:26<00:05, 8.61MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 207M/256M [00:26<00:05, 8.42MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████▏ | 208M/256M [00:26<00:05, 8.41MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 209M/256M [00:26<00:05, 8.20MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 210M/256M [00:26<00:06, 7.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 211M/256M [00:26<00:06, 7.72MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 211M/256M [00:26<00:06, 7.70MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 212M/256M [00:26<00:05, 7.71MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 213M/256M [00:27<00:05, 7.49MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▎ | 213M/256M [00:27<00:05, 7.54MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 214M/256M [00:27<00:05, 8.07MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 215M/256M [00:27<00:05, 8.06MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 216M/256M [00:27<00:05, 7.77MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 217M/256M [00:27<00:05, 7.88MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 218M/256M [00:27<00:04, 8.18MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 218M/256M [00:27<00:04, 7.81MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 219M/256M [00:27<00:04, 7.97MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 220M/256M [00:27<00:04, 8.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▋ | 221M/256M [00:28<00:04, 7.99MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 221M/256M [00:28<00:04, 8.11MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 222M/256M [00:28<00:04, 8.12MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 223M/256M [00:28<00:04, 8.46MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 224M/256M [00:28<00:03, 8.38MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 225M/256M [00:28<00:03, 8.14MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 226M/256M [00:28<00:03, 8.58MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▊ | 227M/256M [00:28<00:03, 8.75MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 227M/256M [00:28<00:03, 8.69MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 228M/256M [00:28<00:03, 8.74MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 229M/256M [00:29<00:03, 9.02MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 230M/256M [00:29<00:03, 8.69MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 231M/256M [00:29<00:02, 8.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 232M/256M [00:29<00:02, 9.05MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 233M/256M [00:29<00:02, 9.27MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████▏| 234M/256M [00:29<00:02, 8.91MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 234M/256M [00:29<00:02, 8.82MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 235M/256M [00:29<00:02, 9.04MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 236M/256M [00:29<00:02, 8.76MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 237M/256M [00:30<00:02, 8.64MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 238M/256M [00:30<00:02, 8.98MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 239M/256M [00:30<00:02, 8.39MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 240M/256M [00:30<00:01, 8.53MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 241M/256M [00:30<00:01, 8.47MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 241M/256M [00:30<00:01, 7.70MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 242M/256M [00:30<00:01, 7.61MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 243M/256M [00:30<00:01, 8.15MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 244M/256M [00:30<00:01, 8.55MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 245M/256M [00:31<00:01, 8.14MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 246M/256M [00:31<00:01, 7.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▋| 246M/256M [00:31<00:01, 7.92MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 247M/256M [00:31<00:01, 7.91MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 248M/256M [00:31<00:00, 8.22MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 249M/256M [00:31<00:00, 8.59MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 250M/256M [00:31<00:00, 8.34MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 250M/256M [00:31<00:00, 8.16MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 251M/256M [00:31<00:00, 8.03MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▊| 252M/256M [00:31<00:00, 7.99MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 253M/256M [00:32<00:00, 8.43MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 254M/256M [00:32<00:00, 8.52MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 255M/256M [00:32<00:00, 8.26MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 255M/256M [00:32<00:00, 7.95MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 256M/256M [00:32<00:00, 8.27MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mUsing: cuda:0\u001b[0m\n",
      "\u001b[34mINFO:__main__:Using: cuda:0\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.829 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.830 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.830 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.830 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.831 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.831 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.832 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.832 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.832 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.833 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.833 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.833 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.834 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.834 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.834 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.835 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.835 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.835 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.836 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.836 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.836 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.837 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.837 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.837 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.838 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.838 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.838 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.839 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.839 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.839 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.840 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.840 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.841 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.841 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.842 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.842 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.842 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.843 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.843 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.844 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.844 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.844 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.845 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.845 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.845 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.846 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.846 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.846 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.847 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.847 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.847 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.848 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.848 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.849 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.849 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.849 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.850 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.850 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.850 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.851 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.851 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.851 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.852 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.852 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.852 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.853 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.853 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.854 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.854 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.854 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.855 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.855 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.855 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.855 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.856 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.856 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.857 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.857 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.857 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.858 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.858 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.858 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.859 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.859 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.860 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.860 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.861 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.861 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.861 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.862 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.862 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.862 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.863 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.863 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.864 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.864 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.864 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.865 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.865 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.866 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.866 algo-1:27 INFO hook.py:591] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.866 algo-1:27 INFO hook.py:591] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.867 algo-1:27 INFO hook.py:591] name:classifier.weight count_params:43776\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.867 algo-1:27 INFO hook.py:591] name:classifier.bias count_params:57\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.867 algo-1:27 INFO hook.py:593] Total Trainable Params: 66997305\u001b[0m\n",
      "\u001b[34mcurrent epoch: 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:current epoch: 1\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.948 algo-1:27 INFO hook.py:424] Monitoring the collections: gradients, losses, weights\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.949 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/prestepzero-*-start-1662978291718760.5_train-0-stepstart-1662982252949432.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.971 algo-1:27 WARNING hook.py:410] The detailed profiling using autograd profiler is not supported for torch version 1.9.1\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:52.972 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:55.006 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-0-stepstart-1662982252969989.0_train-0-forwardpassend-1662982255005760.0/python_stats.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/500434 (0%)] Loss: 4.046228\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/500434 (0%)] Loss: 4.046228\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:56.591 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-0-forwardpassend-1662982255012035.8_train-1-stepstart-1662982256590945.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:56.596 algo-1:27 WARNING hook.py:410] The detailed profiling using autograd profiler is not supported for torch version 1.9.1\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:56.647 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-1-stepstart-1662982256596389.8_train-1-forwardpassend-1662982256647059.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:57.690 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-1-forwardpassend-1662982256650793.0_train-2-stepstart-1662982257690389.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:57.692 algo-1:27 WARNING hook.py:410] The detailed profiling using autograd profiler is not supported for torch version 1.9.1\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:57.729 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-2-stepstart-1662982257692518.5_train-2-forwardpassend-1662982257729255.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:58.792 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-2-forwardpassend-1662982257732068.5_train-3-stepstart-1662982258792131.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:58.832 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-3-stepstart-1662982258795249.0_train-3-forwardpassend-1662982258832524.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:59.894 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-3-forwardpassend-1662982258834842.8_train-4-stepstart-1662982259893493.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:30:59.935 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-4-stepstart-1662982259896615.0_train-4-forwardpassend-1662982259934612.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:00.986 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-4-forwardpassend-1662982259937070.2_train-5-stepstart-1662982260986050.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:01.029 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-5-stepstart-1662982260988846.8_train-5-forwardpassend-1662982261029235.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:02.094 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-5-forwardpassend-1662982261031857.2_train-6-stepstart-1662982262093977.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:02.137 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-6-stepstart-1662982262096608.5_train-6-forwardpassend-1662982262137243.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:03.196 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-6-forwardpassend-1662982262139800.0_train-7-stepstart-1662982263195523.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:03.240 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-7-stepstart-1662982263199518.2_train-7-forwardpassend-1662982263240186.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:04.297 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-7-forwardpassend-1662982263242496.0_train-8-stepstart-1662982264296827.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:04.345 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-8-stepstart-1662982264299851.5_train-8-forwardpassend-1662982264344693.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:05.404 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-8-forwardpassend-1662982264348066.0_train-9-stepstart-1662982265403915.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:05.448 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-9-stepstart-1662982265406757.5_train-9-forwardpassend-1662982265447867.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-09-12 11:31:06.505 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-9-forwardpassend-1662982265450120.5_train-10-stepstart-1662982266504610.8/python_stats.\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/500434 (0%)] Loss: 3.292271\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1280/500434 (0%)] Loss: 3.292271\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/500434 (1%)] Loss: 2.940822\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2560/500434 (1%)] Loss: 2.940822\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/500434 (1%)] Loss: 2.440665\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3840/500434 (1%)] Loss: 2.440665\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/500434 (1%)] Loss: 2.374924\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5120/500434 (1%)] Loss: 2.374924\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/500434 (1%)] Loss: 2.342410\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/500434 (1%)] Loss: 2.342410\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/500434 (2%)] Loss: 2.148994\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7680/500434 (2%)] Loss: 2.148994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/500434 (2%)] Loss: 2.196857\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8960/500434 (2%)] Loss: 2.196857\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/500434 (2%)] Loss: 2.037091\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10240/500434 (2%)] Loss: 2.037091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/500434 (2%)] Loss: 2.137676\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [11520/500434 (2%)] Loss: 2.137676\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/500434 (3%)] Loss: 2.027149\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/500434 (3%)] Loss: 2.027149\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/500434 (3%)] Loss: 1.859613\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14080/500434 (3%)] Loss: 1.859613\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/500434 (3%)] Loss: 1.762255\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [15360/500434 (3%)] Loss: 1.762255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16640/500434 (3%)] Loss: 1.982746\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16640/500434 (3%)] Loss: 1.982746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17920/500434 (4%)] Loss: 1.519226\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17920/500434 (4%)] Loss: 1.519226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/500434 (4%)] Loss: 1.726519\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/500434 (4%)] Loss: 1.726519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/500434 (4%)] Loss: 1.585879\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [20480/500434 (4%)] Loss: 1.585879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [21760/500434 (4%)] Loss: 1.546106\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [21760/500434 (4%)] Loss: 1.546106\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [23040/500434 (5%)] Loss: 1.619528\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [23040/500434 (5%)] Loss: 1.619528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [24320/500434 (5%)] Loss: 1.817596\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [24320/500434 (5%)] Loss: 1.817596\u001b[0m\n",
      "VanishingGradient: IssuesFound\n",
      "Overfit: InProgress\n",
      "Overtraining: InProgress\n",
      "PoorWeightInitialization: InProgress\n",
      "LossNotDecreasing: InProgress\n",
      "\u001b[34mTrain Epoch: 1 [25600/500434 (5%)] Loss: 1.564031\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/500434 (5%)] Loss: 1.564031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [26880/500434 (5%)] Loss: 1.595121\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [26880/500434 (5%)] Loss: 1.595121\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [28160/500434 (6%)] Loss: 1.404391\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [28160/500434 (6%)] Loss: 1.404391\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [29440/500434 (6%)] Loss: 1.406977\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [29440/500434 (6%)] Loss: 1.406977\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/500434 (6%)] Loss: 1.427091\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [30720/500434 (6%)] Loss: 1.427091\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/500434 (6%)] Loss: 1.461255\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/500434 (6%)] Loss: 1.461255\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [33280/500434 (7%)] Loss: 1.458400\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [33280/500434 (7%)] Loss: 1.458400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [34560/500434 (7%)] Loss: 1.369198\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [34560/500434 (7%)] Loss: 1.369198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [35840/500434 (7%)] Loss: 1.622348\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [35840/500434 (7%)] Loss: 1.622348\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [37120/500434 (7%)] Loss: 1.095519\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [37120/500434 (7%)] Loss: 1.095519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/500434 (8%)] Loss: 1.437746\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/500434 (8%)] Loss: 1.437746\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [39680/500434 (8%)] Loss: 1.241641\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [39680/500434 (8%)] Loss: 1.241641\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/500434 (8%)] Loss: 1.136401\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [40960/500434 (8%)] Loss: 1.136401\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [42240/500434 (8%)] Loss: 1.617361\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [42240/500434 (8%)] Loss: 1.617361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [43520/500434 (9%)] Loss: 1.265727\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [43520/500434 (9%)] Loss: 1.265727\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/500434 (9%)] Loss: 1.543064\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/500434 (9%)] Loss: 1.543064\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [46080/500434 (9%)] Loss: 1.331495\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [46080/500434 (9%)] Loss: 1.331495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [47360/500434 (9%)] Loss: 1.174065\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [47360/500434 (9%)] Loss: 1.174065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [48640/500434 (10%)] Loss: 1.151856\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [48640/500434 (10%)] Loss: 1.151856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [49920/500434 (10%)] Loss: 1.475632\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [49920/500434 (10%)] Loss: 1.475632\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/500434 (10%)] Loss: 1.265885\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/500434 (10%)] Loss: 1.265885\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [52480/500434 (10%)] Loss: 1.060702\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [52480/500434 (10%)] Loss: 1.060702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [53760/500434 (11%)] Loss: 1.502408\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [53760/500434 (11%)] Loss: 1.502408\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [55040/500434 (11%)] Loss: 1.153225\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [55040/500434 (11%)] Loss: 1.153225\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [56320/500434 (11%)] Loss: 1.398788\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [56320/500434 (11%)] Loss: 1.398788\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/500434 (12%)] Loss: 1.145681\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/500434 (12%)] Loss: 1.145681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [58880/500434 (12%)] Loss: 1.304103\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [58880/500434 (12%)] Loss: 1.304103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [60160/500434 (12%)] Loss: 1.080436\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [60160/500434 (12%)] Loss: 1.080436\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [61440/500434 (12%)] Loss: 1.178794\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [61440/500434 (12%)] Loss: 1.178794\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [62720/500434 (13%)] Loss: 1.358933\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [62720/500434 (13%)] Loss: 1.358933\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [64000/500434 (13%)] Loss: 1.239681\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [64000/500434 (13%)] Loss: 1.239681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [65280/500434 (13%)] Loss: 1.129724\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [65280/500434 (13%)] Loss: 1.129724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [66560/500434 (13%)] Loss: 1.319990\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [66560/500434 (13%)] Loss: 1.319990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [67840/500434 (14%)] Loss: 1.285015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [67840/500434 (14%)] Loss: 1.285015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [69120/500434 (14%)] Loss: 1.352709\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [69120/500434 (14%)] Loss: 1.352709\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [70400/500434 (14%)] Loss: 1.061568\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [70400/500434 (14%)] Loss: 1.061568\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [71680/500434 (14%)] Loss: 1.115400\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [71680/500434 (14%)] Loss: 1.115400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [72960/500434 (15%)] Loss: 1.388926\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [72960/500434 (15%)] Loss: 1.388926\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [74240/500434 (15%)] Loss: 1.241659\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [74240/500434 (15%)] Loss: 1.241659\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [75520/500434 (15%)] Loss: 1.111696\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [75520/500434 (15%)] Loss: 1.111696\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [76800/500434 (15%)] Loss: 1.621852\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [76800/500434 (15%)] Loss: 1.621852\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [78080/500434 (16%)] Loss: 1.171606\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [78080/500434 (16%)] Loss: 1.171606\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [79360/500434 (16%)] Loss: 1.402951\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [79360/500434 (16%)] Loss: 1.402951\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [80640/500434 (16%)] Loss: 1.224046\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [80640/500434 (16%)] Loss: 1.224046\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [81920/500434 (16%)] Loss: 1.380838\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [81920/500434 (16%)] Loss: 1.380838\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [83200/500434 (17%)] Loss: 1.007892\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [83200/500434 (17%)] Loss: 1.007892\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [84480/500434 (17%)] Loss: 1.300006\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [84480/500434 (17%)] Loss: 1.300006\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [85760/500434 (17%)] Loss: 1.205343\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [85760/500434 (17%)] Loss: 1.205343\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [87040/500434 (17%)] Loss: 1.143132\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [87040/500434 (17%)] Loss: 1.143132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [88320/500434 (18%)] Loss: 1.382117\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [88320/500434 (18%)] Loss: 1.382117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [89600/500434 (18%)] Loss: 1.013498\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [89600/500434 (18%)] Loss: 1.013498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [90880/500434 (18%)] Loss: 1.291927\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [90880/500434 (18%)] Loss: 1.291927\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [92160/500434 (18%)] Loss: 0.862079\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [92160/500434 (18%)] Loss: 0.862079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [93440/500434 (19%)] Loss: 1.168733\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [93440/500434 (19%)] Loss: 1.168733\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [94720/500434 (19%)] Loss: 1.307820\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [94720/500434 (19%)] Loss: 1.307820\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [96000/500434 (19%)] Loss: 1.013923\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [96000/500434 (19%)] Loss: 1.013923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [97280/500434 (19%)] Loss: 1.554518\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [97280/500434 (19%)] Loss: 1.554518\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [98560/500434 (20%)] Loss: 1.210639\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [98560/500434 (20%)] Loss: 1.210639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [99840/500434 (20%)] Loss: 1.315048\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [99840/500434 (20%)] Loss: 1.315048\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [101120/500434 (20%)] Loss: 1.242731\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [101120/500434 (20%)] Loss: 1.242731\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [102400/500434 (20%)] Loss: 1.152132\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [102400/500434 (20%)] Loss: 1.152132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [103680/500434 (21%)] Loss: 1.038523\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [103680/500434 (21%)] Loss: 1.038523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [104960/500434 (21%)] Loss: 1.235661\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [104960/500434 (21%)] Loss: 1.235661\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [106240/500434 (21%)] Loss: 1.221254\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [106240/500434 (21%)] Loss: 1.221254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [107520/500434 (21%)] Loss: 1.369154\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [107520/500434 (21%)] Loss: 1.369154\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [108800/500434 (22%)] Loss: 1.028296\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [108800/500434 (22%)] Loss: 1.028296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [110080/500434 (22%)] Loss: 1.333235\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [110080/500434 (22%)] Loss: 1.333235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [111360/500434 (22%)] Loss: 1.044510\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [111360/500434 (22%)] Loss: 1.044510\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [112640/500434 (23%)] Loss: 1.326504\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [112640/500434 (23%)] Loss: 1.326504\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [113920/500434 (23%)] Loss: 1.238707\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [113920/500434 (23%)] Loss: 1.238707\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [115200/500434 (23%)] Loss: 0.987577\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [115200/500434 (23%)] Loss: 0.987577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [116480/500434 (23%)] Loss: 1.232800\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [116480/500434 (23%)] Loss: 1.232800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [117760/500434 (24%)] Loss: 1.324270\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [117760/500434 (24%)] Loss: 1.324270\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [119040/500434 (24%)] Loss: 1.064162\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [119040/500434 (24%)] Loss: 1.064162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [120320/500434 (24%)] Loss: 1.160769\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [120320/500434 (24%)] Loss: 1.160769\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [121600/500434 (24%)] Loss: 1.194069\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [121600/500434 (24%)] Loss: 1.194069\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [122880/500434 (25%)] Loss: 1.073522\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [122880/500434 (25%)] Loss: 1.073522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [124160/500434 (25%)] Loss: 1.304811\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [124160/500434 (25%)] Loss: 1.304811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [125440/500434 (25%)] Loss: 1.081227\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [125440/500434 (25%)] Loss: 1.081227\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [126720/500434 (25%)] Loss: 0.889503\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [126720/500434 (25%)] Loss: 0.889503\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [128000/500434 (26%)] Loss: 1.017131\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [128000/500434 (26%)] Loss: 1.017131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [129280/500434 (26%)] Loss: 1.304042\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [129280/500434 (26%)] Loss: 1.304042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [130560/500434 (26%)] Loss: 1.271232\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [130560/500434 (26%)] Loss: 1.271232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [131840/500434 (26%)] Loss: 1.259907\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [131840/500434 (26%)] Loss: 1.259907\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [133120/500434 (27%)] Loss: 1.191678\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [133120/500434 (27%)] Loss: 1.191678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [134400/500434 (27%)] Loss: 1.298861\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [134400/500434 (27%)] Loss: 1.298861\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [135680/500434 (27%)] Loss: 1.069607\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [135680/500434 (27%)] Loss: 1.069607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [136960/500434 (27%)] Loss: 0.905395\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [136960/500434 (27%)] Loss: 0.905395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [138240/500434 (28%)] Loss: 0.933257\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [138240/500434 (28%)] Loss: 0.933257\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [139520/500434 (28%)] Loss: 1.347199\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [139520/500434 (28%)] Loss: 1.347199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [140800/500434 (28%)] Loss: 1.299498\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [140800/500434 (28%)] Loss: 1.299498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [142080/500434 (28%)] Loss: 0.909164\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [142080/500434 (28%)] Loss: 0.909164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [143360/500434 (29%)] Loss: 1.378542\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [143360/500434 (29%)] Loss: 1.378542\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [144640/500434 (29%)] Loss: 1.191800\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [144640/500434 (29%)] Loss: 1.191800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [145920/500434 (29%)] Loss: 1.105188\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [145920/500434 (29%)] Loss: 1.105188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [147200/500434 (29%)] Loss: 1.253840\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [147200/500434 (29%)] Loss: 1.253840\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [148480/500434 (30%)] Loss: 1.213097\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [148480/500434 (30%)] Loss: 1.213097\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [149760/500434 (30%)] Loss: 1.165190\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [149760/500434 (30%)] Loss: 1.165190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [151040/500434 (30%)] Loss: 1.266531\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [151040/500434 (30%)] Loss: 1.266531\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [152320/500434 (30%)] Loss: 1.047529\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [152320/500434 (30%)] Loss: 1.047529\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [153600/500434 (31%)] Loss: 1.321497\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [153600/500434 (31%)] Loss: 1.321497\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [154880/500434 (31%)] Loss: 0.977903\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [154880/500434 (31%)] Loss: 0.977903\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [156160/500434 (31%)] Loss: 1.131212\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [156160/500434 (31%)] Loss: 1.131212\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [157440/500434 (31%)] Loss: 1.259976\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [157440/500434 (31%)] Loss: 1.259976\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [158720/500434 (32%)] Loss: 1.143692\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [158720/500434 (32%)] Loss: 1.143692\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [160000/500434 (32%)] Loss: 1.086837\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [160000/500434 (32%)] Loss: 1.086837\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [161280/500434 (32%)] Loss: 0.924967\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [161280/500434 (32%)] Loss: 0.924967\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [162560/500434 (32%)] Loss: 1.143668\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [162560/500434 (32%)] Loss: 1.143668\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [163840/500434 (33%)] Loss: 1.082065\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [163840/500434 (33%)] Loss: 1.082065\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [165120/500434 (33%)] Loss: 1.233779\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [165120/500434 (33%)] Loss: 1.233779\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [166400/500434 (33%)] Loss: 1.075358\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [166400/500434 (33%)] Loss: 1.075358\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [167680/500434 (34%)] Loss: 1.325101\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [167680/500434 (34%)] Loss: 1.325101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [168960/500434 (34%)] Loss: 1.000577\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [168960/500434 (34%)] Loss: 1.000577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [170240/500434 (34%)] Loss: 1.106254\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [170240/500434 (34%)] Loss: 1.106254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [171520/500434 (34%)] Loss: 1.070948\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [171520/500434 (34%)] Loss: 1.070948\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [172800/500434 (35%)] Loss: 1.138059\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [172800/500434 (35%)] Loss: 1.138059\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [174080/500434 (35%)] Loss: 0.991292\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [174080/500434 (35%)] Loss: 0.991292\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [175360/500434 (35%)] Loss: 1.169893\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [175360/500434 (35%)] Loss: 1.169893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [176640/500434 (35%)] Loss: 1.217351\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [176640/500434 (35%)] Loss: 1.217351\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [177920/500434 (36%)] Loss: 1.286415\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [177920/500434 (36%)] Loss: 1.286415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [179200/500434 (36%)] Loss: 1.379329\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [179200/500434 (36%)] Loss: 1.379329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [180480/500434 (36%)] Loss: 1.244200\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [180480/500434 (36%)] Loss: 1.244200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [181760/500434 (36%)] Loss: 1.233832\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [181760/500434 (36%)] Loss: 1.233832\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [183040/500434 (37%)] Loss: 1.183183\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [183040/500434 (37%)] Loss: 1.183183\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [184320/500434 (37%)] Loss: 1.270866\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [184320/500434 (37%)] Loss: 1.270866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [185600/500434 (37%)] Loss: 1.017945\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [185600/500434 (37%)] Loss: 1.017945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [186880/500434 (37%)] Loss: 1.041752\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [186880/500434 (37%)] Loss: 1.041752\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [188160/500434 (38%)] Loss: 1.272858\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [188160/500434 (38%)] Loss: 1.272858\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [189440/500434 (38%)] Loss: 0.955595\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [189440/500434 (38%)] Loss: 0.955595\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [190720/500434 (38%)] Loss: 0.939303\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [190720/500434 (38%)] Loss: 0.939303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [192000/500434 (38%)] Loss: 0.970399\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [192000/500434 (38%)] Loss: 0.970399\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [193280/500434 (39%)] Loss: 0.933500\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [193280/500434 (39%)] Loss: 0.933500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [194560/500434 (39%)] Loss: 1.316019\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [194560/500434 (39%)] Loss: 1.316019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [195840/500434 (39%)] Loss: 0.951303\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [195840/500434 (39%)] Loss: 0.951303\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [197120/500434 (39%)] Loss: 1.344168\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [197120/500434 (39%)] Loss: 1.344168\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [198400/500434 (40%)] Loss: 1.292559\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [198400/500434 (40%)] Loss: 1.292559\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [199680/500434 (40%)] Loss: 0.913592\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [199680/500434 (40%)] Loss: 0.913592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [200960/500434 (40%)] Loss: 1.148099\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [200960/500434 (40%)] Loss: 1.148099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [202240/500434 (40%)] Loss: 1.000188\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [202240/500434 (40%)] Loss: 1.000188\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [203520/500434 (41%)] Loss: 0.937966\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [203520/500434 (41%)] Loss: 0.937966\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [204800/500434 (41%)] Loss: 1.248997\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [204800/500434 (41%)] Loss: 1.248997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [206080/500434 (41%)] Loss: 1.063130\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [206080/500434 (41%)] Loss: 1.063130\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [207360/500434 (41%)] Loss: 1.073751\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [207360/500434 (41%)] Loss: 1.073751\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [208640/500434 (42%)] Loss: 1.007802\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [208640/500434 (42%)] Loss: 1.007802\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [209920/500434 (42%)] Loss: 1.147519\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [209920/500434 (42%)] Loss: 1.147519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [211200/500434 (42%)] Loss: 1.085361\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [211200/500434 (42%)] Loss: 1.085361\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [212480/500434 (42%)] Loss: 1.204722\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [212480/500434 (42%)] Loss: 1.204722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [213760/500434 (43%)] Loss: 1.048879\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [213760/500434 (43%)] Loss: 1.048879\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [215040/500434 (43%)] Loss: 1.012767\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [215040/500434 (43%)] Loss: 1.012767\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [216320/500434 (43%)] Loss: 1.080427\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [216320/500434 (43%)] Loss: 1.080427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [217600/500434 (43%)] Loss: 1.004357\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [217600/500434 (43%)] Loss: 1.004357\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [218880/500434 (44%)] Loss: 1.059610\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [218880/500434 (44%)] Loss: 1.059610\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [220160/500434 (44%)] Loss: 0.925396\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [220160/500434 (44%)] Loss: 0.925396\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [221440/500434 (44%)] Loss: 1.179000\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [221440/500434 (44%)] Loss: 1.179000\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [222720/500434 (45%)] Loss: 1.052865\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [222720/500434 (45%)] Loss: 1.052865\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [224000/500434 (45%)] Loss: 0.827132\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [224000/500434 (45%)] Loss: 0.827132\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [225280/500434 (45%)] Loss: 1.057863\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [225280/500434 (45%)] Loss: 1.057863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [226560/500434 (45%)] Loss: 1.165336\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [226560/500434 (45%)] Loss: 1.165336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [227840/500434 (46%)] Loss: 0.929266\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [227840/500434 (46%)] Loss: 0.929266\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [229120/500434 (46%)] Loss: 1.196938\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [229120/500434 (46%)] Loss: 1.196938\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [230400/500434 (46%)] Loss: 0.923475\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [230400/500434 (46%)] Loss: 0.923475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [231680/500434 (46%)] Loss: 1.074336\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [231680/500434 (46%)] Loss: 1.074336\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [232960/500434 (47%)] Loss: 0.960587\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [232960/500434 (47%)] Loss: 0.960587\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [234240/500434 (47%)] Loss: 1.219687\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [234240/500434 (47%)] Loss: 1.219687\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [235520/500434 (47%)] Loss: 1.262360\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [235520/500434 (47%)] Loss: 1.262360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [236800/500434 (47%)] Loss: 1.447377\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [236800/500434 (47%)] Loss: 1.447377\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [238080/500434 (48%)] Loss: 1.139107\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [238080/500434 (48%)] Loss: 1.139107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [239360/500434 (48%)] Loss: 1.398756\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [239360/500434 (48%)] Loss: 1.398756\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [240640/500434 (48%)] Loss: 0.936970\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [240640/500434 (48%)] Loss: 0.936970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [241920/500434 (48%)] Loss: 1.450994\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [241920/500434 (48%)] Loss: 1.450994\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [243200/500434 (49%)] Loss: 1.392278\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [243200/500434 (49%)] Loss: 1.392278\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [244480/500434 (49%)] Loss: 0.809911\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [244480/500434 (49%)] Loss: 0.809911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [245760/500434 (49%)] Loss: 0.988847\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [245760/500434 (49%)] Loss: 0.988847\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [247040/500434 (49%)] Loss: 1.196277\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [247040/500434 (49%)] Loss: 1.196277\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [248320/500434 (50%)] Loss: 1.096562\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [248320/500434 (50%)] Loss: 1.096562\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [249600/500434 (50%)] Loss: 0.839609\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [249600/500434 (50%)] Loss: 0.839609\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [250880/500434 (50%)] Loss: 0.970947\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [250880/500434 (50%)] Loss: 0.970947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [252160/500434 (50%)] Loss: 0.823572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [252160/500434 (50%)] Loss: 0.823572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [253440/500434 (51%)] Loss: 1.008697\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [253440/500434 (51%)] Loss: 1.008697\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [254720/500434 (51%)] Loss: 1.166810\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [254720/500434 (51%)] Loss: 1.166810\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [256000/500434 (51%)] Loss: 1.033205\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [256000/500434 (51%)] Loss: 1.033205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [257280/500434 (51%)] Loss: 0.905438\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [257280/500434 (51%)] Loss: 0.905438\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [258560/500434 (52%)] Loss: 1.037459\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [258560/500434 (52%)] Loss: 1.037459\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [259840/500434 (52%)] Loss: 1.063990\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [259840/500434 (52%)] Loss: 1.063990\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [261120/500434 (52%)] Loss: 1.062430\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [261120/500434 (52%)] Loss: 1.062430\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [262400/500434 (52%)] Loss: 1.156421\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [262400/500434 (52%)] Loss: 1.156421\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [263680/500434 (53%)] Loss: 0.915229\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [263680/500434 (53%)] Loss: 0.915229\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [264960/500434 (53%)] Loss: 1.017911\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [264960/500434 (53%)] Loss: 1.017911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [266240/500434 (53%)] Loss: 1.268530\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [266240/500434 (53%)] Loss: 1.268530\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [267520/500434 (53%)] Loss: 0.879627\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [267520/500434 (53%)] Loss: 0.879627\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [268800/500434 (54%)] Loss: 0.924235\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [268800/500434 (54%)] Loss: 0.924235\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [270080/500434 (54%)] Loss: 0.929630\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [270080/500434 (54%)] Loss: 0.929630\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [271360/500434 (54%)] Loss: 0.982254\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [271360/500434 (54%)] Loss: 0.982254\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [272640/500434 (54%)] Loss: 1.378722\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [272640/500434 (54%)] Loss: 1.378722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [273920/500434 (55%)] Loss: 1.216914\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [273920/500434 (55%)] Loss: 1.216914\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [275200/500434 (55%)] Loss: 1.040030\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [275200/500434 (55%)] Loss: 1.040030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [276480/500434 (55%)] Loss: 1.121036\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [276480/500434 (55%)] Loss: 1.121036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [277760/500434 (55%)] Loss: 1.151866\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [277760/500434 (55%)] Loss: 1.151866\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [279040/500434 (56%)] Loss: 0.997736\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [279040/500434 (56%)] Loss: 0.997736\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [280320/500434 (56%)] Loss: 0.987572\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [280320/500434 (56%)] Loss: 0.987572\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [281600/500434 (56%)] Loss: 1.242228\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [281600/500434 (56%)] Loss: 1.242228\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [282880/500434 (57%)] Loss: 1.043233\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [282880/500434 (57%)] Loss: 1.043233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [284160/500434 (57%)] Loss: 1.246513\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [284160/500434 (57%)] Loss: 1.246513\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [285440/500434 (57%)] Loss: 1.038537\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [285440/500434 (57%)] Loss: 1.038537\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [286720/500434 (57%)] Loss: 1.049806\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [286720/500434 (57%)] Loss: 1.049806\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [288000/500434 (58%)] Loss: 0.955467\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [288000/500434 (58%)] Loss: 0.955467\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [289280/500434 (58%)] Loss: 0.892005\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [289280/500434 (58%)] Loss: 0.892005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [290560/500434 (58%)] Loss: 1.282423\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [290560/500434 (58%)] Loss: 1.282423\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [291840/500434 (58%)] Loss: 1.165445\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [291840/500434 (58%)] Loss: 1.165445\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [293120/500434 (59%)] Loss: 1.029008\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [293120/500434 (59%)] Loss: 1.029008\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [294400/500434 (59%)] Loss: 0.918364\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [294400/500434 (59%)] Loss: 0.918364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [295680/500434 (59%)] Loss: 0.987321\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [295680/500434 (59%)] Loss: 0.987321\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [296960/500434 (59%)] Loss: 1.047220\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [296960/500434 (59%)] Loss: 1.047220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [298240/500434 (60%)] Loss: 1.019003\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [298240/500434 (60%)] Loss: 1.019003\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [299520/500434 (60%)] Loss: 0.960353\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [299520/500434 (60%)] Loss: 0.960353\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [300800/500434 (60%)] Loss: 0.832084\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [300800/500434 (60%)] Loss: 0.832084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [302080/500434 (60%)] Loss: 1.110093\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [302080/500434 (60%)] Loss: 1.110093\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [303360/500434 (61%)] Loss: 1.026340\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [303360/500434 (61%)] Loss: 1.026340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [304640/500434 (61%)] Loss: 1.084424\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [304640/500434 (61%)] Loss: 1.084424\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [305920/500434 (61%)] Loss: 0.718159\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [305920/500434 (61%)] Loss: 0.718159\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [307200/500434 (61%)] Loss: 1.144523\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [307200/500434 (61%)] Loss: 1.144523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [308480/500434 (62%)] Loss: 0.889561\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [308480/500434 (62%)] Loss: 0.889561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [309760/500434 (62%)] Loss: 1.322658\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [309760/500434 (62%)] Loss: 1.322658\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [311040/500434 (62%)] Loss: 1.077910\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [311040/500434 (62%)] Loss: 1.077910\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [312320/500434 (62%)] Loss: 1.172090\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [312320/500434 (62%)] Loss: 1.172090\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [313600/500434 (63%)] Loss: 1.171242\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [313600/500434 (63%)] Loss: 1.171242\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [314880/500434 (63%)] Loss: 0.922312\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [314880/500434 (63%)] Loss: 0.922312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [316160/500434 (63%)] Loss: 0.873019\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [316160/500434 (63%)] Loss: 0.873019\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [317440/500434 (63%)] Loss: 0.893533\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [317440/500434 (63%)] Loss: 0.893533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [318720/500434 (64%)] Loss: 0.905167\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [318720/500434 (64%)] Loss: 0.905167\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [320000/500434 (64%)] Loss: 1.051524\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [320000/500434 (64%)] Loss: 1.051524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [321280/500434 (64%)] Loss: 1.165045\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [321280/500434 (64%)] Loss: 1.165045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [322560/500434 (64%)] Loss: 0.826581\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [322560/500434 (64%)] Loss: 0.826581\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [323840/500434 (65%)] Loss: 0.975475\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [323840/500434 (65%)] Loss: 0.975475\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [325120/500434 (65%)] Loss: 1.041633\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [325120/500434 (65%)] Loss: 1.041633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [326400/500434 (65%)] Loss: 0.997350\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [326400/500434 (65%)] Loss: 0.997350\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [327680/500434 (65%)] Loss: 1.027067\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [327680/500434 (65%)] Loss: 1.027067\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [328960/500434 (66%)] Loss: 0.813040\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [328960/500434 (66%)] Loss: 0.813040\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [330240/500434 (66%)] Loss: 1.193494\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [330240/500434 (66%)] Loss: 1.193494\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [331520/500434 (66%)] Loss: 0.960116\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [331520/500434 (66%)] Loss: 0.960116\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [332800/500434 (66%)] Loss: 1.045815\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [332800/500434 (66%)] Loss: 1.045815\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [334080/500434 (67%)] Loss: 1.028305\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [334080/500434 (67%)] Loss: 1.028305\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [335360/500434 (67%)] Loss: 1.118312\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [335360/500434 (67%)] Loss: 1.118312\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [336640/500434 (67%)] Loss: 1.034803\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [336640/500434 (67%)] Loss: 1.034803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [337920/500434 (68%)] Loss: 0.811454\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [337920/500434 (68%)] Loss: 0.811454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [339200/500434 (68%)] Loss: 1.126138\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [339200/500434 (68%)] Loss: 1.126138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [340480/500434 (68%)] Loss: 0.878818\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [340480/500434 (68%)] Loss: 0.878818\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [341760/500434 (68%)] Loss: 1.210578\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [341760/500434 (68%)] Loss: 1.210578\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [343040/500434 (69%)] Loss: 0.908772\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [343040/500434 (69%)] Loss: 0.908772\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [344320/500434 (69%)] Loss: 1.013400\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [344320/500434 (69%)] Loss: 1.013400\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [345600/500434 (69%)] Loss: 0.953027\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [345600/500434 (69%)] Loss: 0.953027\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [346880/500434 (69%)] Loss: 1.165252\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [346880/500434 (69%)] Loss: 1.165252\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [348160/500434 (70%)] Loss: 1.095451\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [348160/500434 (70%)] Loss: 1.095451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [349440/500434 (70%)] Loss: 0.778285\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [349440/500434 (70%)] Loss: 0.778285\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [350720/500434 (70%)] Loss: 0.909425\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [350720/500434 (70%)] Loss: 0.909425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [352000/500434 (70%)] Loss: 0.977489\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [352000/500434 (70%)] Loss: 0.977489\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [353280/500434 (71%)] Loss: 0.895850\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [353280/500434 (71%)] Loss: 0.895850\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [354560/500434 (71%)] Loss: 0.920612\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [354560/500434 (71%)] Loss: 0.920612\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [355840/500434 (71%)] Loss: 0.797985\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [355840/500434 (71%)] Loss: 0.797985\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [357120/500434 (71%)] Loss: 1.220100\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [357120/500434 (71%)] Loss: 1.220100\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [358400/500434 (72%)] Loss: 0.732812\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [358400/500434 (72%)] Loss: 0.732812\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [359680/500434 (72%)] Loss: 1.124622\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [359680/500434 (72%)] Loss: 1.124622\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [360960/500434 (72%)] Loss: 1.231205\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [360960/500434 (72%)] Loss: 1.231205\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [362240/500434 (72%)] Loss: 1.187304\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [362240/500434 (72%)] Loss: 1.187304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [363520/500434 (73%)] Loss: 0.810935\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [363520/500434 (73%)] Loss: 0.810935\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [364800/500434 (73%)] Loss: 0.929761\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [364800/500434 (73%)] Loss: 0.929761\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [366080/500434 (73%)] Loss: 0.865476\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [366080/500434 (73%)] Loss: 0.865476\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [367360/500434 (73%)] Loss: 1.173759\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [367360/500434 (73%)] Loss: 1.173759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [368640/500434 (74%)] Loss: 1.044718\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [368640/500434 (74%)] Loss: 1.044718\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [369920/500434 (74%)] Loss: 1.148566\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [369920/500434 (74%)] Loss: 1.148566\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [371200/500434 (74%)] Loss: 1.010345\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [371200/500434 (74%)] Loss: 1.010345\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [372480/500434 (74%)] Loss: 1.162394\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [372480/500434 (74%)] Loss: 1.162394\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [373760/500434 (75%)] Loss: 0.991777\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [373760/500434 (75%)] Loss: 0.991777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [375040/500434 (75%)] Loss: 0.919795\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [375040/500434 (75%)] Loss: 0.919795\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [376320/500434 (75%)] Loss: 0.983479\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [376320/500434 (75%)] Loss: 0.983479\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [377600/500434 (75%)] Loss: 0.650633\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [377600/500434 (75%)] Loss: 0.650633\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [378880/500434 (76%)] Loss: 0.888672\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [378880/500434 (76%)] Loss: 0.888672\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [380160/500434 (76%)] Loss: 1.038179\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [380160/500434 (76%)] Loss: 1.038179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [381440/500434 (76%)] Loss: 0.952017\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [381440/500434 (76%)] Loss: 0.952017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [382720/500434 (76%)] Loss: 0.788370\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [382720/500434 (76%)] Loss: 0.788370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [384000/500434 (77%)] Loss: 1.074012\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [384000/500434 (77%)] Loss: 1.074012\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [385280/500434 (77%)] Loss: 1.007308\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [385280/500434 (77%)] Loss: 1.007308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [386560/500434 (77%)] Loss: 0.946315\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [386560/500434 (77%)] Loss: 0.946315\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [387840/500434 (77%)] Loss: 0.952556\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [387840/500434 (77%)] Loss: 0.952556\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [389120/500434 (78%)] Loss: 0.836440\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [389120/500434 (78%)] Loss: 0.836440\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [390400/500434 (78%)] Loss: 0.859448\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [390400/500434 (78%)] Loss: 0.859448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [391680/500434 (78%)] Loss: 0.980775\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [391680/500434 (78%)] Loss: 0.980775\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [392960/500434 (79%)] Loss: 0.931814\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [392960/500434 (79%)] Loss: 0.931814\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [394240/500434 (79%)] Loss: 1.056807\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [394240/500434 (79%)] Loss: 1.056807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [395520/500434 (79%)] Loss: 0.942031\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [395520/500434 (79%)] Loss: 0.942031\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [396800/500434 (79%)] Loss: 0.891558\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [396800/500434 (79%)] Loss: 0.891558\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [398080/500434 (80%)] Loss: 0.757306\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [398080/500434 (80%)] Loss: 0.757306\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [399360/500434 (80%)] Loss: 0.966332\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [399360/500434 (80%)] Loss: 0.966332\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [400640/500434 (80%)] Loss: 0.868013\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [400640/500434 (80%)] Loss: 0.868013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [401920/500434 (80%)] Loss: 1.025888\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [401920/500434 (80%)] Loss: 1.025888\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [403200/500434 (81%)] Loss: 1.094451\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [403200/500434 (81%)] Loss: 1.094451\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [404480/500434 (81%)] Loss: 0.980209\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [404480/500434 (81%)] Loss: 0.980209\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [405760/500434 (81%)] Loss: 0.934498\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [405760/500434 (81%)] Loss: 0.934498\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [407040/500434 (81%)] Loss: 1.002269\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [407040/500434 (81%)] Loss: 1.002269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [408320/500434 (82%)] Loss: 1.011162\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [408320/500434 (82%)] Loss: 1.011162\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [409600/500434 (82%)] Loss: 0.892565\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [409600/500434 (82%)] Loss: 0.892565\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [410880/500434 (82%)] Loss: 0.911395\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [410880/500434 (82%)] Loss: 0.911395\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [412160/500434 (82%)] Loss: 0.959465\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [412160/500434 (82%)] Loss: 0.959465\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [413440/500434 (83%)] Loss: 0.748532\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [413440/500434 (83%)] Loss: 0.748532\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [414720/500434 (83%)] Loss: 0.886013\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [414720/500434 (83%)] Loss: 0.886013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [416000/500434 (83%)] Loss: 1.087477\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [416000/500434 (83%)] Loss: 1.087477\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [417280/500434 (83%)] Loss: 1.081276\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [417280/500434 (83%)] Loss: 1.081276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [418560/500434 (84%)] Loss: 0.937136\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [418560/500434 (84%)] Loss: 0.937136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [419840/500434 (84%)] Loss: 1.128881\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [419840/500434 (84%)] Loss: 1.128881\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [421120/500434 (84%)] Loss: 0.909908\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [421120/500434 (84%)] Loss: 0.909908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [422400/500434 (84%)] Loss: 0.925340\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [422400/500434 (84%)] Loss: 0.925340\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [423680/500434 (85%)] Loss: 1.227190\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [423680/500434 (85%)] Loss: 1.227190\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [424960/500434 (85%)] Loss: 1.023163\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [424960/500434 (85%)] Loss: 1.023163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [426240/500434 (85%)] Loss: 0.915547\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [426240/500434 (85%)] Loss: 0.915547\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator_profile.fit({'training': data_input}, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37e164",
   "metadata": {},
   "source": [
    "# Profiling Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32400e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-09-12 14:34:59 Starting - Preparing the instances for training\n",
      "2022-09-12 14:34:59 Downloading - Downloading input data\n",
      "2022-09-12 14:34:59 Training - Training image download completed. Training in progress.\n",
      "2022-09-12 14:34:59 Uploading - Uploading generated training model\n",
      "2022-09-12 14:34:59 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator_profile = huggingface_estimator_profile.attach('huggingface-pytorch-training-2022-09-12-10-18-38-954')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f650b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-18 03:00:14.642 ip-172-16-83-215.ec2.internal:17800 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-18 03:00:15.271 ip-172-16-83-215.ec2.internal:17800 INFO s3_trial.py:42] Loading trial  at path s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-12-10-18-38-954/debug-output/\n",
      "[2022-09-18 03:00:16.126 ip-172-16-83-215.ec2.internal:17800 WARNING s3handler.py:183] Encountered the exception An error occurred while reading from response stream: ('Connection broken: IncompleteRead(0 bytes read, 24207 more expected)', IncompleteRead(0 bytes read, 24207 more expected)) while reading s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-12-10-18-38-954/debug-output/index/000000004/000000004305_worker_0.json . Will retry now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "debug_output_s3_path = 's3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-12-10-18-38-954/debug-output/'\n",
    "\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "trial = create_trial(debug_output_s3_path)\n",
    "\n",
    "def get_data(trial, tname, mode):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps(mode=mode)\n",
    "    vals = []\n",
    "    for s in steps:\n",
    "        vals.append(tensor.value(s, mode=mode))\n",
    "    return steps, vals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "\n",
    "def plot_tensor(trial, tensor_name):\n",
    "\n",
    "    steps_train, vals_train = get_data(trial, tensor_name, mode=ModeKeys.TRAIN)\n",
    "    print(\"loaded TRAIN data\")\n",
    "    steps_eval, vals_eval = get_data(trial, tensor_name, mode=ModeKeys.EVAL)\n",
    "    print(\"loaded EVAL data\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    host = host_subplot(111)\n",
    "\n",
    "    par = host.twiny()\n",
    "\n",
    "    host.set_xlabel(\"Steps (TRAIN)\")\n",
    "    par.set_xlabel(\"Steps (EVAL)\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)\n",
    "    print(\"completed TRAIN plot\")\n",
    "    (p2,) = par.plot(steps_eval, vals_eval, label=\"val_\" + tensor_name)\n",
    "    print(\"completed EVAL plot\")\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "    plt.ylabel(tensor_name)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dd2371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded TRAIN data\n",
      "loaded EVAL data\n",
      "completed TRAIN plot\n",
      "completed EVAL plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_tensor(trial, 'CrossEntropyLoss_output_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.system_metrics_reader import S3SystemMetricsReader\n",
    "#from smdebug.profiler.analysis.notebook_utils.metrics_histogram import MetricsHistogram\n",
    "\n",
    "system_metrics_reader = S3SystemMetricsReader(path)\n",
    "\n",
    "\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "#metrics_histogram = MetricsHistogram(system_metrics_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4bd29",
   "metadata": {},
   "source": [
    "# Deploy to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b74a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f67d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958d240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e324a98",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[huggingface tutorial notebook](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
    "\n",
    "\n",
    "[huggingface sagemaker tutorial](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb)\n",
    "\n",
    "[data augmentation from netune ai](https://neptune.ai/blog/data-augmentation-nlp)\n",
    "\n",
    "[textual augmentation example code](https://github.com/makcedward/nlpaug/blob/23800cbb9632c7fc8c4a88d46f9c4ecf68a96299/example/textual_augmenter.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
