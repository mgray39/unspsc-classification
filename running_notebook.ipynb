{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35e3d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.10.0)\n",
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 KB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch) (4.0.0)\n",
      "Collecting gdown>=4.0.0\n",
      "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gdown>=4.0.0->nlpaug) (4.10.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14933 sha256=d98c2e8e57c9776880738b91498e3555c558bbe7791110d131444e9680c9f8a2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/8d/df/71/846b2aa0fabaac2af23fbc5214eeaa55f0616e9d1a05187d72\n",
      "Successfully built gdown\n",
      "Installing collected packages: tokenizers, xxhash, responses, huggingface-hub, transformers, gdown, nlpaug, datasets\n",
      "Successfully installed datasets-2.4.0 gdown-4.5.1 huggingface-hub-0.9.1 nlpaug-1.1.11 responses-0.18.0 tokenizers-0.12.1 transformers-4.21.3 xxhash-3.0.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch nlpaug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb57b02",
   "metadata": {},
   "source": [
    "# Download Data & Prepare\n",
    "\n",
    "The script `wrangling_segment.py` by default will use files downloaded to prepare the datasets for predicting UNSPSC market Segment. If the --download flag is passed, the program first hits the web sources for this data and downloads them prior to creating prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14a9bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://data.ok.gov/dataset/18a622a6-32d1-48f6-842a-8232bc4ca06c/resource/b92ad3ac-b0f5-4c62-9bd0-eac023cfd083/download/data-unspsc-codes.csv to ./data/codes/data-unspsc-codes.csv\n",
      "    done\n",
      "downloading https://data.ca.gov/dataset/ae343670-f827-4bc8-9d44-2af937d60190/resource/bb82edc5-9c78-44e2-8947-68ece26197c5/download/purchase-order-data-2012-2015-.csv to ./data/california/purchase-order-data-2012-2015-.csv\n",
      "    done\n",
      "downloading https://data.gov.au/data/dataset/5c7fa69b-b0e9-4553-b8df-2a022dd2e982/resource/561a549b-5a65-450e-86cf-81d392d8fef3/download/20142015fy.csv to ./data/australia/20142015fy.csv\n",
      "    done\n",
      "downloading https://data.gov.au/data/dataset/5c7fa69b-b0e9-4553-b8df-2a022dd2e982/resource/21212500-169f-4745-86b3-6ac1c1174151/download/2016-2017-australian-government-contract-data.csv to ./data/australia/2016-2017-australian-government-contract-data.csv\n",
      "    done\n",
      "downloading https://data.gov.au/data/dataset/5c7fa69b-b0e9-4553-b8df-2a022dd2e982/resource/bc2097b7-8116-4e9d-9953-98813635892a/download/17-18-fy-dataset.csv to ./data/australia/17-18-fy-dataset.csv\n",
      "    done\n",
      "57\n",
      "check if code numbers preserved...\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "!python wrangling_segment.py --download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db27694",
   "metadata": {},
   "source": [
    "# Conduct Data Augmentation\n",
    "\n",
    "Class imbalance is a significant problem in this task. The largest segment has several thousand langauge samples, while the smallest has less than 100. As a result, data augmentation using randomised synonym replacement has been used to try to augment the training set.\n",
    "\n",
    "The script `data_augmentation.py` is able to implement this and includes options for the augmentation routine. In particular, it is possible to perform augmentation to a certain level while undersampling the larger segments to ensure that the classification problem is perfectly balanced. Excess samples are redistributed to the test set for later use in validation. It is also possible to set the number to increase the under-represented classes to. \n",
    "\n",
    "The default behaviour lifts the number of samples in each of the under-represented classes to 1000 records, while leaving the over-represented classes unchanged.\n",
    "\n",
    "Defaults are sufficient for our current purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir prepared_data/rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531f15e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "!python data_augmentation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afd118",
   "metadata": {},
   "source": [
    "# Upload Prepared Files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983efe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload again.\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'unspsc-data-private'\n",
    "prefix = 'segment_training'\n",
    "\n",
    "data_input = sagemaker_session.upload_data(path = './prepared_data/rebalanced/', bucket= bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a804454",
   "metadata": {},
   "source": [
    "# RNN Bprepared_data/ne Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f1b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756ef49",
   "metadata": {},
   "source": [
    "We need to pass with our entry point script a requirements.txt file for sagemaker to be able to install torchtext.\n",
    "\n",
    "Information on how to do this obtained from:\n",
    "\n",
    "https://github.com/awslabs/sagemaker-privacy-for-nlp/blob/master/source/sagemaker/2.Model_Training.ipynb\n",
    "\n",
    "I install pip-tools and use the pip-compile feature to extract all libraries to support what is in requirements.in which are all of the modules imported by train_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1fc4e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pip-tools\n",
      "  Downloading pip_tools-6.8.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting build\n",
      "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pip>=21.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (22.0.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (59.2.0)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (0.37.0)\n",
      "Requirement already satisfied: click>=7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pip-tools) (8.0.3)\n",
      "Requirement already satisfied: packaging>=19.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from build->pip-tools) (21.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from build->pip-tools) (1.2.2)\n",
      "Collecting pep517>=0.9.1\n",
      "  Downloading pep517-0.13.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=19.0->build->pip-tools) (3.0.6)\n",
      "Installing collected packages: pep517, build, pip-tools\n",
      "Successfully installed build-0.8.0 pep517-0.13.0 pip-tools-6.8.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pip-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79b540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#\u001b[0m\u001b[0m\n",
      "\u001b[32m# This file is autogenerated by pip-compile with python 3.8\u001b[0m\u001b[0m\n",
      "\u001b[32m# To update, run:\u001b[0m\u001b[0m\n",
      "\u001b[32m#\u001b[0m\u001b[0m\n",
      "\u001b[32m#    pip-compile ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "\u001b[32m#\u001b[0m\u001b[0m\n",
      "--extra-index-url https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[0m\n",
      "attrs==21.4.0\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "awscli==1.25.71\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "boto3==1.24.70\n",
      "    \u001b[32m# via\n",
      "    #   -r ./rnn_baseline/requirements.in\n",
      "    #   sagemaker\u001b[0m\u001b[0m\n",
      "botocore==1.27.70\n",
      "    \u001b[32m# via\n",
      "    #   awscli\n",
      "    #   boto3\n",
      "    #   s3transfer\u001b[0m\u001b[0m\n",
      "certifi==2022.6.15.1\n",
      "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
      "charset-normalizer==2.1.1\n",
      "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
      "colorama==0.4.4\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "dill==0.3.5.1\n",
      "    \u001b[32m# via\n",
      "    #   multiprocess\n",
      "    #   pathos\u001b[0m\u001b[0m\n",
      "docutils==0.16\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "google-pasta==0.2.0\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "idna==3.3\n",
      "    \u001b[32m# via requests\u001b[0m\u001b[0m\n",
      "importlib-metadata==4.12.0\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "jmespath==1.0.1\n",
      "    \u001b[32m# via\n",
      "    #   boto3\n",
      "    #   botocore\u001b[0m\u001b[0m\n",
      "joblib==1.1.0\n",
      "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
      "multiprocess==0.70.13\n",
      "    \u001b[32m# via pathos\u001b[0m\u001b[0m\n",
      "numpy==1.23.3\n",
      "    \u001b[32m# via\n",
      "    #   pandas\n",
      "    #   sagemaker\n",
      "    #   scikit-learn\n",
      "    #   scipy\n",
      "    #   torchtext\u001b[0m\u001b[0m\n",
      "packaging==21.3\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "pandas==1.4.4\n",
      "    \u001b[32m# via\n",
      "    #   -r ./rnn_baseline/requirements.in\n",
      "    #   sagemaker\u001b[0m\u001b[0m\n",
      "pathos==0.2.9\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "pox==0.3.1\n",
      "    \u001b[32m# via pathos\u001b[0m\u001b[0m\n",
      "ppft==1.7.6.5\n",
      "    \u001b[32m# via pathos\u001b[0m\u001b[0m\n",
      "protobuf==3.20.1\n",
      "    \u001b[32m# via\n",
      "    #   protobuf3-to-dict\n",
      "    #   sagemaker\u001b[0m\u001b[0m\n",
      "protobuf3-to-dict==0.1.5\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "pyasn1==0.4.8\n",
      "    \u001b[32m# via rsa\u001b[0m\u001b[0m\n",
      "pyparsing==3.0.9\n",
      "    \u001b[32m# via packaging\u001b[0m\u001b[0m\n",
      "python-dateutil==2.8.2\n",
      "    \u001b[32m# via\n",
      "    #   botocore\n",
      "    #   pandas\u001b[0m\u001b[0m\n",
      "pytz==2022.2.1\n",
      "    \u001b[32m# via pandas\u001b[0m\u001b[0m\n",
      "pyyaml==5.4.1\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "requests==2.28.1\n",
      "    \u001b[32m# via torchtext\u001b[0m\u001b[0m\n",
      "rsa==4.7.2\n",
      "    \u001b[32m# via awscli\u001b[0m\u001b[0m\n",
      "s3transfer==0.6.0\n",
      "    \u001b[32m# via\n",
      "    #   awscli\n",
      "    #   boto3\u001b[0m\u001b[0m\n",
      "sagemaker==2.109.0\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "scikit-learn==1.1.2\n",
      "    \u001b[32m# via sklearn\u001b[0m\u001b[0m\n",
      "scipy==1.9.1\n",
      "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
      "six==1.16.0\n",
      "    \u001b[32m# via\n",
      "    #   google-pasta\n",
      "    #   ppft\n",
      "    #   protobuf3-to-dict\n",
      "    #   python-dateutil\u001b[0m\u001b[0m\n",
      "sklearn==0.0\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "smdebug-rulesconfig==1.0.1\n",
      "    \u001b[32m# via sagemaker\u001b[0m\u001b[0m\n",
      "threadpoolctl==3.1.0\n",
      "    \u001b[32m# via scikit-learn\u001b[0m\u001b[0m\n",
      "torch==1.10.0\n",
      "    \u001b[32m# via\n",
      "    #   -r ./rnn_baseline/requirements.in\n",
      "    #   torchtext\u001b[0m\u001b[0m\n",
      "torchtext==0.11.0\n",
      "    \u001b[32m# via -r ./rnn_baseline/requirements.in\u001b[0m\u001b[0m\n",
      "tqdm==4.64.1\n",
      "    \u001b[32m# via torchtext\u001b[0m\u001b[0m\n",
      "typing-extensions==4.3.0\n",
      "    \u001b[32m# via torch\u001b[0m\u001b[0m\n",
      "urllib3==1.26.12\n",
      "    \u001b[32m# via\n",
      "    #   botocore\n",
      "    #   requests\u001b[0m\u001b[0m\n",
      "zipp==3.8.1\n",
      "    \u001b[32m# via importlib-metadata\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip-compile ./rnn_baseline/requirements.in > ./rnn_baseline/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10368575",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='train_rnn.py',\n",
    "                    source_dir = './rnn_baseline/',\n",
    "                    instance_count = 1,\n",
    "                    instance_type = 'ml.g4dn.xlarge',\n",
    "                    framework_version = '1.10',\n",
    "                    py_version='py38',\n",
    "                    role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896fe09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-10 13:37:21 Starting - Starting the training job...\n",
      "2022-09-10 13:37:45 Starting - Preparing the instances for trainingProfilerReport-1662817041: InProgress\n",
      "......\n",
      "2022-09-10 13:38:46 Downloading - Downloading input data...\n",
      "2022-09-10 13:39:05 Training - Downloading the training image........................\n",
      "2022-09-10 13:43:06 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-10 13:43:06,354 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-10 13:43:06,381 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-10 13:43:06,388 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-10 13:43:06,852 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs==21.4.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting awscli==1.25.71\u001b[0m\n",
      "\u001b[34mDownloading awscli-1.25.71-py3-none-any.whl (3.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 56.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting boto3==1.24.70\u001b[0m\n",
      "\u001b[34mDownloading boto3-1.24.70-py3-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.5/132.5 kB 25.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting botocore==1.27.70\u001b[0m\n",
      "\u001b[34mDownloading botocore-1.27.70-py3-none-any.whl (9.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 74.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting certifi==2022.6.15.1\u001b[0m\n",
      "\u001b[34mDownloading certifi-2022.6.15.1-py3-none-any.whl (160 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.4/160.4 kB 33.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer==2.1.1\u001b[0m\n",
      "\u001b[34mDownloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama==0.4.4 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill==0.3.5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils==0.16 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta==0.2.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna==3.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 36)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata==4.12.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath==1.0.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib==1.1.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 44)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess==0.70.13 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (0.70.13)\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.23.3\u001b[0m\n",
      "\u001b[34mDownloading numpy-1.23.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 67.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging==21.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.4.4\u001b[0m\n",
      "\u001b[34mDownloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 93.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos==0.2.9 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 61)) (0.2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox==0.3.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 63)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft==1.7.6.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 65)) (1.7.6.5)\u001b[0m\n",
      "\u001b[34mCollecting protobuf==3.20.1\u001b[0m\n",
      "\u001b[34mDownloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 57.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict==0.1.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 71)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1==0.4.8 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 73)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing==3.0.9 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 75)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil==2.8.2 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 77)) (2.8.2)\u001b[0m\n",
      "\u001b[34mCollecting pytz==2022.2.1\u001b[0m\n",
      "\u001b[34mDownloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 500.6/500.6 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml==5.4.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 83)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.28.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 85)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa==4.7.2 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 87)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer==0.6.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 89)) (0.6.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker==2.109.0\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.109.0.tar.gz (571 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 571.8/571.8 kB 30.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==1.1.2\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 49.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting scipy==1.9.1\u001b[0m\n",
      "\u001b[34mDownloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.4/43.4 MB 42.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 99)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sklearn==0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 105)) (0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 107)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl==3.1.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 109)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting torch==1.10.0\u001b[0m\n",
      "\u001b[34mDownloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 881.9/881.9 MB 1.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting torchtext==0.11.0\u001b[0m\n",
      "\u001b[34mDownloading torchtext-0.11.0-cp38-cp38-manylinux1_x86_64.whl (8.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/8.0 MB 90.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.64.1\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions==4.3.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 119)) (4.3.0)\u001b[0m\n",
      "\u001b[34mCollecting urllib3==1.26.12\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 28.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp==3.8.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 125)) (3.8.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.109.0-py2.py3-none-any.whl size=787690 sha256=8fdf3d0b477d6103f2b1c235f90c0467ca3a7838531207500376b7dcf325318d\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/57/93/29/bf7214bd2d7d0514e155957ee2d54d5421c1ff75cf6af2227e\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, urllib3, tqdm, torch, protobuf, numpy, charset-normalizer, certifi, scipy, pandas, botocore, torchtext, scikit-learn, boto3, awscli, sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pytz\u001b[0m\n",
      "\u001b[34mFound existing installation: pytz 2022.1\u001b[0m\n",
      "\u001b[34mUninstalling pytz-2022.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pytz-2022.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.10\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.10:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.10\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.63.0\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.63.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.63.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 1.10.2+cu113\u001b[0m\n",
      "\u001b[34mUninstalling torch-1.10.2+cu113:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-1.10.2+cu113\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.19.4\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.19.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.19.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.22.2\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.22.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.22.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: charset-normalizer\u001b[0m\n",
      "\u001b[34mFound existing installation: charset-normalizer 2.0.12\u001b[0m\n",
      "\u001b[34mUninstalling charset-normalizer-2.0.12:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled charset-normalizer-2.0.12\u001b[0m\n",
      "\u001b[34mAttempting uninstall: certifi\u001b[0m\n",
      "\u001b[34mFound existing installation: certifi 2022.6.15\u001b[0m\n",
      "\u001b[34mUninstalling certifi-2022.6.15:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled certifi-2022.6.15\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.8.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.8.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.8.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pandas\u001b[0m\n",
      "\u001b[34mFound existing installation: pandas 1.4.3\u001b[0m\n",
      "\u001b[34mUninstalling pandas-1.4.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pandas-1.4.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: botocore\u001b[0m\n",
      "\u001b[34mFound existing installation: botocore 1.27.36\u001b[0m\n",
      "\u001b[34mUninstalling botocore-1.27.36:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled botocore-1.27.36\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.1.1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.1.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.1.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: boto3\u001b[0m\n",
      "\u001b[34mFound existing installation: boto3 1.24.36\u001b[0m\n",
      "\u001b[34mUninstalling boto3-1.24.36:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled boto3-1.24.36\u001b[0m\n",
      "\u001b[34mAttempting uninstall: awscli\u001b[0m\n",
      "\u001b[34mFound existing installation: awscli 1.25.36\u001b[0m\n",
      "\u001b[34mUninstalling awscli-1.25.36:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled awscli-1.25.36\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.100.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.100.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.100.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-training 4.2.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed awscli-1.25.71 boto3-1.24.70 botocore-1.27.70 certifi-2022.6.15.1 charset-normalizer-2.1.1 numpy-1.23.3 pandas-1.4.4 protobuf-3.20.1 pytz-2022.2.1 sagemaker-2.109.0 scikit-learn-1.1.2 scipy-1.9.1 torch-1.10.0 torchtext-0.11.0 tqdm-4.64.1 urllib3-1.26.12\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-09-10 13:44:30,350 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-09-10 13:44:30,350 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-09-10 13:44:30,446 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-09-10-13-37-21-486\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-494389057463/pytorch-training-2022-09-10-13-37-21-486/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_rnn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_rnn.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_rnn.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_rnn\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-494389057463/pytorch-training-2022-09-10-13-37-21-486/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-09-10-13-37-21-486\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-494389057463/pytorch-training-2022-09-10-13-37-21-486/source/sourcedir.tar.gz\",\"module_name\":\"train_rnn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_rnn.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_rnn.py\u001b[0m\n",
      "\u001b[34mcurrent epoch: 1\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/500434 (0%)] Loss: 4.067792\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/500434 (2%)] Loss: 3.957036\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [20480/500434 (4%)] Loss: 3.678649\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [30720/500434 (6%)] Loss: 3.536561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [40960/500434 (8%)] Loss: 3.481664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/500434 (10%)] Loss: 3.424484\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [61440/500434 (12%)] Loss: 3.461569\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [71680/500434 (14%)] Loss: 3.493262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [81920/500434 (16%)] Loss: 3.479119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [92160/500434 (18%)] Loss: 3.447248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [102400/500434 (20%)] Loss: 3.471667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [112640/500434 (22%)] Loss: 3.544872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [122880/500434 (25%)] Loss: 3.462682\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [133120/500434 (27%)] Loss: 3.480045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [143360/500434 (29%)] Loss: 3.485705\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [153600/500434 (31%)] Loss: 3.463780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [163840/500434 (33%)] Loss: 3.514079\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [174080/500434 (35%)] Loss: 3.501207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [184320/500434 (37%)] Loss: 3.506749\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [194560/500434 (39%)] Loss: 3.429001\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [204800/500434 (41%)] Loss: 3.419739\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [215040/500434 (43%)] Loss: 3.490744\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [225280/500434 (45%)] Loss: 3.499302\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [235520/500434 (47%)] Loss: 3.468200\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [245760/500434 (49%)] Loss: 3.454916\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [256000/500434 (51%)] Loss: 3.632135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [266240/500434 (53%)] Loss: 3.454923\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [276480/500434 (55%)] Loss: 3.494104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [286720/500434 (57%)] Loss: 3.465329\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [296960/500434 (59%)] Loss: 3.480867\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [307200/500434 (61%)] Loss: 3.501584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [317440/500434 (63%)] Loss: 3.449454\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [327680/500434 (65%)] Loss: 3.491370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [337920/500434 (67%)] Loss: 3.537675\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [348160/500434 (70%)] Loss: 3.469667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [358400/500434 (72%)] Loss: 3.492042\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [368640/500434 (74%)] Loss: 3.422975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [378880/500434 (76%)] Loss: 3.456831\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [389120/500434 (78%)] Loss: 3.523146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [399360/500434 (80%)] Loss: 3.415873\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [409600/500434 (82%)] Loss: 3.472678\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [419840/500434 (84%)] Loss: 3.451179\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [430080/500434 (86%)] Loss: 3.459853\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [440320/500434 (88%)] Loss: 3.467983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [450560/500434 (90%)] Loss: 3.449975\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [460800/500434 (92%)] Loss: 3.488080\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [471040/500434 (94%)] Loss: 3.457247\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [481280/500434 (96%)] Loss: 3.433664\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [491520/500434 (98%)] Loss: 3.474387\u001b[0m\n",
      "\u001b[34mcurrent epoch: 2\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [0/500434 (0%)] Loss: 3.461339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [10240/500434 (2%)] Loss: 3.418624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [20480/500434 (4%)] Loss: 3.481724\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [30720/500434 (6%)] Loss: 3.490474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [40960/500434 (8%)] Loss: 3.471971\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/500434 (10%)] Loss: 3.483748\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [61440/500434 (12%)] Loss: 3.476754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [71680/500434 (14%)] Loss: 3.513221\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [81920/500434 (16%)] Loss: 3.397217\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [92160/500434 (18%)] Loss: 3.434366\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [102400/500434 (20%)] Loss: 3.423882\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [112640/500434 (22%)] Loss: 3.420102\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [122880/500434 (25%)] Loss: 3.385128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [133120/500434 (27%)] Loss: 3.404417\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [143360/500434 (29%)] Loss: 3.301955\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [153600/500434 (31%)] Loss: 3.353239\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [163840/500434 (33%)] Loss: 3.348356\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [174080/500434 (35%)] Loss: 3.339276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [184320/500434 (37%)] Loss: 3.229419\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [194560/500434 (39%)] Loss: 3.272631\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [204800/500434 (41%)] Loss: 3.270993\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [215040/500434 (43%)] Loss: 3.211095\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [225280/500434 (45%)] Loss: 3.302456\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [235520/500434 (47%)] Loss: 3.231983\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [245760/500434 (49%)] Loss: 3.359667\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [256000/500434 (51%)] Loss: 3.283528\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [266240/500434 (53%)] Loss: 3.261859\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [276480/500434 (55%)] Loss: 3.314681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [286720/500434 (57%)] Loss: 3.253138\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [296960/500434 (59%)] Loss: 3.231567\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [307200/500434 (61%)] Loss: 3.224015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [317440/500434 (63%)] Loss: 3.199152\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [327680/500434 (65%)] Loss: 3.207947\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [337920/500434 (67%)] Loss: 3.172481\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [348160/500434 (70%)] Loss: 3.169269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [358400/500434 (72%)] Loss: 3.131017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [368640/500434 (74%)] Loss: 3.175851\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [378880/500434 (76%)] Loss: 3.199745\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [389120/500434 (78%)] Loss: 3.282715\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [399360/500434 (80%)] Loss: 3.143600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [409600/500434 (82%)] Loss: 3.095373\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [419840/500434 (84%)] Loss: 3.231791\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [430080/500434 (86%)] Loss: 3.135104\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [440320/500434 (88%)] Loss: 3.074577\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [450560/500434 (90%)] Loss: 3.160592\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [460800/500434 (92%)] Loss: 3.178443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [471040/500434 (94%)] Loss: 3.232117\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [481280/500434 (96%)] Loss: 3.100541\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [491520/500434 (98%)] Loss: 3.121888\u001b[0m\n",
      "\u001b[34mcurrent epoch: 3\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [0/500434 (0%)] Loss: 3.153896\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [10240/500434 (2%)] Loss: 3.061134\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [20480/500434 (4%)] Loss: 3.115360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [30720/500434 (6%)] Loss: 3.023136\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [40960/500434 (8%)] Loss: 3.107972\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/500434 (10%)] Loss: 3.187945\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [61440/500434 (12%)] Loss: 2.994863\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [71680/500434 (14%)] Loss: 3.082523\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [81920/500434 (16%)] Loss: 3.074807\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [92160/500434 (18%)] Loss: 3.132762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [102400/500434 (20%)] Loss: 3.080032\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [112640/500434 (22%)] Loss: 3.058870\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [122880/500434 (25%)] Loss: 3.019137\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [133120/500434 (27%)] Loss: 3.078429\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [143360/500434 (29%)] Loss: 3.157131\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [153600/500434 (31%)] Loss: 3.098364\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [163840/500434 (33%)] Loss: 3.046500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [174080/500434 (35%)] Loss: 3.046466\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [184320/500434 (37%)] Loss: 3.078326\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [194560/500434 (39%)] Loss: 3.016701\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [204800/500434 (41%)] Loss: 2.976925\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [215040/500434 (43%)] Loss: 2.958368\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [225280/500434 (45%)] Loss: 3.077427\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [235520/500434 (47%)] Loss: 3.033339\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [245760/500434 (49%)] Loss: 3.034653\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [256000/500434 (51%)] Loss: 2.983087\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [266240/500434 (53%)] Loss: 3.032276\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [276480/500434 (55%)] Loss: 3.075416\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [286720/500434 (57%)] Loss: 3.074068\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [296960/500434 (59%)] Loss: 3.054013\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [307200/500434 (61%)] Loss: 3.037308\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [317440/500434 (63%)] Loss: 3.022543\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [327680/500434 (65%)] Loss: 3.047880\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [337920/500434 (67%)] Loss: 3.044843\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [348160/500434 (70%)] Loss: 3.022082\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [358400/500434 (72%)] Loss: 3.019680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [368640/500434 (74%)] Loss: 3.088089\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [378880/500434 (76%)] Loss: 3.015316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [389120/500434 (78%)] Loss: 3.041478\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [399360/500434 (80%)] Loss: 3.079509\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [409600/500434 (82%)] Loss: 3.064754\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [419840/500434 (84%)] Loss: 3.024448\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [430080/500434 (86%)] Loss: 3.040512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [440320/500434 (88%)] Loss: 3.092160\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [450560/500434 (90%)] Loss: 2.980551\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [460800/500434 (92%)] Loss: 2.915495\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [471040/500434 (94%)] Loss: 2.958017\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [481280/500434 (96%)] Loss: 3.044946\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [491520/500434 (98%)] Loss: 2.931814\u001b[0m\n",
      "\u001b[34mcurrent epoch: 4\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [0/500434 (0%)] Loss: 2.981198\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [10240/500434 (2%)] Loss: 3.049639\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [20480/500434 (4%)] Loss: 2.961369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [30720/500434 (6%)] Loss: 2.990561\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [40960/500434 (8%)] Loss: 2.933755\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/500434 (10%)] Loss: 2.948602\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [61440/500434 (12%)] Loss: 2.944808\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [71680/500434 (14%)] Loss: 2.904800\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [81920/500434 (16%)] Loss: 2.956184\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [92160/500434 (18%)] Loss: 2.988243\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [102400/500434 (20%)] Loss: 2.916005\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [112640/500434 (22%)] Loss: 2.999711\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [122880/500434 (25%)] Loss: 2.925652\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [133120/500434 (27%)] Loss: 2.936623\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [143360/500434 (29%)] Loss: 2.888199\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [153600/500434 (31%)] Loss: 2.886379\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [163840/500434 (33%)] Loss: 2.864671\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [174080/500434 (35%)] Loss: 2.896524\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [184320/500434 (37%)] Loss: 2.950941\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [194560/500434 (39%)] Loss: 3.034997\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [204800/500434 (41%)] Loss: 2.941018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [215040/500434 (43%)] Loss: 2.906316\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [225280/500434 (45%)] Loss: 2.901474\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [235520/500434 (47%)] Loss: 2.884500\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [245760/500434 (49%)] Loss: 2.880214\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [256000/500434 (51%)] Loss: 2.912284\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [266240/500434 (53%)] Loss: 2.916480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [276480/500434 (55%)] Loss: 2.962920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [286720/500434 (57%)] Loss: 2.900443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [296960/500434 (59%)] Loss: 2.843570\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [307200/500434 (61%)] Loss: 2.852269\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [317440/500434 (63%)] Loss: 2.889703\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [327680/500434 (65%)] Loss: 2.878970\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [337920/500434 (67%)] Loss: 2.860533\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [348160/500434 (70%)] Loss: 2.974480\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [358400/500434 (72%)] Loss: 2.903286\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [368640/500434 (74%)] Loss: 2.897763\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [378880/500434 (76%)] Loss: 2.892073\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [389120/500434 (78%)] Loss: 2.910164\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [399360/500434 (80%)] Loss: 2.886425\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [409600/500434 (82%)] Loss: 2.887589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [419840/500434 (84%)] Loss: 2.906441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [430080/500434 (86%)] Loss: 2.890702\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [440320/500434 (88%)] Loss: 2.878103\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [450560/500434 (90%)] Loss: 2.871998\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [460800/500434 (92%)] Loss: 2.875908\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [471040/500434 (94%)] Loss: 2.889607\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [481280/500434 (96%)] Loss: 2.864268\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [491520/500434 (98%)] Loss: 2.905924\u001b[0m\n",
      "\u001b[34mcurrent epoch: 5\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [0/500434 (0%)] Loss: 2.900085\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [10240/500434 (2%)] Loss: 2.867233\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [20480/500434 (4%)] Loss: 2.831539\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [30720/500434 (6%)] Loss: 2.956397\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [40960/500434 (8%)] Loss: 2.945128\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/500434 (10%)] Loss: 2.934747\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [61440/500434 (12%)] Loss: 2.878441\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [71680/500434 (14%)] Loss: 2.888889\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [81920/500434 (16%)] Loss: 3.010712\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [92160/500434 (18%)] Loss: 2.824600\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [102400/500434 (20%)] Loss: 2.887996\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [112640/500434 (22%)] Loss: 2.900226\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [122880/500434 (25%)] Loss: 2.923044\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [133120/500434 (27%)] Loss: 2.886432\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [143360/500434 (29%)] Loss: 2.900973\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [153600/500434 (31%)] Loss: 2.864823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [163840/500434 (33%)] Loss: 2.876550\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [174080/500434 (35%)] Loss: 2.887256\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [184320/500434 (37%)] Loss: 2.785872\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [194560/500434 (39%)] Loss: 2.933738\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [204800/500434 (41%)] Loss: 2.837253\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [215040/500434 (43%)] Loss: 2.844875\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [225280/500434 (45%)] Loss: 2.824215\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [235520/500434 (47%)] Loss: 2.747762\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [245760/500434 (49%)] Loss: 2.838805\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [256000/500434 (51%)] Loss: 2.891512\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [266240/500434 (53%)] Loss: 2.831645\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [276480/500434 (55%)] Loss: 2.784107\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [286720/500434 (57%)] Loss: 2.906499\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [296960/500434 (59%)] Loss: 2.910045\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [307200/500434 (61%)] Loss: 2.791803\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [317440/500434 (63%)] Loss: 2.899146\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [327680/500434 (65%)] Loss: 2.884750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [337920/500434 (67%)] Loss: 2.813589\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [348160/500434 (70%)] Loss: 2.889585\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [358400/500434 (72%)] Loss: 2.891404\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [368640/500434 (74%)] Loss: 2.887393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [378880/500434 (76%)] Loss: 2.860219\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [389120/500434 (78%)] Loss: 2.804757\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [399360/500434 (80%)] Loss: 2.899369\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [409600/500434 (82%)] Loss: 2.881083\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [419840/500434 (84%)] Loss: 2.857786\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [430080/500434 (86%)] Loss: 2.854084\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [440320/500434 (88%)] Loss: 2.800367\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [450560/500434 (90%)] Loss: 2.829163\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [460800/500434 (92%)] Loss: 2.872626\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [471040/500434 (94%)] Loss: 2.892415\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [481280/500434 (96%)] Loss: 2.889296\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [491520/500434 (98%)] Loss: 2.828088\u001b[0m\n",
      "\u001b[34mcurrent epoch: 6\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [0/500434 (0%)] Loss: 2.902493\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [10240/500434 (2%)] Loss: 2.849759\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [20480/500434 (4%)] Loss: 2.847030\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [30720/500434 (6%)] Loss: 2.956119\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [40960/500434 (8%)] Loss: 2.857216\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/500434 (10%)] Loss: 2.760685\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [61440/500434 (12%)] Loss: 2.779101\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [71680/500434 (14%)] Loss: 2.790250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [81920/500434 (16%)] Loss: 2.866722\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [92160/500434 (18%)] Loss: 2.767135\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [102400/500434 (20%)] Loss: 2.831158\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [112640/500434 (22%)] Loss: 2.791145\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [122880/500434 (25%)] Loss: 2.861651\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [133120/500434 (27%)] Loss: 2.832828\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [143360/500434 (29%)] Loss: 2.785443\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [153600/500434 (31%)] Loss: 2.891207\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [163840/500434 (33%)] Loss: 2.879911\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [174080/500434 (35%)] Loss: 2.857711\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training':data_input}, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd0c31",
   "metadata": {},
   "source": [
    "# Distilbert HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0230a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c41a39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://unspsc-data/segment_training'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f54589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for restart --\n",
    "\n",
    "data_input = 's3://unspsc-data/segment_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4be33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4087a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='unspsc_distilbert_sagemaker_hpo.py',\n",
    "                                    instance_count=1,\n",
    "                                    instance_type=\"ml.g4dn.xlarge\",\n",
    "                                    transformers_version='4.12',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "                                    role=role)\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(1e-5, 1e-3),\n",
    "    \"batch-size\": CategoricalParameter([32, 64, 128]),\n",
    "    \"epochs\": IntegerParameter(1, 2),\n",
    "    'eps': ContinuousParameter(1e-8, 1e-7)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"Balanced Accuracy Final:\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": \"Balanced Accuracy Final:\", \"Regex\": \"Balanced Accuracy Final: ([0-9\\\\.]+)\"}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    huggingface_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type=objective_type,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({\"training\": data_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccf05f",
   "metadata": {},
   "source": [
    "# Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93e87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tuner.attach(tuning_job_name='huggingface-pytorch--220910-1109')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa4703d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6992034316062927"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best hyperparameter outcome.\n",
    "tuner.describe()['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f7c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-09-10 18:53:30 Starting - Found matching resource for reuse\n",
      "2022-09-10 18:53:30 Downloading - Downloading input data\n",
      "2022-09-10 18:53:30 Training - Training image download completed. Training in progress.\n",
      "2022-09-10 18:53:30 Uploading - Uploading generated training model\n",
      "2022-09-10 18:53:30 Completed - Resource released due to keep alive period expiry\n"
     ]
    }
   ],
   "source": [
    "best_estimator= tuner.best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146626ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_tuning_objective_metric': '\"Balanced Accuracy Final:\"', 'batch-size': '\"128\"', 'epochs': '2', 'eps': '3.848853581472183e-08', 'lr': '9.300726878687543e-05', 'sagemaker_container_log_level': '20', 'sagemaker_estimator_class_name': '\"HuggingFace\"', 'sagemaker_estimator_module': '\"sagemaker.huggingface.estimator\"', 'sagemaker_job_name': '\"huggingface-pytorch-training-2022-09-10-11-09-13-895\"', 'sagemaker_program': '\"unspsc_distilbert_sagemaker_hpo.py\"', 'sagemaker_region': '\"us-east-1\"', 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-10-11-09-13-895/source/sourcedir.tar.gz\"'}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = best_estimator.hyperparameters()\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474dccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_hyperparameter_names = ['batch-size', 'epochs', 'eps', 'lr']\n",
    "\n",
    "hyperparameters_for_profiling = {key:value for key, value in hyperparameters.items() if key in actual_hyperparameter_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214ecc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch-size': '\"128\"',\n",
       " 'epochs': '2',\n",
       " 'eps': '3.848853581472183e-08',\n",
       " 'lr': '9.300726878687543e-05'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_for_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592ebfb",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c377ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs, CollectionConfig\n",
    "from sagemaker.debugger import DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    "\n",
    "rules = [Rule.sagemaker(rule_configs.vanishing_gradient(), collections_to_save=[CollectionConfig(name = 'gradients',\n",
    "                                                                                                        parameters = {\n",
    "                                                                                                            'train.save_interval': '50',\n",
    "                                                                                                            'eval.save_interval': '5'\n",
    "                                                                                                        }\n",
    "                                                                                                        )]),\n",
    "         Rule.sagemaker(rule_configs.overfit()),\n",
    "         Rule.sagemaker(rule_configs.overtraining()),\n",
    "         Rule.sagemaker(rule_configs.poor_weight_initialization(), collections_to_save=[CollectionConfig(name = 'weights',\n",
    "                                                                                                        parameters = {\n",
    "                                                                                                            'train.save_interval': '50',\n",
    "                                                                                                            'eval.save_interval': '5'\n",
    "                                                                                                        }\n",
    "                                                                                                        )]),\n",
    "         Rule.sagemaker(rule_configs.loss_not_decreasing(), collections_to_save=[CollectionConfig(name=\"losses\",\n",
    "                                                                                                  parameters={\n",
    "                                                                                                      \"train.save_interval\": \"50\",\n",
    "                                                                                                      \"eval.save_interval\": \"5\"\n",
    "                                                                                                  }\n",
    "                                                                                                 )]),\n",
    "         ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "         ProfilerRule.sagemaker(rule_configs.ProfilerReport())\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "profiler_config = ProfilerConfig(system_monitor_interval_millis=500, \n",
    "                                 framework_profile_params=FrameworkProfile(num_steps=10))\n",
    "\n",
    "debugger_config = DebuggerHookConfig(hook_parameters={\"train.save_interval\": \"50\",\n",
    "                                                      \"eval.save_interval\": \"5\"\n",
    "                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2654ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator_profile = HuggingFace(entry_point='train_distilbert.py',\n",
    "                                    instance_count=1,\n",
    "                                    instance_type=\"ml.g4dn.xlarge\",\n",
    "                                    hyperparameters=hyperparameters_for_profiling,\n",
    "                                    transformers_version='4.12',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "                                    role=role, \n",
    "                                    profiler_config = profiler_config,\n",
    "                                    debugger_hook_config= debugger_config,\n",
    "                                    rules = rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b41c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-11 12:08:41 Starting - Starting the training job...VanishingGradient: InProgress\n",
      "Overfit: InProgress\n",
      "Overtraining: InProgress\n",
      "PoorWeightInitialization: InProgress\n",
      "LossNotDecreasing: InProgress\n",
      "LowGPUUtilization: InProgress\n",
      "ProfilerReport: InProgress\n",
      "......\n",
      "2022-09-11 12:10:06 Starting - Preparing the instances for training...\n",
      "2022-09-11 12:10:34 Downloading - Downloading input data\n",
      "2022-09-11 12:10:34 Training - Downloading the training image..........................\n",
      "2022-09-11 12:15:14 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-11 12:14:58,313 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-11 12:14:58,344 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-11 12:14:58,350 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-11 12:14:58,917 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": \"\\\"128\\\"\",\n",
      "        \"epochs\": \"2\",\n",
      "        \"eps\": \"3.848853581472183e-08\",\n",
      "        \"lr\": \"9.300726878687543e-05\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-09-11-12-08-40-872\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-11-12-08-40-872/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_distilbert\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_distilbert.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":\"\\\"128\\\"\",\"epochs\":\"2\",\"eps\":\"3.848853581472183e-08\",\"lr\":\"9.300726878687543e-05\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_distilbert.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_distilbert\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-11-12-08-40-872/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":\"\\\"128\\\"\",\"epochs\":\"2\",\"eps\":\"3.848853581472183e-08\",\"lr\":\"9.300726878687543e-05\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-09-11-12-08-40-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-556976497373/huggingface-pytorch-training-2022-09-11-12-08-40-872/source/sourcedir.tar.gz\",\"module_name\":\"train_distilbert\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_distilbert.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"\\\"128\\\"\",\"--epochs\",\"2\",\"--eps\",\"3.848853581472183e-08\",\"--lr\",\"9.300726878687543e-05\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=\"128\"\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_EPS=3.848853581472183e-08\u001b[0m\n",
      "\u001b[34mSM_HP_LR=9.300726878687543e-05\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_distilbert.py --batch-size \"128\" --epochs 2 --eps 3.848853581472183e-08 --lr 9.300726878687543e-05\u001b[0m\n",
      "\u001b[34m[2022-09-11 12:15:03.415 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-09-11 12:15:03.524 algo-1:26 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 7.20MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 17.5kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 7.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 483/483 [00:00<00:00, 460kB/s]\u001b[0m\n",
      "\u001b[34m[2022-09-11 12:15:04.288 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-11 12:15:04.290 algo-1:26 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-11 12:15:04.292 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-11 12:15:04.292 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "LowGPUUtilization: IssuesFound\n",
      "ProfilerReport: InProgress\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator_profile.fit({'training': data_input}, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398dc1a",
   "metadata": {},
   "source": [
    "# Profiling Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a65def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcdd5f4f",
   "metadata": {},
   "source": [
    "# Deploy to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b3756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f4d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bac218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd3722fd",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[huggingface tutorial notebook](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
    "\n",
    "\n",
    "[huggingface sagemaker tutorial](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb)\n",
    "\n",
    "[data augmentation from netune ai](https://neptune.ai/blog/data-augmentation-nlp)\n",
    "\n",
    "[textual augmentation example code](https://github.com/makcedward/nlpaug/blob/23800cbb9632c7fc8c4a88d46f9c4ecf68a96299/example/textual_augmenter.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
